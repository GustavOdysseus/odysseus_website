{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\n",
      "  Using cached openai-1.52.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Using cached anyio-4.6.2.post1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Using cached jiter-0.6.1-cp312-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>4 in d:\\caleb\\caleb\\python\\mercado\\mercado\\ulysses\\vectorbt_pro\\venv\\lib\\site-packages (from openai) (4.66.5)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\caleb\\caleb\\python\\mercado\\mercado\\ulysses\\vectorbt_pro\\venv\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in d:\\caleb\\caleb\\python\\mercado\\mercado\\ulysses\\vectorbt_pro\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached pydantic_core-2.23.4-cp312-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: colorama in d:\\caleb\\caleb\\python\\mercado\\mercado\\ulysses\\vectorbt_pro\\venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Using cached openai-1.52.0-py3-none-any.whl (386 kB)\n",
      "Using cached anyio-4.6.2.post1-py3-none-any.whl (90 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.27.2-py3-none-any.whl (76 kB)\n",
      "Using cached httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
      "Using cached jiter-0.6.1-cp312-none-win_amd64.whl (198 kB)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.4-cp312-none-win_amd64.whl (1.9 MB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: typing-extensions, sniffio, jiter, h11, distro, annotated-types, pydantic-core, httpcore, anyio, pydantic, httpx, openai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.6.2.post1 distro-1.9.0 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.6.1 openai-1.52.0 pydantic-2.9.2 pydantic-core-2.23.4 sniffio-1.3.1 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 152\u001b[0m\n\u001b[0;32m    148\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFalha ao deletar \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Motivo: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Obtém o diretório raiz do projeto\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m project_root \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m)))\n\u001b[0;32m    154\u001b[0m \u001b[38;5;66;03m# Constrói o caminho para a pasta vectorbt_pro_site\u001b[39;00m\n\u001b[0;32m    155\u001b[0m base_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(project_root, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorbt_pro_site\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import hashlib\n",
    "import shutil\n",
    "import readability\n",
    "\n",
    "# Set of visited URLs to prevent infinite recursion\n",
    "visited_urls = set()\n",
    "\n",
    "def download_page(url):\n",
    "    \"\"\"\n",
    "    Downloads the content of a web page from the given URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the web page to download.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the web page as a string, or None if there was an error.\n",
    "\n",
    "    Raises:\n",
    "        requests.RequestException: If there was an error while downloading the web page.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raises an error for bad status codes\n",
    "        return response.text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_urls(html, base_url, ignored_extensions = ['.txt', '.pdf', '.docx']):\n",
    "    \"\"\"\n",
    "    Extracts all URLs from the given HTML content, resolving relative URLs and ignoring hash fragments.\n",
    "\n",
    "    Args:\n",
    "        html (str): The HTML content to extract URLs from.\n",
    "        base_url (str): The base URL used to resolve relative URLs.\n",
    "\n",
    "    Returns:\n",
    "        set: A set of URLs extracted from the HTML content.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    urls = set()\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        # Resolve relative URLs and filter by hash fragment\n",
    "        full_url = urljoin(base_url, href.split('#', 1)[0])\n",
    "        # Ignore URLs ending with specific file extensions\n",
    "        if any(full_url.endswith(ext) for ext in ignored_extensions):\n",
    "            continue\n",
    "        if urlparse(full_url).netloc == urlparse(base_url).netloc:\n",
    "            urls.add(full_url)\n",
    "    return urls\n",
    "\n",
    "\n",
    "\n",
    "def html_to_markdown(html):\n",
    "    \"\"\"\n",
    "    Converts HTML content to Markdown format.\n",
    "\n",
    "    Parameters:\n",
    "    html (str): The HTML content to be converted.\n",
    "\n",
    "    Returns:\n",
    "    str: The Markdown representation of the HTML content.\n",
    "    \"\"\"\n",
    "    # Using readability to extract the main content\n",
    "    document = readability.Document(html)\n",
    "    summary = document.summary()\n",
    "\n",
    "    converter = html2text.HTML2Text()\n",
    "    converter.ignore_links = False\n",
    "    return converter.handle(summary)\n",
    "\n",
    "def save_markdown(markdown, folder, filename):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(markdown)\n",
    "\n",
    "\n",
    "def generate_filename(url, base_url):\n",
    "    \"\"\"\n",
    "    Generate a filename based on the given URL and base URL.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL from which the filename will be generated.\n",
    "        base_url (str): The base URL used to remove the common path from the URL.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated filename.\n",
    "\n",
    "    \"\"\"\n",
    "    # Parse the URLs\n",
    "    parsed_url = urlparse(url)\n",
    "    parsed_base_url = urlparse(base_url)\n",
    "\n",
    "    # Remove the base URL path to get the unique part of the path\n",
    "    base_path = parsed_base_url.path.strip('/')\n",
    "    unique_path = parsed_url.path.strip('/')\n",
    "\n",
    "    # If the base_path is not empty, remove it from the start of unique_path\n",
    "    if base_path and unique_path.startswith(base_path):\n",
    "        unique_path = unique_path[len(base_path):].strip('/')\n",
    "\n",
    "    # Split the path into segments and join them with hyphens\n",
    "    if unique_path:\n",
    "        filename = unique_path.replace('/', '-').lower() + \".md\"\n",
    "    else:\n",
    "        filename = \"index.md\"\n",
    "    return filename\n",
    "\n",
    "\n",
    "def scrape_site(url, base_url, base_folder=''):\n",
    "    \"\"\"\n",
    "    Scrapes a website recursively, saving the content as markdown files.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the website to scrape.\n",
    "        base_url (str): The base URL of the website.\n",
    "        base_folder (str, optional): The base folder to save the markdown files. Defaults to ''.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Ensure the URL starts with the base URL\n",
    "    if not url.startswith(base_url):\n",
    "        return\n",
    "\n",
    "    if url in visited_urls or urlparse(url).netloc != urlparse(base_url).netloc:\n",
    "        return\n",
    "    visited_urls.add(url)\n",
    "\n",
    "    print(f\"Scraping {url}\")\n",
    "    html = download_page(url)\n",
    "    if html:\n",
    "        markdown = html_to_markdown(html)\n",
    "        filename = generate_filename(url, base_url)\n",
    "        folder = os.path.join(base_folder, urlparse(base_url).netloc)\n",
    "        save_markdown(markdown, folder, filename)\n",
    "\n",
    "        for link in extract_urls(html, url):\n",
    "            scrape_site(link, base_url, base_folder)\n",
    "\n",
    "\n",
    "\n",
    "def clean_directory(folder):\n",
    "    \"\"\"\n",
    "    Deletes all files and folders in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        folder (str): The path to the directory to be cleaned.\n",
    "\n",
    "    Raises:\n",
    "        OSError: If there is an error while deleting files or folders.\n",
    "\n",
    "    \"\"\"\n",
    "    if os.path.exists(folder):\n",
    "        for filename in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)\n",
    "            except Exception as e:\n",
    "                print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "\n",
    "# Example usage\n",
    "\n",
    "base_url = 'https://vectorbt.pro/'  # Change this URL to your target\n",
    "url_secret = os.getenv('VBT_PRO_SECRET_URL') # '5af3d00f'\n",
    "print(f\"Secret URL: {url_secret}\")\n",
    "start_url = f'{base_url}{url_secret}/'  # Change this URL to your target\n",
    "base_folder = 'output'\n",
    "\n",
    "clean_directory(base_folder)\n",
    "scrape_site(start_url, start_url, base_folder)\n",
    "print(\"Scraping complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m OPENAI_API_KEY \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize the OpenAI client\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOPENAI_API_KEY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Create a date-based SEMVER version for the Vector Store's name\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_version_tag\u001b[39m():\n",
      "File \u001b[1;32md:\\Caleb\\caleb\\Python\\mercado\\mercado\\ulysses\\vectorbt_pro\\venv\\Lib\\site-packages\\openai\\_client.py:105\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    103\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m     )\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from datetime import date\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "# Set OpenAI API key as an environment variable\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "VECTORBT_PRO_SECRET_URL = os.getenv(\"VBT_PRO_SECRET_URL\")\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.Client(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Create a date-based SEMVER version for the Vector Store's name\n",
    "def generate_version_tag():\n",
    "    today = date.today()\n",
    "    timestamp = int(time.time())\n",
    "    commit_hash = hashlib.sha1(str(timestamp).encode()).hexdigest()[:4].upper()\n",
    "    version_tag = VECTORBT_PRO_SECRET_URL + \"_\" + today.strftime(\"%m.%d.%Y\")\n",
    "    return version_tag\n",
    "\n",
    "version_tag = generate_version_tag()\n",
    "\n",
    "assistant_name = f\"QuantGPT {version_tag}\"\n",
    "\n",
    "assistant_instructions = f\"\"\"You are a helpful assistant that has a knowledge base uploaded to you containing information on how the closed-source VectorBT (PRO) Python library and its modules work for building financial backtests and simulations.\n",
    "\n",
    "VectorBT PRO (vectorbtpro) is a next-generation engine for backtesting, algorithmic trading, and research. It's a high-performance, actively-developed, proprietary successor to the vectorbt library, one of the world's most innovative open-source backtesting packages. The PRO version extends the open-source package with new impressive features and useful enhancements.\n",
    "\n",
    "You are an expert at reading through the provided VectorBT (PRO) documentation and coming up with clear, accurate answers to users' queries.\n",
    "\n",
    "You have been given a massive index to search through which contains all of the text from VBT (PRO)'s documentation. If you cannot find/retrieve the answer in your vector store, you simply let the user know. Respond saying that you can't find any information on that topic specifically.\n",
    "\n",
    "Also, FYI, VectorBT (PRO) can also be referred to in this context as VBT, so if VBT is mentioned in the messages, assume the user is referring to this closed source version, NOT the open source `vectorbt`. VectorBT PRO has been completely refactored to improve performance and enable new groundbreaking features, such as parallelization support, so many things are different from how the older, open source version worked.\"\"\"\n",
    "\n",
    "# Step 1: Create a new Assistant with File Search Enabled\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=assistant_name,\n",
    "    instructions=assistant_instructions,\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.40,\n",
    "    # tools array Optional Defaults to [] A list of tool enabled on the assistant. There can be a maximum of 128 tools per assistant. Tools can be of types code_interpreter, file_search, or function.\n",
    "    tools=[{\"type\": \"file_search\"}, {\"type\": \"code_interpreter\"}],\n",
    ")\n",
    "print(f\"Assistant created successfully ✔\")\n",
    "print(f\"Assistant Name: {assistant.name}\")\n",
    "print(f\"Assistant ID: {assistant.id}\")\n",
    "\n",
    "# Step 2: Upload files and add them to a Vector Store\n",
    "# Define the directories where files are located (there can be one or multiple)\n",
    "directories = [\n",
    "\n",
    "    './output/vectorbt.pro/', \n",
    "\n",
    "]\n",
    "\n",
    "# Supported file extensions\n",
    "supported_extensions = {\n",
    "    '.c', '.cs', '.cpp', '.doc', '.docx', '.html', '.java', '.json', '.md', \n",
    "    '.pdf', '.php', '.pptx', '.py', '.rb', '.tex', '.txt', '.css', '.js', \n",
    "    '.sh', '.ts'\n",
    "}\n",
    "\n",
    "# Create a vector store \n",
    "vector_store = client.beta.vector_stores.create(name=assistant_name)\n",
    "\n",
    "# Ready the files for upload to OpenAI\n",
    "file_paths = [\n",
    "    os.path.join(directory, filename) \n",
    "    for directory in directories\n",
    "    for filename in os.listdir(directory) \n",
    "    if os.path.isfile(os.path.join(directory, filename)) and os.path.splitext(filename)[1] in supported_extensions\n",
    "]\n",
    "\n",
    "# Batch the file uploads\n",
    "batch_size = 500\n",
    "file_ids = []\n",
    "for i in range(0, len(file_paths), batch_size):\n",
    "    batch = file_paths[i:i+batch_size]\n",
    "    file_streams = [open(path, \"rb\") for path in batch]\n",
    "    file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "        vector_store_id=vector_store.id, files=file_streams\n",
    "    )\n",
    "    \n",
    "    # Print the entire file_batch object to understand its structure\n",
    "    print(file_batch)\n",
    "    print(file_batch.status)\n",
    "    print(file_batch.file_counts)\n",
    "\n",
    "    # If there are any errors, print them out\n",
    "    if hasattr(file_batch, 'errors'):\n",
    "        for error in file_batch.errors:\n",
    "            print(f\"Error uploading file {error.file}: {error.message}\")\n",
    "\n",
    "    # Close the file streams\n",
    "    for file_stream in file_streams:\n",
    "        file_stream.close()\n",
    "\n",
    "# Step 3: Update the assistant to use the new Vector Store\n",
    "assistant = client.beta.assistants.update(\n",
    "    assistant_id=assistant.id,\n",
    "    tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}}\n",
    ")\n",
    "print(f\"Assistant updated with vector store: {vector_store.id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
