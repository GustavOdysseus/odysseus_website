### Arquivo: accessors.py
#### Docstring do Módulo
```
Root Pandas accessors of vectorbtpro.

An accessor adds additional "namespace" to pandas objects.

The `vectorbtpro.accessors` registers a custom `vbt` accessor on top of each `pd.Index`, `pd.Series`,
and `pd.DataFrame` object. It is the main entry point for all other accessors:

```plaintext
vbt.base.accessors.BaseIDX/SR/DFAccessor       -> pd.Index/Series/DataFrame.vbt.*
vbt.generic.accessors.GenericSR/DFAccessor     -> pd.Series/DataFrame.vbt.*
vbt.signals.accessors.SignalsSR/DFAccessor     -> pd.Series/DataFrame.vbt.signals.*
vbt.returns.accessors.ReturnsSR/DFAccessor     -> pd.Series/DataFrame.vbt.returns.*
vbt.ohlcv.accessors.OHLCVDFAccessor            -> pd.DataFrame.vbt.ohlcv.*
vbt.px.accessors.PXSR/DFAccessor               -> pd.Series/DataFrame.vbt.px.*
```

Additionally, some accessors subclass other accessors building the following inheritance hiearchy:

```plaintext
vbt.base.accessors.BaseIDXAccessor
vbt.base.accessors.BaseSR/DFAccessor
    -> vbt.generic.accessors.GenericSR/DFAccessor
        -> vbt.signals.accessors.SignalsSR/DFAccessor
        -> vbt.returns.accessors.ReturnsSR/DFAccessor
        -> vbt.ohlcv.accessors.OHLCVDFAccessor
    -> vbt.px.accessors.PXSR/DFAccessor
```

So, for example, the method `pd.Series.vbt.to_2d_array` is also available as
`pd.Series.vbt.returns.to_2d_array`.

Class methods of any accessor can be conveniently accessed using `pd_acc`, `sr_acc`, and `df_acc` shortcuts:

```pycon
>>> from vectorbtpro import *

>>> vbt.pd_acc.signals.generate
<bound method SignalsAccessor.generate of <class 'vectorbtpro.signals.accessors.SignalsAccessor'>>
```

!!! note
    Accessors in vectorbt are not cached, so querying `df.vbt` twice will also call `Vbt_DFAccessor` twice.
    You can change this in global settings.
```

---

### Arquivo: accessors.py
#### Classe: Accessor
```
Accessor.
```

---

### Arquivo: accessors.py
#### Classe: CachedAccessor
```
Cached accessor.
```

---

### Arquivo: accessors.py
#### Função: register_accessor
```
Register a custom accessor.

`cls` must subclass `pandas.core.accessor.DirNamesMixin`.
```

---

### Arquivo: accessors.py
#### Função: register_index_accessor
```
Decorator to register a custom `pd.Index` accessor.
```

---

### Arquivo: accessors.py
#### Função: register_series_accessor
```
Decorator to register a custom `pd.Series` accessor.
```

---

### Arquivo: accessors.py
#### Função: register_dataframe_accessor
```
Decorator to register a custom `pd.DataFrame` accessor.
```

---

### Arquivo: accessors.py
#### Classe: Vbt_IDXAccessor
```
The main vectorbt accessor for `pd.Index`.
```

---

### Arquivo: accessors.py
#### Classe: Vbt_Accessor
```
The main vectorbt accessor for `pd.Series` and `pd.DataFrame`.
```

---

### Arquivo: accessors.py
#### Classe: Vbt_SRAccessor
```
The main vectorbt accessor for `pd.Series`.
```

---

### Arquivo: accessors.py
#### Classe: Vbt_DFAccessor
```
The main vectorbt accessor for `pd.DataFrame`.
```

---

### Arquivo: accessors.py
#### Função: register_vbt_accessor
```
Decorator to register an accessor on top of a parent accessor.
```

---

### Arquivo: accessors.py
#### Função: register_idx_vbt_accessor
```
Decorator to register a `pd.Index` accessor on top of a parent accessor.
```

---

### Arquivo: accessors.py
#### Função: register_sr_vbt_accessor
```
Decorator to register a `pd.Series` accessor on top of a parent accessor.
```

---

### Arquivo: accessors.py
#### Função: register_df_vbt_accessor
```
Decorator to register a `pd.DataFrame` accessor on top of a parent accessor.
```

---

### Arquivo: _opt_deps.py
#### Docstring do Módulo
```
Optional dependencies.
```

---

### Arquivo: _settings.py
#### Docstring do Módulo
```
Global settings of vectorbtpro.

`settings` config is also accessible via `vectorbtpro.settings`.

!!! note
    All places in vectorbt import `vectorbtpro._settings.settings`, not `vectorbtpro.settings`.
    Overwriting `vectorbtpro.settings` only overwrites the reference created for the user.
    Consider updating the settings config instead of replacing it.

Here are the main properties of the `settings` config:

* It's a nested config, that is, a config that consists of multiple sub-configs.
    one per sub-package (e.g., 'data'), module (e.g., 'wrapping'), or even class (e.g., 'configured').
    Each sub-config may consist of other sub-configs.
* It has frozen keys - you cannot add other sub-configs or remove the existing ones, but you can modify them.
* Each sub-config can be `frozen_cfg` or `flex_cfg`. The main reason for defining a flexible config
    is to allow adding new keys (e.g., 'plotting.layout').

For example, you can change default width and height of each plot:

```pycon
>>> from vectorbtpro import *

>>> vbt.settings['plotting']['layout']['width'] = 800
>>> vbt.settings['plotting']['layout']['height'] = 400
```

The main sub-configs such as for plotting can be also accessed/modified using the dot notation:

```pycon
>>> vbt.settings.plotting['layout']['width'] = 800
```

Some sub-configs allow the dot notation too but this depends on whether they are an instance of `frozen_cfg`:

```pycon
>>> type(vbt.settings)
vectorbtpro._settings.frozen_cfg
>>> vbt.settings.data  # ok

>>> type(vbt.settings.data)
vectorbtpro._settings.frozen_cfg
>>> vbt.settings.data.silence_warnings  # ok

>>> type(vbt.settings.data.custom)
vectorbtpro._settings.flex_cfg
>>> vbt.settings.data.custom.binance  # error
>>> vbt.settings.data.custom["binance"]  # ok
```

Since this is only visible when looking at the source code, the advice is to always use the bracket notation.

!!! note
    Whether the change takes effect immediately depends upon the place that accesses the settings.
    For example, changing 'wrapping.freq` has an immediate effect because the value is resolved
    every time `vectorbtpro.base.wrapping.ArrayWrapper.freq` is called. On the other hand, changing
    'portfolio.fillna_close' has only effect on `vectorbtpro.portfolio.base.Portfolio` instances created
    in the future, not the existing ones, because the value is resolved upon the object's construction.
    Moreover, some settings are only accessed when importing the package for the first time,
    such as 'jitting.jit_decorator'. In any case, make sure to check whether the update actually took place.

## Saving and loading

Like any other class subclassing `vectorbtpro.utils.config.Config`, we can persist settings to the disk,
load it back, and replace in-place. There are several ways of how to update the settings.

### Binary file

Pickling will dump the entire settings object into a byte stream and save as a binary file.
Supported file extensions are "pickle" (default) and "pkl".

```pycon
>>> vbt.settings.save('my_settings')
>>> vbt.settings['caching']['disable'] = True
>>> vbt.settings['caching']['disable']
True

>>> vbt.settings.load_update('my_settings', clear=True)  # replace in-place
>>> vbt.settings['caching']['disable']
False
```

!!! note
    Argument `clear=True` will replace the entire settings object. Disable it to apply
    only a subset of settings (default).

### Config file

We can also encode the settings object into a config and save as a text file that can be edited
easily. Supported file extensions are "config" (default), "cfg", and "ini".

```pycon
>>> vbt.settings.save('my_settings', file_format="config")
>>> vbt.settings['caching']['disable'] = True
>>> vbt.settings['caching']['disable']
True

>>> vbt.settings.load_update('my_settings', file_format="config", clear=True)  # replace in-place
>>> vbt.settings['caching']['disable']
False
```

### On import

Some settings (such as Numba-related ones) are applied only on import, so changing them during the runtime
will have no effect. In this case, change the settings, save them to the disk, and then either
rename the file to "vbt" (with extension) and place it in the working directory for it to be
recognized automatically, or create an environment variable "VBT_SETTINGS_PATH" that holds the full path
to the file - vectorbt will load it before any other module. You can also change the recognized file
name using an environment variable "VBT_SETTINGS_NAME", which defaults to "vbt".

!!! note
    Environment variables must be set before importing vectorbtpro.

For example, to set the default theme to dark, create the following "vbt.ini" file:

```ini
[plotting]
default_theme = dark
```
```

---

### Arquivo: _settings.py
#### Classe: frozen_cfg
```
Class representing a frozen sub-config.
```

---

### Arquivo: _settings.py
#### Classe: flex_cfg
```
Class representing a flexible sub-config.
```

---

### Arquivo: _settings.py
#### Classe: SettingsConfig
```
Extends `vectorbtpro.utils.config.Config` for global settings.
```

---

### Arquivo: _settings.py
#### Classe: SettingsConfig
#### Função: register_template
```
Register template of a theme.
```

---

### Arquivo: _settings.py
#### Classe: SettingsConfig
#### Função: register_templates
```
Register templates of all themes.
```

---

### Arquivo: _settings.py
#### Classe: SettingsConfig
#### Função: set_theme
```
Set default theme.
```

---

### Arquivo: _settings.py
#### Classe: SettingsConfig
#### Função: reset_theme
```
Reset to default theme.
```

---

### Arquivo: _settings.py
#### Classe: SettingsConfig
#### Função: substitute_sub_config_docs
```
Substitute templates in sub-config docs.
```

---

### Arquivo: _typing.py
#### Docstring do Módulo
```
General types used across vectorbtpro.
```

---

### Arquivo: __init__.py
#### Função: whats_imported
```
Print references and their values that got imported when running `from vectorbtpro import *`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Docstring do Módulo
```
Custom Pandas accessors for base operations with Pandas objects.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
```
Accessor on top of Index.

Accessible via `pd.Index.vbt` and all child accessors.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
```
Accessor on top of Series and DataFrames.

Accessible via `pd.Series.vbt` and `pd.DataFrame.vbt`, and all child accessors.

Series is just a DataFrame with one column, hence to avoid defining methods exclusively for 1-dim data,
we will convert any Series to a DataFrame and perform matrix computation on it. Afterwards,
by using `BaseAccessor.wrapper`, we will convert the 2-dim output back to a Series.

`**kwargs` will be passed to `vectorbtpro.base.wrapping.ArrayWrapper`.

!!! note
    When using magic methods, ensure that `.vbt` is called on the operand on the left
    if the other operand is an array.

    Accessors do not utilize caching.

    Grouping is only supported by the methods that accept the `group_by` argument.

Usage:
    * Build a symmetric matrix:

    ```pycon
    >>> from vectorbtpro import *

    >>> # vectorbtpro.base.accessors.BaseAccessor.make_symmetric
    >>> pd.Series([1, 2, 3]).vbt.make_symmetric()
         0    1    2
    0  1.0  2.0  3.0
    1  2.0  NaN  NaN
    2  3.0  NaN  NaN
    ```

    * Broadcast pandas objects:

    ```pycon
    >>> sr = pd.Series([1])
    >>> df = pd.DataFrame([1, 2, 3])

    >>> vbt.base.reshaping.broadcast_to(sr, df)
       0
    0  1
    1  1
    2  1

    >>> sr.vbt.broadcast_to(df)
       0
    0  1
    1  1
    2  1
    ```

    * Many methods such as `BaseAccessor.broadcast` are both class and instance methods:

    ```pycon
    >>> from vectorbtpro.base.accessors import BaseAccessor

    >>> # Same as sr.vbt.broadcast(df)
    >>> new_sr, new_df = BaseAccessor.broadcast(sr, df)
    >>> new_sr
       0
    0  1
    1  1
    2  1
    >>> new_df
       0
    0  1
    1  2
    2  3
    ```

    * Instead of explicitly importing `BaseAccessor` or any other accessor, we can use `pd_acc` instead:

    ```pycon
    >>> vbt.pd_acc.broadcast(sr, df)
    >>> new_sr
       0
    0  1
    1  1
    2  1
    >>> new_df
       0
    0  1
    1  2
    2  3
    ```

    * `BaseAccessor` implements arithmetic (such as `+`), comparison (such as `>`) and
    logical operators (such as `&`) by forwarding the operation to `BaseAccessor.combine`:

    ```pycon
    >>> sr.vbt + df
       0
    0  2
    1  3
    2  4
    ```

    Many interesting use cases can be implemented this way.

    * For example, let's compare an array with 3 different thresholds:

    ```pycon
    >>> df.vbt > vbt.Param(np.arange(3), name='threshold')
    threshold     0                  1                  2
                 a2    b2    c2     a2    b2    c2     a2     b2    c2
    x2         True  True  True  False  True  True  False  False  True
    y2         True  True  True   True  True  True   True   True  True
    z2         True  True  True   True  True  True   True   True  True
    ```

    * The same using the broadcasting mechanism:

    ```pycon
    >>> df.vbt > vbt.Param(np.arange(3), name='threshold')
    threshold     0                  1                  2
                 a2    b2    c2     a2    b2    c2     a2     b2    c2
    x2         True  True  True  False  True  True  False  False  True
    y2         True  True  True   True  True  True   True   True  True
    z2         True  True  True   True  True  True   True   True  True
    ```
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseSRAccessor
```
Accessor on top of Series.

Accessible via `pd.Series.vbt` and all child accessors.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseDFAccessor
```
Accessor on top of DataFrames.

Accessible via `pd.DataFrame.vbt` and all child accessors.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: obj
```
Pandas object.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: get
```
Get `IDXAccessor.obj`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: to_ns
```
Convert index to an 64-bit integer array.

Timestamps will be converted to nanoseconds.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: to_period
```
Convert index to period.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: to_period_ts
```
Convert index to period and then to timestamp.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: to_period_ns
```
Convert index to period and then to an 64-bit integer array.

Timestamps will be converted to nanoseconds.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: from_values
```
See `vectorbtpro.base.indexes.index_from_values`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: repeat
```
See `vectorbtpro.base.indexes.repeat_index`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: tile
```
See `vectorbtpro.base.indexes.tile_index`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: stack
```
See `vectorbtpro.base.indexes.stack_indexes`.

Set `on_top` to True to stack the second index on top of this one.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: combine
```
See `vectorbtpro.base.indexes.combine_indexes`.

Set `on_top` to True to stack the second index on top of this one.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: concat
```
See `vectorbtpro.base.indexes.concat_indexes`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: align_to
```
See `vectorbtpro.base.indexes.align_index_to`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: align
```
See `vectorbtpro.base.indexes.align_indexes`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: cross_with
```
See `vectorbtpro.base.indexes.cross_index_with`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: cross
```
See `vectorbtpro.base.indexes.cross_indexes`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: find_first_occurrence
```
See `vectorbtpro.base.indexes.find_first_occurrence`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: get_freq
```
Index frequency as `pd.Timedelta` or None if it cannot be converted.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: freq
```
`BaseIDXAccessor.get_freq` with date offsets and integer frequencies not allowed.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: ns_freq
```
Convert frequency to a 64-bit integer.

Timedelta will be converted to nanoseconds.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: any_freq
```
Index frequency of any type.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: get_period
```
Get the period of the index, without taking into account its datetime-like properties.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: period
```
`BaseIDXAccessor.get_period` with default arguments.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: get_dt_period
```
Get the period of the index, taking into account its datetime-like properties.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: dt_period
```
`BaseIDXAccessor.get_dt_period` with default arguments.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: arr_to_timedelta
```
Convert array to duration using `BaseIDXAccessor.freq`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: get_grouper
```
Get an index grouper of type `vectorbtpro.base.grouping.base.Grouper`.

Argument `by` can be a grouper itself, an instance of Pandas `GroupBy`,
an instance of Pandas `Resampler`, but also any supported input to any of them
such as a frequency or an array of indices.

Keyword arguments `groupby_kwargs` are passed to the Pandas methods `groupby` and `resample`,
while `**kwargs` are passed to initialize `vectorbtpro.base.grouping.base.Grouper`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: get_resampler
```
Get an index resampler of type `vectorbtpro.base.resampling.base.Resampler`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: get_points
```
See `vectorbtpro.base.indexing.get_index_points`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: get_ranges
```
See `vectorbtpro.base.indexing.get_index_ranges`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: split
```
Split using `vectorbtpro.generic.splitting.base.Splitter.split_and_take`.

!!! note
    Splits Pandas object, not accessor!
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseIDXAccessor
#### Função: split_apply
```
Split using `vectorbtpro.generic.splitting.base.Splitter.split_and_apply`.

!!! note
    Splits Pandas object, not accessor!
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: resolve_row_stack_kwargs
```
Resolve keyword arguments for initializing `BaseAccessor` after stacking along rows.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: resolve_column_stack_kwargs
```
Resolve keyword arguments for initializing `BaseAccessor` after stacking along columns.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: row_stack
```
Stack multiple `BaseAccessor` instances along rows.

Uses `vectorbtpro.base.wrapping.ArrayWrapper.row_stack` to stack the wrappers.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: column_stack
```
Stack multiple `BaseAccessor` instances along columns.

Uses `vectorbtpro.base.wrapping.ArrayWrapper.column_stack` to stack the wrappers.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: __call__
```
Allows passing arguments to the initializer.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: sr_accessor_cls
```
Accessor class for `pd.Series`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: df_accessor_cls
```
Accessor class for `pd.DataFrame`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: indexing_func
```
Perform indexing on `BaseAccessor`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: indexing_setter_func
```
Perform indexing setter on `BaseAccessor`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: obj
```
Pandas object.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: get
```
Get `BaseAccessor.obj`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: resolve_shape
```
Resolve shape.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: empty
```
Generate an empty Series/DataFrame of shape `shape` and fill with `fill_value`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: empty_like
```
Generate an empty Series/DataFrame like `other` and fill with `fill_value`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: set
```
Set value at each index point using `vectorbtpro.base.indexing.get_index_points`.

If `value_or_func` is a function, selects all keyword arguments that were not passed
to the `get_index_points` method, substitutes any templates, and passes everything to the function.
As context uses `kwargs`, `template_context`, and various variables such as `i` (iteration index),
`index_point` (absolute position in the index), `wrapper`, and `obj`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: set_between
```
Set value at each index range using `vectorbtpro.base.indexing.get_index_ranges`.

If `value_or_func` is a function, selects all keyword arguments that were not passed
to the `get_index_points` method, substitutes any templates, and passes everything to the function.
As context uses `kwargs`, `template_context`, and various variables such as `i` (iteration index),
`index_slice` (absolute slice of the index), `wrapper`, and `obj`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: to_1d_array
```
See `vectorbtpro.base.reshaping.to_1d` with `raw` set to True.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: to_2d_array
```
See `vectorbtpro.base.reshaping.to_2d` with `raw` set to True.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: tile
```
See `vectorbtpro.base.reshaping.tile`.

Set `axis` to 1 for columns and 0 for index.
Use `keys` as the outermost level.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: repeat
```
See `vectorbtpro.base.reshaping.repeat`.

Set `axis` to 1 for columns and 0 for index.
Use `keys` as the outermost level.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: align_to
```
Align to `other` on their axes using `vectorbtpro.base.indexes.align_index_to`.

Usage:
    ```pycon
    >>> df1 = pd.DataFrame(
    ...     [[1, 2], [3, 4]],
    ...     index=['x', 'y'],
    ...     columns=['a', 'b']
    ... )
    >>> df1
       a  b
    x  1  2
    y  3  4

    >>> df2 = pd.DataFrame(
    ...     [[5, 6, 7, 8], [9, 10, 11, 12]],
    ...     index=['x', 'y'],
    ...     columns=pd.MultiIndex.from_arrays([[1, 1, 2, 2], ['a', 'b', 'a', 'b']])
    ... )
    >>> df2
           1       2
       a   b   a   b
    x  5   6   7   8
    y  9  10  11  12

    >>> df1.vbt.align_to(df2)
          1     2
       a  b  a  b
    x  1  2  1  2
    y  3  4  3  4
    ```
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: align
```
Align objects using `vectorbtpro.base.indexes.align_indexes`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: cross_with
```
Align to `other` on their axes using `vectorbtpro.base.indexes.cross_index_with`.

Usage:
    ```pycon
    >>> df1 = pd.DataFrame(
    ...     [[1, 2, 3, 4], [5, 6, 7, 8]],
    ...     index=['x', 'y'],
    ...     columns=pd.MultiIndex.from_arrays([[1, 1, 2, 2], ['a', 'b', 'a', 'b']])
    ... )
    >>> df1
       1     2
       a  b  a  b
    x  1  2  3  4
    y  5  6  7  8

    >>> df2 = pd.DataFrame(
    ...     [[9, 10, 11, 12], [13, 14, 15, 16]],
    ...     index=['x', 'y'],
    ...     columns=pd.MultiIndex.from_arrays([[3, 3, 4, 4], ['a', 'b', 'a', 'b']])
    ... )
    >>> df2
        3       4
        a   b   a   b
    x   9  10  11  12
    y  13  14  15  16

    >>> df1.vbt.cross_with(df2)
       1           2
       3     4     3     4
       a  b  a  b  a  b  a  b
    x  1  2  1  2  3  4  3  4
    y  5  6  5  6  7  8  7  8
    ```
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: cross
```
Align objects using `vectorbtpro.base.indexes.cross_indexes`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: broadcast
```
See `vectorbtpro.base.reshaping.broadcast`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: broadcast_to
```
See `vectorbtpro.base.reshaping.broadcast_to`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: broadcast_combs
```
See `vectorbtpro.base.reshaping.broadcast_combs`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: make_symmetric
```
See `vectorbtpro.base.reshaping.make_symmetric`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: unstack_to_array
```
See `vectorbtpro.base.reshaping.unstack_to_array`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: unstack_to_df
```
See `vectorbtpro.base.reshaping.unstack_to_df`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: to_dict
```
See `vectorbtpro.base.reshaping.to_dict`.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: to_data
```
Convert to a `vectorbtpro.data.base.Data` instance.
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: apply
```
Apply a function `apply_func`.

Set `keep_pd` to True to keep inputs as pandas objects, otherwise convert to NumPy arrays.

Set `to_2d` to True to reshape inputs to 2-dim arrays, otherwise keep as-is.

`*args` and `**kwargs` are passed to `apply_func`.

!!! note
    The resulted array must have the same shape as the original array.

Usage:
    * Using instance method:

    ```pycon
    >>> sr = pd.Series([1, 2], index=['x', 'y'])
    >>> sr.vbt.apply(lambda x: x ** 2)
    x    1
    y    4
    dtype: int64
    ```

    * Using class method, templates, and broadcasting:

    ```pycon
    >>> sr.vbt.apply(
    ...     lambda x, y: x + y,
    ...     vbt.Rep('y'),
    ...     broadcast_named_args=dict(
    ...         y=pd.DataFrame([[3, 4]], columns=['a', 'b'])
    ...     )
    ... )
       a  b
    x  4  5
    y  5  6
    ```
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: concat
```
Concatenate with `others` along columns.

Usage:
    ```pycon
    >>> sr = pd.Series([1, 2], index=['x', 'y'])
    >>> df = pd.DataFrame([[3, 4], [5, 6]], index=['x', 'y'], columns=['a', 'b'])
    >>> sr.vbt.concat(df, keys=['c', 'd'])
          c     d
       a  b  a  b
    x  1  1  3  4
    y  2  2  5  6
    ```
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: apply_and_concat
```
Apply `apply_func` `ntimes` times and concatenate the results along columns.

See `vectorbtpro.base.combining.apply_and_concat`.

`ntimes` is the number of times to call `apply_func`, while `n_outputs` is the number of outputs to expect.

`*args` and `**kwargs` are passed to `vectorbtpro.base.combining.apply_and_concat`.

!!! note
    The resulted arrays to be concatenated must have the same shape as broadcast input arrays.

Usage:
    * Using instance method:

    ```pycon
    >>> df = pd.DataFrame([[3, 4], [5, 6]], index=['x', 'y'], columns=['a', 'b'])
    >>> df.vbt.apply_and_concat(
    ...     3,
    ...     lambda i, a, b: a * b[i],
    ...     [1, 2, 3],
    ...     keys=['c', 'd', 'e']
    ... )
          c       d       e
       a  b   a   b   a   b
    x  3  4   6   8   9  12
    y  5  6  10  12  15  18
    ```

    * Using class method, templates, and broadcasting:

    ```pycon
    >>> sr = pd.Series([1, 2, 3], index=['x', 'y', 'z'])
    >>> sr.vbt.apply_and_concat(
    ...     3,
    ...     lambda i, a, b: a * b + i,
    ...     vbt.Rep('df'),
    ...     broadcast_named_args=dict(
    ...         df=pd.DataFrame([[1, 2, 3]], columns=['a', 'b', 'c'])
    ...     )
    ... )
    apply_idx        0         1         2
               a  b  c  a  b   c  a  b   c
    x          1  2  3  2  3   4  3  4   5
    y          2  4  6  3  5   7  4  6   8
    z          3  6  9  4  7  10  5  8  11
    ```

    * To change the execution engine or specify other engine-related arguments, use `execute_kwargs`:

    ```pycon
    >>> import time

    >>> def apply_func(i, a):
    ...     time.sleep(1)
    ...     return a

    >>> sr = pd.Series([1, 2, 3])

    >>> %timeit sr.vbt.apply_and_concat(3, apply_func)
    3.02 s ± 3.76 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

    >>> %timeit sr.vbt.apply_and_concat(3, apply_func, execute_kwargs=dict(engine='dask'))
    1.02 s ± 927 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
    ```
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: combine
```
Combine with `other` using `combine_func`.

Args:
    obj (array_like): Object(s) to combine this array with.
    combine_func (callable): Function to combine two arrays.

        Can be Numba-compiled.
    *args: Variable arguments passed to `combine_func`.
    allow_multiple (bool): Whether a tuple/list/Index will be considered as multiple objects in `other`.

        Takes effect only when using the instance method.
    keep_pd (bool): Whether to keep inputs as pandas objects, otherwise convert to NumPy arrays.
    to_2d (bool): Whether to reshape inputs to 2-dim arrays, otherwise keep as-is.
    concat (bool): Whether to concatenate the results along the column axis.
        Otherwise, pairwise combine into a Series/DataFrame of the same shape.

        If True, see `vectorbtpro.base.combining.combine_and_concat`.
        If False, see `vectorbtpro.base.combining.combine_multiple`.
        If None, becomes True if there are multiple objects to combine.

        Can only concatenate using the instance method.
    keys (index_like): Outermost column level.
    broadcast_named_args (dict): Dictionary with arguments to broadcast against each other.
    broadcast_kwargs (dict): Keyword arguments passed to `vectorbtpro.base.reshaping.broadcast`.
    template_context (dict): Context used to substitute templates in `args` and `kwargs`.
    wrap_kwargs (dict): Keyword arguments passed to `vectorbtpro.base.wrapping.ArrayWrapper.wrap`.
    **kwargs: Keyword arguments passed to `combine_func`.

!!! note
    If `combine_func` is Numba-compiled, will broadcast using `WRITEABLE` and `C_CONTIGUOUS`
    flags, which can lead to an expensive computation overhead if passed objects are large and
    have different shape/memory order. You also must ensure that all objects have the same data type.

    Also remember to bring each in `*args` to a Numba-compatible format.

Usage:
    * Using instance method:

    ```pycon
    >>> sr = pd.Series([1, 2], index=['x', 'y'])
    >>> df = pd.DataFrame([[3, 4], [5, 6]], index=['x', 'y'], columns=['a', 'b'])

    >>> # using instance method
    >>> sr.vbt.combine(df, np.add)
       a  b
    x  4  5
    y  7  8

    >>> sr.vbt.combine([df, df * 2], np.add, concat=False)
        a   b
    x  10  13
    y  17  20

    >>> sr.vbt.combine([df, df * 2], np.add)
    combine_idx     0       1
                 a  b   a   b
    x            4  5   7   9
    y            7  8  12  14

    >>> sr.vbt.combine([df, df * 2], np.add, keys=['c', 'd'])
          c       d
       a  b   a   b
    x  4  5   7   9
    y  7  8  12  14

    >>> sr.vbt.combine(vbt.Param([1, 2], name='param'), np.add)
    param  1  2
    x      2  3
    y      3  4

    >>> # using class method
    >>> sr.vbt.combine([df, df * 2], np.add, concat=False)
        a   b
    x  10  13
    y  17  20
    ```

    * Using class method, templates, and broadcasting:

    ```pycon
    >>> sr = pd.Series([1, 2, 3], index=['x', 'y', 'z'])
    >>> sr.vbt.combine(
    ...     [1, 2, 3],
    ...     lambda x, y, z: x + y + z,
    ...     vbt.Rep('df'),
    ...     broadcast_named_args=dict(
    ...         df=pd.DataFrame([[1, 2, 3]], columns=['a', 'b', 'c'])
    ...     )
    ... )
    combine_idx        0        1        2
                 a  b  c  a  b  c  a  b  c
    x            3  4  5  4  5  6  5  6  7
    y            4  5  6  5  6  7  6  7  8
    z            5  6  7  6  7  8  7  8  9
    ```

    * To change the execution engine or specify other engine-related arguments, use `execute_kwargs`:

    ```pycon
    >>> import time

    >>> def combine_func(a, b):
    ...     time.sleep(1)
    ...     return a + b

    >>> sr = pd.Series([1, 2, 3])

    >>> %timeit sr.vbt.combine([1, 1, 1], combine_func)
    3.01 s ± 2.98 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

    >>> %timeit sr.vbt.combine([1, 1, 1], combine_func, execute_kwargs=dict(engine='dask'))
    1.02 s ± 2.18 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    ```
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: eval
```
Evaluate a simple array expression element-wise using NumExpr or NumPy.

If NumExpr is enables, only one-line statements are supported. Otherwise, uses
`vectorbtpro.utils.eval_.multiline_eval`.

!!! note
    All required variables will broadcast against each other prior to the evaluation.

Usage:
    ```pycon
    >>> sr = pd.Series([1, 2, 3], index=['x', 'y', 'z'])
    >>> df = pd.DataFrame([[4, 5, 6]], index=['x', 'y', 'z'], columns=['a', 'b', 'c'])
    >>> vbt.pd_acc.eval('sr + df')
       a  b  c
    x  5  6  7
    y  6  7  8
    z  7  8  9
    ```
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: split
```
Split using `vectorbtpro.generic.splitting.base.Splitter.split_and_take`.

Uses the option `into="reset_stacked"` by default.

!!! note
    Splits Pandas object, not accessor!
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: split_apply
```
Split using `vectorbtpro.generic.splitting.base.Splitter.split_and_apply`.

!!! note
    Splits Pandas object, not accessor!
```

---

# Pasta: base
### Arquivo: accessors.py
#### Classe: BaseAccessor
#### Função: items
```
See `vectorbtpro.base.wrapping.Wrapping.items`.

!!! note
    Splits Pandas object, not accessor!
```

---

# Pasta: base
### Arquivo: chunking.py
#### Docstring do Módulo
```
Extensions for chunking of base operations.
```

---

# Pasta: base
### Arquivo: chunking.py
#### Classe: GroupLensSizer
```
Class for getting the size from group lengths.

Argument can be either a group map tuple or a group lengths array.
```

---

# Pasta: base
### Arquivo: chunking.py
#### Classe: GroupLensSlicer
```
Class for slicing multiple elements from group lengths based on the chunk range.
```

---

# Pasta: base
### Arquivo: chunking.py
#### Classe: ChunkedGroupLens
```
Class representing chunkable group lengths.
```

---

# Pasta: base
### Arquivo: chunking.py
#### Função: get_group_lens_slice
```
Get slice of each chunk in group lengths.
```

---

# Pasta: base
### Arquivo: chunking.py
#### Classe: GroupLensMapper
```
Class for mapping chunk metadata to per-group column lengths.

Argument can be either a group map tuple or a group lengths array.
```

---

# Pasta: base
### Arquivo: chunking.py
#### Classe: GroupMapSlicer
```
Class for slicing multiple elements from a group map based on the chunk range.
```

---

# Pasta: base
### Arquivo: chunking.py
#### Classe: ChunkedGroupMap
```
Class representing a chunkable group map.
```

---

# Pasta: base
### Arquivo: chunking.py
#### Classe: GroupIdxsMapper
```
Class for mapping chunk metadata to per-group column indices.

Argument must be a group map tuple.
```

---

# Pasta: base
### Arquivo: chunking.py
#### Classe: FlexArraySizer
```
Class for getting the size from the length of an axis in a flexible array.
```

---

# Pasta: base
### Arquivo: chunking.py
#### Classe: FlexArraySelector
```
Class for selecting one element from a NumPy array's axis flexibly based on the chunk index.

The result is intended to be used together with `vectorbtpro.base.flex_indexing.flex_select_1d_nb`
and `vectorbtpro.base.flex_indexing.flex_select_nb`.
```

---

# Pasta: base
### Arquivo: chunking.py
#### Classe: FlexArraySlicer
```
Class for selecting one element from a NumPy array's axis flexibly based on the chunk index.

The result is intended to be used together with `vectorbtpro.base.flex_indexing.flex_select_1d_nb`
and `vectorbtpro.base.flex_indexing.flex_select_nb`.
```

---

# Pasta: base
### Arquivo: chunking.py
#### Classe: ChunkedFlexArray
```
Class representing a chunkable flexible array.
```

---

# Pasta: base
### Arquivo: chunking.py
#### Classe: GroupLensSizer
#### Função: get_obj_size
```
Get size of an object.
```

---

# Pasta: base
### Arquivo: chunking.py
#### Classe: FlexArraySizer
#### Função: get_obj_size
```
Get size of an object.
```

---

# Pasta: base
### Arquivo: combining.py
#### Docstring do Módulo
```
Functions for combining arrays.

Combine functions combine two or more NumPy arrays using a custom function. The emphasis here is
done upon stacking the results into one NumPy array - since vectorbt is all about brute-forcing
large spaces of hyper-parameters, concatenating the results of each hyper-parameter combination into
a single DataFrame is important. All functions are available in both Python and Numba-compiled form.
```

---

# Pasta: base
### Arquivo: combining.py
#### Função: custom_apply_and_concat_none_nb
```
Run `apply_func_nb` that returns nothing for each index.

Meant for in-place outputs.
```

---

# Pasta: base
### Arquivo: combining.py
#### Função: apply_and_concat_none_nb
```
Run `apply_func_nb` that returns nothing number of times.

Uses `custom_apply_and_concat_none_nb`.
```

---

# Pasta: base
### Arquivo: combining.py
#### Função: to_2d_one_nb
```
Expand the dimensions of the array along the axis 1.
```

---

# Pasta: base
### Arquivo: combining.py
#### Função: custom_apply_and_concat_one_nb
```
Run `apply_func_nb` that returns one array for each index.
```

---

# Pasta: base
### Arquivo: combining.py
#### Função: apply_and_concat_one_nb
```
Run `apply_func_nb` that returns one array number of times.

Uses `custom_apply_and_concat_one_nb`.
```

---

# Pasta: base
### Arquivo: combining.py
#### Função: to_2d_multiple_nb
```
Expand the dimensions of each array in `a` along axis 1.
```

---

# Pasta: base
### Arquivo: combining.py
#### Função: custom_apply_and_concat_multiple_nb
```
Run `apply_func_nb` that returns multiple arrays for each index.
```

---

# Pasta: base
### Arquivo: combining.py
#### Função: apply_and_concat_multiple_nb
```
Run `apply_func_nb` that returns multiple arrays number of times.

Uses `custom_apply_and_concat_multiple_nb`.
```

---

# Pasta: base
### Arquivo: combining.py
#### Função: apply_and_concat_each
```
Apply each function on its own set of positional and keyword arguments.

Executes the function using `vectorbtpro.utils.execution.execute`.
```

---

# Pasta: base
### Arquivo: combining.py
#### Função: apply_and_concat
```
Run `apply_func` function a number of times and concatenate the results depending upon how
many array-like objects it generates.

`apply_func` must accept arguments `i`, `*args`, and `**kwargs`.

Set `jitted_loop` to True to use the JIT-compiled version.

All jitted iteration functions are resolved using `vectorbtpro.registries.jit_registry.JITRegistry.resolve`.

!!! note
    `n_outputs` must be set when `jitted_loop` is True.

    Numba doesn't support variable keyword arguments.
```

---

# Pasta: base
### Arquivo: combining.py
#### Função: select_and_combine_nb
```
Numba-compiled version of `select_and_combine`.
```

---

# Pasta: base
### Arquivo: combining.py
#### Função: combine_and_concat_nb
```
Numba-compiled version of `combine_and_concat`.
```

---

# Pasta: base
### Arquivo: combining.py
#### Função: select_and_combine
```
Combine `obj` with an array at position `i` in `others` using `combine_func`.
```

---

# Pasta: base
### Arquivo: combining.py
#### Função: combine_and_concat
```
Combine `obj` with each in `others` using `combine_func` and concatenate.

`select_and_combine_nb` is resolved using `vectorbtpro.registries.jit_registry.JITRegistry.resolve`.
```

---

# Pasta: base
### Arquivo: combining.py
#### Função: combine_multiple_nb
```
Numba-compiled version of `combine_multiple`.
```

---

# Pasta: base
### Arquivo: combining.py
#### Função: combine_multiple
```
Combine `objs` pairwise into a single object.

Set `jitted_loop` to True to use the JIT-compiled version.

`combine_multiple_nb` is resolved using `vectorbtpro.registries.jit_registry.JITRegistry.resolve`.

!!! note
    Numba doesn't support variable keyword arguments.
```

---

# Pasta: base
### Arquivo: decorators.py
#### Docstring do Módulo
```
Class decorators for base classes.
```

---

# Pasta: base
### Arquivo: decorators.py
#### Função: override_arg_config
```
Class decorator to override the argument config of a class subclassing
`vectorbtpro.base.preparing.BasePreparer`.

Instead of overriding `_arg_config` class attribute, you can pass `config` directly to this decorator.

Disable `merge_configs` to not merge, which will effectively disable field inheritance.
```

---

# Pasta: base
### Arquivo: decorators.py
#### Função: attach_arg_properties
```
Class decorator to attach properties for arguments defined in the argument config
of a `vectorbtpro.base.preparing.BasePreparer` subclass.
```

---

# Pasta: base
### Arquivo: flex_indexing.py
#### Docstring do Módulo
```
Classes and functions for flexible indexing.
```

---

# Pasta: base
### Arquivo: flex_indexing.py
#### Função: flex_choose_i_1d_nb
```
Choose a position in an array as if it has been broadcast against rows or columns.

!!! note
    Array must be one-dimensional.
```

---

# Pasta: base
### Arquivo: flex_indexing.py
#### Função: flex_select_1d_nb
```
Select an element of an array as if it has been broadcast against rows or columns.

!!! note
    Array must be one-dimensional.
```

---

# Pasta: base
### Arquivo: flex_indexing.py
#### Função: flex_choose_i_pr_1d_nb
```
Choose a position in an array as if it has been broadcast against rows.

Can use rotational indexing along rows.

!!! note
    Array must be one-dimensional.
```

---

# Pasta: base
### Arquivo: flex_indexing.py
#### Função: flex_choose_i_pr_nb
```
Choose a position in an array as if it has been broadcast against rows.

Can use rotational indexing along rows.

!!! note
    Array must be two-dimensional.
```

---

# Pasta: base
### Arquivo: flex_indexing.py
#### Função: flex_select_1d_pr_nb
```
Select an element of an array as if it has been broadcast against rows.

Can use rotational indexing along rows.

!!! note
    Array must be one-dimensional.
```

---

# Pasta: base
### Arquivo: flex_indexing.py
#### Função: flex_choose_i_pc_1d_nb
```
Choose a position in an array as if it has been broadcast against columns.

Can use rotational indexing along columns.

!!! note
    Array must be one-dimensional.
```

---

# Pasta: base
### Arquivo: flex_indexing.py
#### Função: flex_choose_i_pc_nb
```
Choose a position in an array as if it has been broadcast against columns.

Can use rotational indexing along columns.

!!! note
    Array must be two-dimensional.
```

---

# Pasta: base
### Arquivo: flex_indexing.py
#### Função: flex_select_1d_pc_nb
```
Select an element of an array as if it has been broadcast against columns.

Can use rotational indexing along columns.

!!! note
    Array must be one-dimensional.
```

---

# Pasta: base
### Arquivo: flex_indexing.py
#### Função: flex_choose_i_and_col_nb
```
Choose a position in an array as if it has been broadcast rows and columns.

Can use rotational indexing along rows and columns.

!!! note
    Array must be two-dimensional.
```

---

# Pasta: base
### Arquivo: flex_indexing.py
#### Função: flex_select_nb
```
Select element of an array as if it has been broadcast rows and columns.

Can use rotational indexing along rows and columns.

!!! note
    Array must be two-dimensional.
```

---

# Pasta: base
### Arquivo: flex_indexing.py
#### Função: flex_select_row_nb
```
Select a row from a flexible 2-dim array. Returns a 1-dim array.

!!! note
    Array must be two-dimensional.
```

---

# Pasta: base
### Arquivo: flex_indexing.py
#### Função: flex_select_col_nb
```
Select a column from a flexible 2-dim array. Returns a 1-dim array.

!!! note
    Array must be two-dimensional.
```

---

# Pasta: base
### Arquivo: flex_indexing.py
#### Função: flex_select_2d_row_nb
```
Select a row from a flexible 2-dim array. Returns a 2-dim array.

!!! note
    Array must be two-dimensional.
```

---

# Pasta: base
### Arquivo: flex_indexing.py
#### Função: flex_select_2d_col_nb
```
Select a column from a flexible 2-dim array. Returns a 2-dim array.

!!! note
    Array must be two-dimensional.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Docstring do Módulo
```
Functions for working with indexes: index and columns.

They perform operations on index objects, such as stacking, combining, and cleansing MultiIndex levels.

!!! note
    "Index" in pandas context is referred to both index and columns.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Classe: ExceptLevel
```
Class for grouping except one or more levels.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: to_any_index
```
Convert any index-like object to an index.

Index objects are kept as-is.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: get_index
```
Get index of `arg` by `axis`.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: index_from_values
```
Create a new `pd.Index` with `name` by parsing an iterable `values`.

Each in `values` will correspond to an element in the new index.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: repeat_index
```
Repeat each element in `index` `n` times.

Set `ignore_ranges` to True to ignore indexes of type `pd.RangeIndex`.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: tile_index
```
Tile the whole `index` `n` times.

Set `ignore_ranges` to True to ignore indexes of type `pd.RangeIndex`.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: clean_index
```
Clean index.

Set `drop_duplicates` to True to remove duplicate levels.

For details on `keep`, see `drop_duplicate_levels`.

Set `drop_redundant` to True to use `drop_redundant_levels`.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: stack_indexes
```
Stack each index in `indexes` on top of each other, from top to bottom.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: combine_indexes
```
Combine each index in `indexes` using Cartesian product.

Keyword arguments will be passed to `stack_indexes`.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: combine_index_with_keys
```
Build keys based on index lengths.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: concat_indexes
```
Concatenate indexes.

The following index concatenation methods are supported:

* 'append': append one index to another
* 'union': build a union of indexes
* 'pd_concat': convert indexes to Pandas Series or DataFrames and use `pd.concat`
* 'factorize': factorize the concatenated index
* 'factorize_each': factorize each index and concatenate while keeping numbers unique
* 'reset': reset the concatenated index without applying `keys`
* Callable: a custom callable that takes the indexes and returns the concatenated index

Argument `index_concat_method` also accepts a tuple of two options: the second option gets applied
if the first one fails.

Use `keys` as an index with the same number of elements as there are indexes to add
another index level on top of the concatenated indexes.

If `verify_integrity` is True and `keys` is None, performs various checks depending on the axis.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: drop_levels
```
Drop `levels` in `index` by their name(s)/position(s).

Provide `levels` as an instance of `ExceptLevel` to drop everything apart from the specified levels.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: rename_levels
```
Rename levels in `index` by `mapper`.

Mapper can be a single or multiple levels to rename to, or a dictionary that maps
old level names to new level names.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: select_levels
```
Build a new index by selecting one or multiple `levels` from `index`.

Provide `levels` as an instance of `ExceptLevel` to select everything apart from the specified levels.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: drop_redundant_levels
```
Drop levels in `index` that either have a single unnamed value or a range from 0 to n.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: drop_duplicate_levels
```
Drop levels in `index` with the same name and values.

Set `keep` to 'last' to keep last levels, otherwise 'first'.

Set `keep` to None to use the default.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: align_arr_indices_nb
```
Return indices required to align `a` to `b`.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: align_index_to
```
Align `index1` to have the same shape as `index2` if they have any levels in common.

Returns index slice for the aligning.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: align_indexes
```
Align multiple indexes to each other with `align_index_to`.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: block_index_product_nb
```
Return indices required for building a block-wise Cartesian product of two factorized indexes.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: cross_index_with
```
Build a Cartesian product of one index with another while taking into account levels they have in common.

Returns index slices for the aligning.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: cross_indexes
```
Cross multiple indexes with `cross_index_with`.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: pick_levels
```
Pick optional and required levels and return their indices.

Raises an exception if index has less or more levels than expected.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Função: find_first_occurrence
```
Return index of the first occurrence in `index`.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Classe: IndexApplier
```
Abstract class that can apply a function on an index.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Classe: IndexApplier
#### Função: apply_to_index
```
Apply function `apply_func` on the index of the instance and return a new instance.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Classe: IndexApplier
#### Função: add_levels
```
Append or prepend levels using `stack_indexes`.

Set `on_top` to False to stack at bottom.

See `IndexApplier.apply_to_index` for other keyword arguments.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Classe: IndexApplier
#### Função: drop_levels
```
Drop levels using `drop_levels`.

See `IndexApplier.apply_to_index` for other keyword arguments.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Classe: IndexApplier
#### Função: rename_levels
```
Rename levels using `rename_levels`.

See `IndexApplier.apply_to_index` for other keyword arguments.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Classe: IndexApplier
#### Função: select_levels
```
Select levels using `select_levels`.

See `IndexApplier.apply_to_index` for other keyword arguments.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Classe: IndexApplier
#### Função: drop_redundant_levels
```
Drop any redundant levels using `drop_redundant_levels`.

See `IndexApplier.apply_to_index` for other keyword arguments.
```

---

# Pasta: base
### Arquivo: indexes.py
#### Classe: IndexApplier
#### Função: drop_duplicate_levels
```
Drop any duplicate levels using `drop_duplicate_levels`.

See `IndexApplier.apply_to_index` for other keyword arguments.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Docstring do Módulo
```
Classes and functions for indexing.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IndexingError
```
Exception raised when an indexing error has occurred.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IndexingBase
```
Class that supports indexing through `IndexingBase.indexing_func`.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: LocBase
```
Class that implements location-based indexing.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: pdLoc
```
Forwards a Pandas-like indexing operation to each Series/DataFrame and returns a new class instance.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: iLoc
```
Forwards `pd.Series.iloc`/`pd.DataFrame.iloc` operation to each
Series/DataFrame and returns a new class instance.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: Loc
```
Forwards `pd.Series.loc`/`pd.DataFrame.loc` operation to each
Series/DataFrame and returns a new class instance.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: PandasIndexer
```
Implements indexing using `iloc`, `loc`, `xs` and `__getitem__`.

Usage:
    ```pycon
    >>> from vectorbtpro import *
    >>> from vectorbtpro.base.indexing import PandasIndexer

    >>> class C(PandasIndexer):
    ...     def __init__(self, df1, df2):
    ...         self.df1 = df1
    ...         self.df2 = df2
    ...         super().__init__()
    ...
    ...     def indexing_func(self, pd_indexing_func):
    ...         return type(self)(
    ...             pd_indexing_func(self.df1),
    ...             pd_indexing_func(self.df2)
    ...         )

    >>> df1 = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})
    >>> df2 = pd.DataFrame({'a': [5, 6], 'b': [7, 8]})
    >>> c = C(df1, df2)

    >>> c.iloc[:, 0]
    <__main__.C object at 0x1a1cacbbe0>

    >>> c.iloc[:, 0].df1
    0    1
    1    2
    Name: a, dtype: int64

    >>> c.iloc[:, 0].df2
    0    5
    1    6
    Name: a, dtype: int64
    ```
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: xLoc
```
Subclass of `iLoc` that transforms an `Idxr`-based operation with
`get_idxs` to an `iLoc` operation.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: ExtPandasIndexer
```
Extension of `PandasIndexer` that also implements indexing using `xLoc`.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: ParamLoc
```
Access a group of columns by parameter using `pd.Series.loc`.

Uses `mapper` to establish link between columns and parameter values.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Função: indexing_on_mapper
```
Broadcast `mapper` Series to `ref_obj` and perform pandas indexing using `pd_indexing_func`.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Função: build_param_indexer
```
A factory to create a class with parameter indexing.

Parameter indexer enables accessing a group of rows and columns by a parameter array (similar to `loc`).
This way, one can query index/columns by another Series called a parameter mapper, which is just a
`pd.Series` that maps columns (its index) to params (its values).

Parameter indexing is important, since querying by column/index labels alone is not always the best option.
For example, `pandas` doesn't let you query by list at a specific index/column level.

Args:
    param_names (list of str): Names of the parameters.
    class_name (str): Name of the generated class.
    module_name (str): Name of the module to which the class should be bound.

Usage:
    ```pycon
    >>> from vectorbtpro import *
    >>> from vectorbtpro.base.indexing import build_param_indexer, indexing_on_mapper

    >>> MyParamIndexer = build_param_indexer(['my_param'])
    >>> class C(MyParamIndexer):
    ...     def __init__(self, df, param_mapper):
    ...         self.df = df
    ...         self._my_param_mapper = param_mapper
    ...         super().__init__([param_mapper])
    ...
    ...     def indexing_func(self, pd_indexing_func):
    ...         return type(self)(
    ...             pd_indexing_func(self.df),
    ...             indexing_on_mapper(self._my_param_mapper, self.df, pd_indexing_func)
    ...         )

    >>> df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})
    >>> param_mapper = pd.Series(['First', 'Second'], index=['a', 'b'])
    >>> c = C(df, param_mapper)

    >>> c.my_param_loc['First'].df
    0    1
    1    2
    Name: a, dtype: int64

    >>> c.my_param_loc['Second'].df
    0    3
    1    4
    Name: b, dtype: int64

    >>> c.my_param_loc[['First', 'First', 'Second', 'Second']].df
          a     b
    0  1  1  3  3
    1  2  2  4  4
    ```
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: hslice
```
Hashable slice.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IdxrBase
```
Abstract class for resolving indices.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Função: normalize_idxs
```
Normalize indexes into a 1-dim integer array.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: UniIdxr
```
Abstract class for resolving indices based on a single index.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: UniIdxrOp
```
Class for applying an operation to one or more indexers.

Produces a single set of indices.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: PosIdxr
```
Class for resolving indices provided as integer positions.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: MaskIdxr
```
Class for resolving indices provided as a mask.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: LabelIdxr
```
Class for resolving indices provided as labels.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: DatetimeIdxr
```
Class for resolving indices provided as datetime-like objects.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: DTCIdxr
```
Class for resolving indices provided as datetime-like components.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: PointIdxr
```
Class for resolving index points.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Função: get_index_points
```
Translate indices or labels into index points.

See `PointIdxr` for arguments.

Usage:
    * Provide nothing to generate at the beginning:

    ```pycon
    >>> from vectorbtpro import *

    >>> index = pd.date_range("2020-01", "2020-02", freq="1d")

    >>> vbt.get_index_points(index)
    array([0])
    ```

    * Provide `every` as an integer frequency to generate index points using NumPy:

    ```pycon
    >>> # Generate a point every five rows
    >>> vbt.get_index_points(index, every=5)
    array([ 0,  5, 10, 15, 20, 25, 30])

    >>> # Generate a point every five rows starting at 6th row
    >>> vbt.get_index_points(index, every=5, start=5)
    array([ 5, 10, 15, 20, 25, 30])

    >>> # Generate a point every five rows from 6th to 16th row
    >>> vbt.get_index_points(index, every=5, start=5, end=15)
    array([ 5, 10])
    ```

    * Provide `every` as a time delta frequency to generate index points using Pandas:

    ```pycon
    >>> # Generate a point every week
    >>> vbt.get_index_points(index, every="W")
    array([ 4, 11, 18, 25])

    >>> # Generate a point every second day of the week
    >>> vbt.get_index_points(index, every="W", add_delta="2d")
    array([ 6, 13, 20, 27])

    >>> # Generate a point every week, starting at 11th row
    >>> vbt.get_index_points(index, every="W", start=10)
    array([11, 18, 25])

    >>> # Generate a point every week, starting exactly at 11th row
    >>> vbt.get_index_points(index, every="W", start=10, exact_start=True)
    array([10, 11, 18, 25])

    >>> # Generate a point every week, starting at 2020-01-10
    >>> vbt.get_index_points(index, every="W", start="2020-01-10")
    array([11, 18, 25])
    ```

    * Instead of using `every`, provide indices explicitly:

    ```pycon
    >>> # Generate one point
    >>> vbt.get_index_points(index, on="2020-01-07")
    array([6])

    >>> # Generate multiple points
    >>> vbt.get_index_points(index, on=["2020-01-07", "2020-01-14"])
    array([ 6, 13])
    ```
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: RangeIdxr
```
Class for resolving index ranges.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Função: get_index_ranges
```
Translate indices, labels, or bounds into index ranges.

See `RangeIdxr` for arguments.

Usage:
    * Provide nothing to generate one largest index range:

    ```pycon
    >>> from vectorbtpro import *

    >>> index = pd.date_range("2020-01", "2020-02", freq="1d")

    >>> np.column_stack(vbt.get_index_ranges(index))
    array([[ 0, 32]])
    ```

    * Provide `every` as an integer frequency to generate index ranges using NumPy:

    ```pycon
    >>> # Generate a range every five rows
    >>> np.column_stack(vbt.get_index_ranges(index, every=5))
    array([[ 0,  5],
           [ 5, 10],
           [10, 15],
           [15, 20],
           [20, 25],
           [25, 30]])

    >>> # Generate a range every five rows, starting at 6th row
    >>> np.column_stack(vbt.get_index_ranges(
    ...     index,
    ...     every=5,
    ...     start=5
    ... ))
    array([[ 5, 10],
           [10, 15],
           [15, 20],
           [20, 25],
           [25, 30]])

    >>> # Generate a range every five rows from 6th to 16th row
    >>> np.column_stack(vbt.get_index_ranges(
    ...     index,
    ...     every=5,
    ...     start=5,
    ...     end=15
    ... ))
    array([[ 5, 10],
           [10, 15]])
    ```

    * Provide `every` as a time delta frequency to generate index ranges using Pandas:

    ```pycon
    >>> # Generate a range every week
    >>> np.column_stack(vbt.get_index_ranges(index, every="W"))
    array([[ 4, 11],
           [11, 18],
           [18, 25]])

    >>> # Generate a range every second day of the week
    >>> np.column_stack(vbt.get_index_ranges(
    ...     index,
    ...     every="W",
    ...     add_start_delta="2d"
    ... ))
    array([[ 6, 11],
           [13, 18],
           [20, 25]])

    >>> # Generate a range every week, starting at 11th row
    >>> np.column_stack(vbt.get_index_ranges(
    ...     index,
    ...     every="W",
    ...     start=10
    ... ))
    array([[11, 18],
           [18, 25]])

    >>> # Generate a range every week, starting exactly at 11th row
    >>> np.column_stack(vbt.get_index_ranges(
    ...     index,
    ...     every="W",
    ...     start=10,
    ...     exact_start=True
    ... ))
    array([[10, 11],
           [11, 18],
           [18, 25]])

    >>> # Generate a range every week, starting at 2020-01-10
    >>> np.column_stack(vbt.get_index_ranges(
    ...     index,
    ...     every="W",
    ...     start="2020-01-10"
    ... ))
    array([[11, 18],
           [18, 25]])

    >>> # Generate a range every week, each starting at 2020-01-10
    >>> np.column_stack(vbt.get_index_ranges(
    ...     index,
    ...     every="W",
    ...     start="2020-01-10",
    ...     fixed_start=True
    ... ))
    array([[11, 18],
           [11, 25]])

    >>> # Generate an expanding range that increments by week
    >>> np.column_stack(vbt.get_index_ranges(
    ...     index,
    ...     every="W",
    ...     start=0,
    ...     exact_start=True,
    ...     fixed_start=True
    ... ))
    array([[ 0,  4],
           [ 0, 11],
           [ 0, 18],
           [ 0, 25]])
    ```

    * Use a look-back period (instead of an end index):

    ```pycon
    >>> # Generate a range every week, looking 5 days back
    >>> np.column_stack(vbt.get_index_ranges(
    ...     index,
    ...     every="W",
    ...     lookback_period=5
    ... ))
    array([[ 6, 11],
           [13, 18],
           [20, 25]])

    >>> # Generate a range every week, looking 2 weeks back
    >>> np.column_stack(vbt.get_index_ranges(
    ...     index,
    ...     every="W",
    ...     lookback_period="2W"
    ... ))
    array([[ 0, 11],
           [ 4, 18],
           [11, 25]])
    ```

    * Instead of using `every`, provide start and end indices explicitly:

    ```pycon
    >>> # Generate one range
    >>> np.column_stack(vbt.get_index_ranges(
    ...     index,
    ...     start="2020-01-01",
    ...     end="2020-01-07"
    ... ))
    array([[0, 6]])

    >>> # Generate ranges between multiple dates
    >>> np.column_stack(vbt.get_index_ranges(
    ...     index,
    ...     start=["2020-01-01", "2020-01-07"],
    ...     end=["2020-01-07", "2020-01-14"]
    ... ))
    array([[ 0,  6],
           [ 6, 13]])

    >>> # Generate ranges with a fixed start
    >>> np.column_stack(vbt.get_index_ranges(
    ...     index,
    ...     start="2020-01-01",
    ...     end=["2020-01-07", "2020-01-14"]
    ... ))
    array([[ 0,  6],
           [ 0, 13]])
    ```

    * Use `closed_start` and `closed_end` to exclude any of the bounds:

    ```pycon
    >>> # Generate ranges between multiple dates
    >>> # by excluding the start date and including the end date
    >>> np.column_stack(vbt.get_index_ranges(
    ...     index,
    ...     start=["2020-01-01", "2020-01-07"],
    ...     end=["2020-01-07", "2020-01-14"],
    ...     closed_start=False,
    ...     closed_end=True
    ... ))
    array([[ 1,  7],
           [ 7, 14]])
    ```
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: AutoIdxr
```
Class for resolving indices, datetime-like objects, frequency-like objects, and labels for one axis.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: RowIdxr
```
Class for resolving row indices.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: ColIdxr
```
Class for resolving column indices.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: Idxr
```
Class for resolving indices.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Função: get_idxs
```
Translate indexer to row and column indices.

If `idxr` is not an indexer class, wraps it with `Idxr`.

Keyword arguments are passed when constructing a new `Idxr`.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: index_dict
```
Dict that contains indexer objects as keys and values to be set as values.

Each indexer object must be hashable. To make a slice hashable, use `hslice`.
To make an array hashable, convert it into a tuple.

To set a default value, use the `_def` key (case-sensitive!).
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IdxSetter
```
Class for setting values based on indexing.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IdxSetterFactory
```
Class for building index setters.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IdxDict
```
Class for building an index setter from a dict.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IdxSeries
```
Class for building an index setter from a Series.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IdxFrame
```
Class for building an index setter from a DataFrame.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IdxRecords
```
Class for building index setters from records - one per field.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IndexingBase
#### Função: indexing_func
```
Apply `pd_indexing_func` on all pandas objects in question and return a new instance of the class.

Should be overridden.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IndexingBase
#### Função: indexing_setter_func
```
Apply `pd_indexing_setter_func` on all pandas objects in question.

Should be overridden.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: LocBase
#### Função: indexing_func
```
Indexing function.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: LocBase
#### Função: indexing_setter_func
```
Indexing setter function.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: LocBase
#### Função: indexing_kwargs
```
Keyword arguments passed to `LocBase.indexing_func`.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: pdLoc
#### Função: pd_indexing_func
```
Pandas-like indexing operation.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: pdLoc
#### Função: pd_indexing_setter_func
```
Pandas-like indexing setter operation.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: PandasIndexer
#### Função: indexing_kwargs
```
Indexing keyword arguments.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: PandasIndexer
#### Função: iloc
```
Purely integer-location based indexing for selection by position.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: PandasIndexer
#### Função: loc
```
Purely label-location based indexer for selection by label.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: PandasIndexer
#### Função: xs
```
Forwards `pd.Series.xs`/`pd.DataFrame.xs`
operation to each Series/DataFrame and returns a new class instance.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: ExtPandasIndexer
#### Função: xloc
```
`Idxr`-based indexing.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: ParamLoc
#### Função: mapper
```
Mapper.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: ParamLoc
#### Função: level_name
```
Level name.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: ParamLoc
#### Função: get_idxs
```
Get array of indices affected by this key.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: hslice
#### Função: from_slice
```
Construct from a slice.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: hslice
#### Função: to_slice
```
Convert to a slice.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IdxrBase
#### Função: get
```
Get indices.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IdxrBase
#### Função: slice_indexer
```
Compute the slice indexer for input labels and step.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IdxrBase
#### Função: check_idxs
```
Check indices after resolving them.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: DTCIdxr
#### Função: get_dtc_namedtuple
```
Convert a value to a `vectorbtpro.utils.datetime_.DTCNT` instance.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IdxSetter
#### Função: set_row_idxs
```
Set row indices in an array.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IdxSetter
#### Função: set_col_idxs
```
Set column indices in an array.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IdxSetter
#### Função: set_row_and_col_idxs
```
Set row and column indices in an array.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IdxSetter
#### Função: get_set_meta
```
Get meta of setting operations in `IdxSetter.idx_items`.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IdxSetter
#### Função: set
```
Set values of a NumPy array based on `IdxSetter.get_set_meta`.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IdxSetter
#### Função: set_pd
```
Set values of a Pandas array based on `IdxSetter.get_set_meta`.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IdxSetter
#### Função: fill_and_set
```
Fill a new array and set its values based on `IdxSetter.get_set_meta`.

If `keep_flex` is True, will return the most memory-efficient array representation
capable of flexible indexing.

If `fill_value` is None, will search for the `_def` key in `IdxSetter.idx_items`.
If there's none, will be set to NaN.
```

---

# Pasta: base
### Arquivo: indexing.py
#### Classe: IdxSetterFactory
#### Função: get
```
Get an instance of `IdxSetter` or a dict of such instances - one per array name.
```

---

# Pasta: base
### Arquivo: merging.py
#### Docstring do Módulo
```
Functions for merging arrays.
```

---

# Pasta: base
### Arquivo: merging.py
#### Função: concat_arrays
```
Concatenate arrays.
```

---

# Pasta: base
### Arquivo: merging.py
#### Função: row_stack_arrays
```
Stack arrays along rows.
```

---

# Pasta: base
### Arquivo: merging.py
#### Função: column_stack_arrays
```
Stack arrays along columns.
```

---

# Pasta: base
### Arquivo: merging.py
#### Função: concat_merge
```
Merge multiple array-like objects through concatenation.

Supports a sequence of tuples.

If `wrap` is None, it will become True if `wrapper`, `keys`, or `wrap_kwargs` are not None.
If `wrap` is True, each array will be wrapped with Pandas Series and merged using `pd.concat`.
Otherwise, arrays will be kept as-is and merged using `concat_arrays`.
`wrap_kwargs` can be a dictionary or a list of dictionaries.

If `wrapper` is provided, will use `vectorbtpro.base.wrapping.ArrayWrapper.wrap_reduced`.

Keyword arguments `**kwargs` are passed to `pd.concat` only.

!!! note
    All arrays are assumed to have the same type and dimensionality.
```

---

# Pasta: base
### Arquivo: merging.py
#### Função: row_stack_merge
```
Merge multiple array-like or `vectorbtpro.base.wrapping.Wrapping` objects through row stacking.

Supports a sequence of tuples.

Argument `wrap` supports the following options:

* None: will become True if `wrapper`, `keys`, or `wrap_kwargs` are not None
* True: each array will be wrapped with Pandas Series/DataFrame (depending on dimensions)
* 'sr', 'series': each array will be wrapped with Pandas Series
* 'df', 'frame', 'dataframe': each array will be wrapped with Pandas DataFrame

Without wrapping, arrays will be kept as-is and merged using `row_stack_arrays`.
Argument `wrap_kwargs` can be a dictionary or a list of dictionaries.

If `wrapper` is provided, will use `vectorbtpro.base.wrapping.ArrayWrapper.wrap`.

Keyword arguments `**kwargs` are passed to `pd.concat` and
`vectorbtpro.base.wrapping.Wrapping.row_stack` only.

!!! note
    All arrays are assumed to have the same type and dimensionality.
```

---

# Pasta: base
### Arquivo: merging.py
#### Função: column_stack_merge
```
Merge multiple array-like or `vectorbtpro.base.wrapping.Wrapping` objects through column stacking.

Supports a sequence of tuples.

Argument `wrap` supports the following options:

* None: will become True if `wrapper`, `keys`, or `wrap_kwargs` are not None
* True: each array will be wrapped with Pandas Series/DataFrame (depending on dimensions)
* 'sr', 'series': each array will be wrapped with Pandas Series
* 'df', 'frame', 'dataframe': each array will be wrapped with Pandas DataFrame

Without wrapping, arrays will be kept as-is and merged using `column_stack_arrays`.
Argument `wrap_kwargs` can be a dictionary or a list of dictionaries.

If `wrapper` is provided, will use `vectorbtpro.base.wrapping.ArrayWrapper.wrap`.

Keyword arguments `**kwargs` are passed to `pd.concat` and
`vectorbtpro.base.wrapping.Wrapping.column_stack` only.

Argument `reset_index` supports the following options:

* False or None: Keep original index of each object
* True or 'from_start': Reset index of each object and align them at start
* 'from_end': Reset index of each object and align them at end

Options above work on Pandas, NumPy, and `vectorbtpro.base.wrapping.Wrapping` instances.

!!! note
    All arrays are assumed to have the same type and dimensionality.
```

---

# Pasta: base
### Arquivo: merging.py
#### Função: imageio_merge
```
Merge multiple figure-like objects by writing them with `imageio`.

Keyword arguments `to_image_kwargs` are passed to `plotly.graph_objects.Figure.to_image`.
Keyword arguments `imread_kwargs` and `**imwrite_kwargs` are passed to
`imageio.imread` and `imageio.imwrite` respectively.

Keys are not used in any way.
```

---

# Pasta: base
### Arquivo: merging.py
#### Função: mixed_merge
```
Merge objects of mixed types.
```

---

# Pasta: base
### Arquivo: merging.py
#### Função: resolve_merge_func
```
Resolve a merging function into a callable.

If a string, looks up into `merge_func_config`. If a sequence, uses `mixed_merge` with
`merge_funcs=merge_func`. If an instance of `vectorbtpro.utils.merging.MergeFunc`, calls
`vectorbtpro.utils.merging.MergeFunc.resolve_merge_func` to get the actual callable.
```

---

# Pasta: base
### Arquivo: merging.py
#### Função: is_merge_func_from_config
```
Return whether the merging function can be found in `merge_func_config`.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Docstring do Módulo
```
Classes for preparing arguments.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: MetaArgs
```
Meta class that exposes a read-only class property `MetaArgs.arg_config`.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
```
Base class for preparing target functions and arguments.

!!! warning
    Most properties are force-cached - create a new instance to override any attribute.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: MetaArgs
#### Função: arg_config
```
Argument config.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: arg_config
```
Argument config of `${cls_name}`.

```python
${arg_config}
```
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: map_enum_value
```
Map enumerated value(s).
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: prepare_td_obj
```
Prepare a timedelta object for broadcasting.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: prepare_dt_obj
```
Prepare a datetime object for broadcasting.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: get_raw_arg_default
```
Get raw argument default.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: get_raw_arg
```
Get raw argument.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: idx_setters
```
Index setters from resolving the argument `records`.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: get_arg_default
```
Get argument default according to the argument config.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: get_arg
```
Get mapped argument according to the argument config.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: prepare_td_arr
```
Prepare a timedelta array.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: prepare_dt_arr
```
Prepare a datetime array.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: td_arr_to_ns
```
Prepare a timedelta array and convert it to nanoseconds.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: dt_arr_to_ns
```
Prepare a datetime array and convert it to nanoseconds.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: prepare_post_arg
```
Prepare an argument after broadcasting and/or template substitution.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: adapt_staticized_to_udf
```
Adapt `staticized` dictionary to a UDF.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: find_target_func
```
Find target function by its name.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: resolve_dynamic_target_func
```
Resolve a dynamic target function.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: set_seed
```
Set seed.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: _pre_template_context
```
Argument `template_context` before broadcasting.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: pre_args
```
Arguments before broadcasting.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: args_to_broadcast
```
Arguments to broadcast.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: def_broadcast_kwargs
```
Default keyword arguments for broadcasting.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: broadcast_kwargs
```
Argument `broadcast_kwargs`.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: broadcast_result
```
Result of broadcasting.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: post_args
```
Arguments after broadcasting.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: post_broadcast_named_args
```
Custom arguments after broadcasting.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: wrapper
```
Array wrapper.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: target_shape
```
Target shape.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: index
```
Index in nanosecond format.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: freq
```
Frequency in nanosecond format.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: template_context
```
Argument `template_context`.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: target_func
```
Target function.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: target_arg_map
```
Map of the target arguments to the preparer attributes.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: target_args
```
Arguments to be passed to the target function.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: build_arg_config_doc
```
Build argument config documentation.
```

---

# Pasta: base
### Arquivo: preparing.py
#### Classe: BasePreparer
#### Função: override_arg_config_doc
```
Call this method on each subclass that overrides `BasePreparer.arg_config`.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Docstring do Módulo
```
Functions for reshaping arrays.

Reshape functions transform a Pandas object/NumPy array in some way.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: to_tuple_shape
```
Convert a shape-like object to a tuple.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: to_1d_shape
```
Convert a shape-like object to a 1-dim shape.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: to_2d_shape
```
Convert a shape-like object to a 2-dim shape.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: repeat_shape
```
Repeat shape `n` times along the specified axis.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: tile_shape
```
Tile shape `n` times along the specified axis.

Identical to `repeat_shape`. Exists purely for naming consistency.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: index_to_series
```
Convert Index to Series.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: index_to_frame
```
Convert Index to DataFrame.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: mapping_to_series
```
Convert a mapping-like object to Series.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: to_any_array
```
Convert any array-like object to an array.

Pandas objects are kept as-is unless `raw` is True.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: to_pd_array
```
Convert any array-like object to a Pandas object.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: soft_to_ndim
```
Try to softly bring `arg` to the specified number of dimensions `ndim` (max 2).
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: to_1d
```
Reshape argument to one dimension.

If `raw` is True, returns NumPy array.
If 2-dim, will collapse along axis 1 (i.e., DataFrame with one column to Series).
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: to_2d
```
Reshape argument to two dimensions.

If `raw` is True, returns NumPy array.
If 1-dim, will expand along axis 1 (i.e., Series to DataFrame with one column).
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: to_1d_array_nb
```
Resize array to one dimension.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: to_2d_array_nb
```
Resize array to two dimensions.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: to_2d_pr_array_nb
```
`to_2d_array_nb` with `expand_axis=1`.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: to_2d_pc_array_nb
```
`to_2d_array_nb` with `expand_axis=0`.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: to_dict
```
Convert object to dict.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: repeat
```
Repeat `arg` `n` times along the specified axis.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: tile
```
Tile `arg` `n` times along the specified axis.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: broadcast_shapes
```
Broadcast shape-like objects using vectorbt's broadcasting rules.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: broadcast_array_to
```
Broadcast an array-like object to a target shape using vectorbt's broadcasting rules.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: broadcast_arrays
```
Broadcast array-like objects using vectorbt's broadcasting rules.

Optionally to a target shape.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: broadcast_index
```
Produce a broadcast index/columns.

Args:
    args (iterable of array_like): Array-like objects.
    to_shape (tuple of int): Target shape.
    index_from (any): Broadcasting rule for this index/these columns.

        Accepts the following values:

        * 'keep' or None - keep the original index/columns of the objects in `args`
        * 'stack' - stack different indexes/columns using `vectorbtpro.base.indexes.stack_indexes`
        * 'strict' - ensure that all Pandas objects have the same index/columns
        * 'reset' - reset any index/columns (they become a simple range)
        * integer - use the index/columns of the i-th object in `args`
        * everything else will be converted to `pd.Index`
    axis (int): Set to 0 for index and 1 for columns.
    ignore_sr_names (bool): Whether to ignore Series names if they are in conflict.

        Conflicting Series names are those that are different but not None.
    ignore_ranges (bool): Whether to ignore indexes of type `pd.RangeIndex`.
    check_index_names (bool): See `vectorbtpro.utils.checks.is_index_equal`.
    **clean_index_kwargs: Keyword arguments passed to `vectorbtpro.base.indexes.clean_index`.

For defaults, see `vectorbtpro._settings.broadcasting`.

!!! note
    Series names are treated as columns with a single element but without a name.
    If a column level without a name loses its meaning, better to convert Series to DataFrames
    with one column prior to broadcasting. If the name of a Series is not that important,
    better to drop it altogether by setting it to None.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: wrap_broadcasted
```
If the newly brodcasted array was originally a Pandas object, make it Pandas object again
and assign it the newly broadcast index/columns.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: align_pd_arrays
```
Align Pandas arrays against common index and/or column levels using reindexing
and `vectorbtpro.base.indexes.align_indexes` respectively.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Classe: BCO
```
Class that represents an object passed to `broadcast`.

If any value is None, mostly defaults to the global value passed to `broadcast`.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Classe: Default
```
Class for wrapping default values.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Classe: Ref
```
Class for wrapping references to other values.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: resolve_ref
```
Resolve a potential reference.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: broadcast
```
Bring any array-like object in `args` to the same shape by using NumPy-like broadcasting.

See [Broadcasting](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html).

!!! important
    The major difference to NumPy is that one-dimensional arrays will always broadcast against the row axis!

Can broadcast Pandas objects by broadcasting their index/columns with `broadcast_index`.

Args:
    *args: Objects to broadcast.

        If the first and only argument is a mapping, will return a dict.

        Allows using `BCO`, `Ref`, `Default`, `vectorbtpro.utils.params.Param`,
        `vectorbtpro.base.indexing.index_dict`, `vectorbtpro.base.indexing.IdxSetter`,
        `vectorbtpro.base.indexing.IdxSetterFactory`, and templates.
        If an index dictionary, fills using `vectorbtpro.base.wrapping.ArrayWrapper.fill_and_set`.
    to_shape (tuple of int): Target shape. If set, will broadcast every object in `args` to `to_shape`.
    align_index (bool): Whether to align index of Pandas objects using union.

        Pass None to use the default.
    align_columns (bool): Whether to align columns of Pandas objects using multi-index.

        Pass None to use the default.
    index_from (any): Broadcasting rule for index.

        Pass None to use the default.
    columns_from (any): Broadcasting rule for columns.

        Pass None to use the default.
    to_frame (bool): Whether to convert all Series to DataFrames.
    axis (int, sequence or mapping): See `BCO.axis`.
    to_pd (bool, sequence or mapping): See `BCO.to_pd`.

        If None, converts only if there is at least one Pandas object among them.
    keep_flex (bool, sequence or mapping): See `BCO.keep_flex`.
    min_ndim (int, sequence or mapping): See `BCO.min_ndim`.

        If None, becomes 2 if `keep_flex` is True, otherwise 1.
    expand_axis (int, sequence or mapping): See `BCO.expand_axis`.
    post_func (callable, sequence or mapping): See `BCO.post_func`.

        Applied only when `keep_flex` is False.
    require_kwargs (dict, sequence or mapping): See `BCO.require_kwargs`.

        This key will be merged with any argument-specific dict. If the mapping contains all keys in
        `np.require`, it will be applied to all objects.
    reindex_kwargs (dict, sequence or mapping): See `BCO.reindex_kwargs`.

        This key will be merged with any argument-specific dict. If the mapping contains all keys in
        `pd.DataFrame.reindex`, it will be applied to all objects.
    merge_kwargs (dict, sequence or mapping): See `BCO.merge_kwargs`.

        This key will be merged with any argument-specific dict. If the mapping contains all keys in
        `pd.DataFrame.merge`, it will be applied to all objects.
    tile (int or index_like): Tile the final object by the number of times or index.
    random_subset (int): Select a random subset of parameter values.

        Seed can be set using NumPy before calling this function.
    seed (int): Seed to make output deterministic.
    keep_wrap_default (bool): Whether to keep wrapping with `vectorbtpro.base.reshaping.Default`.
    return_wrapper (bool): Whether to also return the wrapper associated with the operation.
    wrapper_kwargs (dict): Keyword arguments passed to `vectorbtpro.base.wrapping.ArrayWrapper`.
    ignore_sr_names (bool): See `broadcast_index`.
    ignore_ranges (bool): See `broadcast_index`.
    check_index_names (bool): See `broadcast_index`.
    clean_index_kwargs (dict): Keyword arguments passed to `vectorbtpro.base.indexes.clean_index`.
    template_context (dict): Context used to substitute templates.

For defaults, see `vectorbtpro._settings.broadcasting`.

Any keyword argument that can be associated with an object can be passed as

* a const that is applied to all objects,
* a sequence with value per object, and
* a mapping with value per object name and the special key `_def` denoting the default value.

Additionally, any object can be passed wrapped with `BCO`, which ibutes will override
any of the above arguments if not None.

Usage:
    * Without broadcasting index and columns:

    ```pycon
    >>> from vectorbtpro import *

    >>> v = 0
    >>> a = np.array([1, 2, 3])
    >>> sr = pd.Series([1, 2, 3], index=pd.Index(['x', 'y', 'z']), name='a')
    >>> df = pd.DataFrame(
    ...     [[1, 2, 3], [4, 5, 6], [7, 8, 9]],
    ...     index=pd.Index(['x2', 'y2', 'z2']),
    ...     columns=pd.Index(['a2', 'b2', 'c2']),
    ... )

    >>> for i in vbt.broadcast(
    ...     v, a, sr, df,
    ...     index_from='keep',
    ...     columns_from='keep',
    ...     align_index=False
    ... ): print(i)
       0  1  2
    0  0  0  0
    1  0  0  0
    2  0  0  0
       0  1  2
    0  1  2  3
    1  1  2  3
    2  1  2  3
       a  a  a
    x  1  1  1
    y  2  2  2
    z  3  3  3
        a2  b2  c2
    x2   1   2   3
    y2   4   5   6
    z2   7   8   9
    ```

    * Take index and columns from the argument at specific position:

    ```pycon
    >>> for i in vbt.broadcast(
    ...     v, a, sr, df,
    ...     index_from=2,
    ...     columns_from=3,
    ...     align_index=False
    ... ): print(i)
       a2  b2  c2
    x   0   0   0
    y   0   0   0
    z   0   0   0
       a2  b2  c2
    x   1   2   3
    y   1   2   3
    z   1   2   3
       a2  b2  c2
    x   1   1   1
    y   2   2   2
    z   3   3   3
       a2  b2  c2
    x   1   2   3
    y   4   5   6
    z   7   8   9
    ```

    * Broadcast index and columns through stacking:

    ```pycon
    >>> for i in vbt.broadcast(
    ...     v, a, sr, df,
    ...     index_from='stack',
    ...     columns_from='stack',
    ...     align_index=False
    ... ): print(i)
          a2  b2  c2
    x x2   0   0   0
    y y2   0   0   0
    z z2   0   0   0
          a2  b2  c2
    x x2   1   2   3
    y y2   1   2   3
    z z2   1   2   3
          a2  b2  c2
    x x2   1   1   1
    y y2   2   2   2
    z z2   3   3   3
          a2  b2  c2
    x x2   1   2   3
    y y2   4   5   6
    z z2   7   8   9
    ```

    * Set index and columns manually:

    ```pycon
    >>> for i in vbt.broadcast(
    ...     v, a, sr, df,
    ...     index_from=['a', 'b', 'c'],
    ...     columns_from=['d', 'e', 'f'],
    ...     align_index=False
    ... ): print(i)
       d  e  f
    a  0  0  0
    b  0  0  0
    c  0  0  0
       d  e  f
    a  1  2  3
    b  1  2  3
    c  1  2  3
       d  e  f
    a  1  1  1
    b  2  2  2
    c  3  3  3
       d  e  f
    a  1  2  3
    b  4  5  6
    c  7  8  9
    ```

    * Pass arguments as a mapping returns a mapping:

    ```pycon
    >>> vbt.broadcast(
    ...     dict(v=v, a=a, sr=sr, df=df),
    ...     index_from='stack',
    ...     align_index=False
    ... )
    {'v':       a2  b2  c2
          x x2   0   0   0
          y y2   0   0   0
          z z2   0   0   0,
     'a':       a2  b2  c2
          x x2   1   2   3
          y y2   1   2   3
          z z2   1   2   3,
     'sr':       a2  b2  c2
           x x2   1   1   1
           y y2   2   2   2
           z z2   3   3   3,
     'df':       a2  b2  c2
           x x2   1   2   3
           y y2   4   5   6
           z z2   7   8   9}
    ```

    * Keep all results in a format suitable for flexible indexing apart from one:

    ```pycon
    >>> vbt.broadcast(
    ...     dict(v=v, a=a, sr=sr, df=df),
    ...     index_from='stack',
    ...     keep_flex=dict(_def=True, df=False),
    ...     require_kwargs=dict(df=dict(dtype=float)),
    ...     align_index=False
    ... )
    {'v': array([0]),
     'a': array([1, 2, 3]),
     'sr': array([[1],
                  [2],
                  [3]]),
     'df':        a2   b2   c2
           x x2  1.0  2.0  3.0
           y y2  4.0  5.0  6.0
           z z2  7.0  8.0  9.0}
    ```

    * Specify arguments per object using `BCO`:

    ```pycon
    >>> df_bco = vbt.BCO(df, keep_flex=False, require_kwargs=dict(dtype=float))
    >>> vbt.broadcast(
    ...     dict(v=v, a=a, sr=sr, df=df_bco),
    ...     index_from='stack',
    ...     keep_flex=True,
    ...     align_index=False
    ... )
    {'v': array([0]),
     'a': array([1, 2, 3]),
     'sr': array([[1],
                  [2],
                  [3]]),
     'df':        a2   b2   c2
           x x2  1.0  2.0  3.0
           y y2  4.0  5.0  6.0
           z z2  7.0  8.0  9.0}
    ```

    * Introduce a parameter that should build a Cartesian product of its values and other objects:

    ```pycon
    >>> df_bco = vbt.BCO(df, keep_flex=False, require_kwargs=dict(dtype=float))
    >>> p_bco = vbt.BCO(pd.Param([1, 2, 3], name='my_p'))
    >>> vbt.broadcast(
    ...     dict(v=v, a=a, sr=sr, df=df_bco, p=p_bco),
    ...     index_from='stack',
    ...     keep_flex=True,
    ...     align_index=False
    ... )
    {'v': array([0]),
     'a': array([1, 2, 3, 1, 2, 3, 1, 2, 3]),
     'sr': array([[1],
            [2],
            [3]]),
     'df': my_p        1              2              3
            a2   b2   c2   a2   b2   c2   a2   b2   c2
     x x2  1.0  2.0  3.0  1.0  2.0  3.0  1.0  2.0  3.0
     y y2  4.0  5.0  6.0  4.0  5.0  6.0  4.0  5.0  6.0
     z z2  7.0  8.0  9.0  7.0  8.0  9.0  7.0  8.0  9.0,
     'p': array([[1, 1, 1, 2, 2, 2, 3, 3, 3],
            [1, 1, 1, 2, 2, 2, 3, 3, 3],
            [1, 1, 1, 2, 2, 2, 3, 3, 3]])}
    ```

    * Build a Cartesian product of all parameters:

    ```pycon
    >>> vbt.broadcast(
    ...     dict(
    ...         a=vbt.Param([1, 2, 3]),
    ...         b=vbt.Param(['x', 'y']),
    ...         c=vbt.Param([False, True])
    ...     )
    ... )
    {'a': array([[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3]]),
     'b': array([['x', 'x', 'y', 'y', 'x', 'x', 'y', 'y', 'x', 'x', 'y', 'y']], dtype='<U1'),
     'c': array([[False, True, False, True, False, True, False, True, False, True, False, True]])}
    ```

    * Build a Cartesian product of two groups of parameters - (a, d) and (b, c):

    ```pycon
    >>> vbt.broadcast(
    ...     dict(
    ...         a=vbt.Param([1, 2, 3], level=0),
    ...         b=vbt.Param(['x', 'y'], level=1),
    ...         d=vbt.Param([100., 200., 300.], level=0),
    ...         c=vbt.Param([False, True], level=1)
    ...     )
    ... )
    {'a': array([[1, 1, 2, 2, 3, 3]]),
     'b': array([['x', 'y', 'x', 'y', 'x', 'y']], dtype='<U1'),
     'd': array([[100., 100., 200., 200., 300., 300.]]),
     'c': array([[False,  True, False,  True, False,  True]])}
    ```

    * Select a random subset of parameter combinations:

    ```pycon
    >>> vbt.broadcast(
    ...     dict(
    ...         a=vbt.Param([1, 2, 3]),
    ...         b=vbt.Param(['x', 'y']),
    ...         c=vbt.Param([False, True])
    ...     ),
    ...     random_subset=5,
    ...     seed=42
    ... )
    {'a': array([[1, 2, 3, 3, 3]]),
     'b': array([['x', 'x', 'x', 'x', 'y']], dtype='<U1'),
     'c': array([[False,  True, False,  True, False]])}
    ```
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: broadcast_to
```
Broadcast `arg1` to `arg2`.

Argument `arg2` can be a shape, an instance of `vectorbtpro.base.wrapping.ArrayWrapper`,
or any array-like object.

Pass None to `index_from`/`columns_from` to use index/columns of the second argument.

Keyword arguments `**kwargs` are passed to `broadcast`.

Usage:
    ```pycon
    >>> from vectorbtpro import *
    >>> from vectorbtpro.base.reshaping import broadcast_to

    >>> a = np.array([1, 2, 3])
    >>> sr = pd.Series([4, 5, 6], index=pd.Index(['x', 'y', 'z']), name='a')

    >>> broadcast_to(a, sr)
    x    1
    y    2
    z    3
    Name: a, dtype: int64

    >>> broadcast_to(sr, a)
    array([4, 5, 6])
    ```
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: broadcast_to_array_of
```
Broadcast `arg1` to the shape `(1, *arg2.shape)`.

`arg1` must be either a scalar, a 1-dim array, or have 1 dimension more than `arg2`.

Usage:
    ```pycon
    >>> from vectorbtpro import *
    >>> from vectorbtpro.base.reshaping import broadcast_to_array_of

    >>> broadcast_to_array_of([0.1, 0.2], np.empty((2, 2)))
    [[[0.1 0.1]
      [0.1 0.1]]

     [[0.2 0.2]
      [0.2 0.2]]]
    ```
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: broadcast_to_axis_of
```
Broadcast `arg1` to an axis of `arg2`.

If `arg2` has less dimensions than requested, will broadcast `arg1` to a single number.

For other keyword arguments, see `broadcast`.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: broadcast_combs
```
Align an axis of each array using a combinatoric function and broadcast their indexes.

Usage:
    ```pycon
    >>> from vectorbtpro import *
    >>> from vectorbtpro.base.reshaping import broadcast_combs

    >>> df = pd.DataFrame([[1, 2, 3], [3, 4, 5]], columns=pd.Index(['a', 'b', 'c'], name='df_param'))
    >>> df2 = pd.DataFrame([[6, 7], [8, 9]], columns=pd.Index(['d', 'e'], name='df2_param'))
    >>> sr = pd.Series([10, 11], name='f')

    >>> new_df, new_df2, new_sr = broadcast_combs((df, df2, sr))

    >>> new_df
    df_param   a     b     c
    df2_param  d  e  d  e  d  e
    0          1  1  2  2  3  3
    1          3  3  4  4  5  5

    >>> new_df2
    df_param   a     b     c
    df2_param  d  e  d  e  d  e
    0          6  7  6  7  6  7
    1          8  9  8  9  8  9

    >>> new_sr
    df_param    a       b       c
    df2_param   d   e   d   e   d   e
    0          10  10  10  10  10  10
    1          11  11  11  11  11  11
    ```
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: get_multiindex_series
```
Get Series with a multi-index.

If DataFrame has been passed, must at maximum have one row or column.
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: unstack_to_array
```
Reshape `arg` based on its multi-index into a multi-dimensional array.

Use `levels` to specify what index levels to unstack and in which order.

Usage:
    ```pycon
    >>> from vectorbtpro import *
    >>> from vectorbtpro.base.reshaping import unstack_to_array

    >>> index = pd.MultiIndex.from_arrays(
    ...     [[1, 1, 2, 2], [3, 4, 3, 4], ['a', 'b', 'c', 'd']])
    >>> sr = pd.Series([1, 2, 3, 4], index=index)

    >>> unstack_to_array(sr).shape
    (2, 2, 4)

    >>> unstack_to_array(sr)
    [[[ 1. nan nan nan]
     [nan  2. nan nan]]

     [[nan nan  3. nan]
    [nan nan nan  4.]]]

    >>> unstack_to_array(sr, levels=(2, 0))
    [[ 1. nan]
     [ 2. nan]
     [nan  3.]
     [nan  4.]]
    ```
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: make_symmetric
```
Make `arg` symmetric.

The index and columns of the resulting DataFrame will be identical.

Requires the index and columns to have the same number of levels.

Pass `sort=False` if index and columns should not be sorted, but concatenated
and get duplicates removed.

Usage:
    ```pycon
    >>> from vectorbtpro import *
    >>> from vectorbtpro.base.reshaping import make_symmetric

    >>> df = pd.DataFrame([[1, 2], [3, 4]], index=['a', 'b'], columns=['c', 'd'])

    >>> make_symmetric(df)
         a    b    c    d
    a  NaN  NaN  1.0  2.0
    b  NaN  NaN  3.0  4.0
    c  1.0  3.0  NaN  NaN
    d  2.0  4.0  NaN  NaN
    ```
```

---

# Pasta: base
### Arquivo: reshaping.py
#### Função: unstack_to_df
```
Reshape `arg` based on its multi-index into a DataFrame.

Use `index_levels` to specify what index levels will form new index, and `column_levels`
for new columns. Set `symmetric` to True to make DataFrame symmetric.

Usage:
    ```pycon
    >>> from vectorbtpro import *
    >>> from vectorbtpro.base.reshaping import unstack_to_df

    >>> index = pd.MultiIndex.from_arrays(
    ...     [[1, 1, 2, 2], [3, 4, 3, 4], ['a', 'b', 'c', 'd']],
    ...     names=['x', 'y', 'z'])
    >>> sr = pd.Series([1, 2, 3, 4], index=index)

    >>> unstack_to_df(sr, index_levels=(0, 1), column_levels=2)
    z      a    b    c    d
    x y
    1 3  1.0  NaN  NaN  NaN
    1 4  NaN  2.0  NaN  NaN
    2 3  NaN  NaN  3.0  NaN
    2 4  NaN  NaN  NaN  4.0
    ```
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Docstring do Módulo
```
Classes for wrapping NumPy arrays into Series/DataFrames.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
```
Class that stores index, columns, and shape metadata for wrapping NumPy arrays.
Tightly integrated with `vectorbtpro.base.grouping.base.Grouper` for grouping columns.

If the underlying object is a Series, pass `[sr.name]` as `columns`.

`**kwargs` are passed to `vectorbtpro.base.grouping.base.Grouper`.

!!! note
    This class is meant to be immutable. To change any attribute, use `ArrayWrapper.replace`.

    Use methods that begin with `get_` to get group-aware results.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
```
Class that uses `ArrayWrapper` globally.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: from_obj
```
Derive metadata from an object.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: from_shape
```
Derive metadata from shape.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: extract_init_kwargs
```
Extract keyword arguments that can be passed to `ArrayWrapper` or `Grouper`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: resolve_stack_kwargs
```
Resolve keyword arguments for initializing `ArrayWrapper` after stacking.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: row_stack
```
Stack multiple `ArrayWrapper` instances along rows.

Concatenates indexes using `vectorbtpro.base.indexes.concat_indexes`.

Frequency must be the same across all indexes. A custom frequency can be provided via `freq`.

If column levels in some instances differ, they will be stacked upon each other.
Custom columns can be provided via `columns`.

If `group_by` is None, all instances must be either grouped or not, and they must
contain the same group values and labels.

All instances must contain the same keys and values in their configs and configs of their
grouper instances, apart from those arguments provided explicitly via `kwargs`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: column_stack
```
Stack multiple `ArrayWrapper` instances along columns.

If indexes are the same in each wrapper index, will use that index. If indexes differ and
`union_index` is True, they will be merged into a single one by the set union operation.
Otherwise, an error will be raised. The merged index must have no duplicates or mixed data,
and must be monotonically increasing. A custom index can be provided via `index`.

Frequency must be the same across all indexes. A custom frequency can be provided via `freq`.

Concatenates columns and groups using `vectorbtpro.base.indexes.concat_indexes`.

If any of the instances has `column_only_select` being enabled, the final wrapper will also enable it.
If any of the instances has `group_select` or other grouping-related flags being disabled, the final
wrapper will also disable them.

All instances must contain the same keys and values in their configs and configs of their
grouper instances, apart from those arguments provided explicitly via `kwargs`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: indexing_func_meta
```
Perform indexing on `ArrayWrapper` and also return metadata.

Takes into account column grouping.

Flipping rows and columns is not allowed. If one row is selected, the result will still be
a Series when indexing a Series and a DataFrame when indexing a DataFrame.

Set `column_only_select` to True to index the array wrapper as a Series of columns/groups.
This way, selection of index (axis 0) can be avoided. Set `range_only_select` to True to
allow selection of rows only using slices. Set `group_select` to True to allow selection of groups.
Otherwise, indexing is performed on columns, even if grouping is enabled. Takes effect only if
grouping is enabled.

Returns the new array wrapper, row indices, column indices, and group indices.
If `return_slices` is True (default), indices will be returned as a slice if they were
identified as a range. If `return_none_slices` is True (default), indices will be returned as a slice
`(None, None, None)` if the axis hasn't been changed.

!!! note
    If `column_only_select` is True, make sure to index the array wrapper
    as a Series of columns rather than a DataFrame. For example, the operation
    `.iloc[:, :2]` should become `.iloc[:2]`. Operations are not allowed if the
    object is already a Series and thus has only one column/group.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: indexing_func
```
Perform indexing on `ArrayWrapper`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: select_from_flex_array
```
Select rows and columns from a flexible array.

Always returns a 2-dim NumPy array.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: get_resampler
```
See `vectorbtpro.base.accessors.BaseIDXAccessor.get_resampler`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: resample_meta
```
Perform resampling on `ArrayWrapper` and also return metadata.

`*args` and `**kwargs` are passed to `ArrayWrapper.get_resampler`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: resample
```
Perform resampling on `ArrayWrapper`.

Uses `ArrayWrapper.resample_meta`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: index
```
Index.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: index_acc
```
Get index accessor of the type `vectorbtpro.base.accessors.BaseIDXAccessor`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: ns_index
```
See `vectorbtpro.base.accessors.BaseIDXAccessor.to_ns`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: get_period_ns_index
```
See `vectorbtpro.base.accessors.BaseIDXAccessor.to_period_ns`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: columns
```
Columns.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: get_columns
```
Get group-aware `ArrayWrapper.columns`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: name
```
Name.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: get_name
```
Get group-aware `ArrayWrapper.name`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: ndim
```
Number of dimensions.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: get_ndim
```
Get group-aware `ArrayWrapper.ndim`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: shape
```
Shape.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: get_shape
```
Get group-aware `ArrayWrapper.shape`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: shape_2d
```
Shape as if the object was two-dimensional.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: get_shape_2d
```
Get group-aware `ArrayWrapper.shape_2d`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: get_freq
```
See `vectorbtpro.base.accessors.BaseIDXAccessor.get_freq`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: freq
```
See `vectorbtpro.base.accessors.BaseIDXAccessor.freq`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: ns_freq
```
See `vectorbtpro.base.accessors.BaseIDXAccessor.ns_freq`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: any_freq
```
See `vectorbtpro.base.accessors.BaseIDXAccessor.any_freq`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: period
```
See `vectorbtpro.base.accessors.BaseIDXAccessor.period`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: dt_period
```
See `vectorbtpro.base.accessors.BaseIDXAccessor.dt_period`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: arr_to_timedelta
```
See `vectorbtpro.base.accessors.BaseIDXAccessor.arr_to_timedelta`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: parse_index
```
Whether to try to convert the index into a datetime index.

Applied during the initialization and passed to `vectorbtpro.utils.datetime_.prepare_dt_index`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: column_only_select
```
Whether to perform indexing on columns only.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: range_only_select
```
Whether to perform indexing on rows using slices only.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: group_select
```
Whether to allow indexing on groups.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: grouper
```
Column grouper.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: grouped_ndim
```
Number of dimensions under column grouping.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: regroup
```
Regroup this object.

Only creates a new instance if grouping has changed, otherwise returns itself.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: flip
```
Flip index and columns.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: resolve
```
Resolve this object.

Replaces columns and other metadata with groups.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: get_index_grouper
```
See `vectorbtpro.base.accessors.BaseIDXAccessor.get_grouper`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: wrap
```
Wrap a NumPy array using the stored metadata.

Runs the following pipeline:

1) Converts to NumPy array
2) Fills NaN (optional)
3) Wraps using index, columns, and dtype (optional)
4) Converts to index (optional)
5) Converts to timedelta using `ArrayWrapper.arr_to_timedelta` (optional)
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: wrap_reduced
```
Wrap result of reduction.

`name_or_index` can be the name of the resulting series if reducing to a scalar per column,
or the index of the resulting series/dataframe if reducing to an array per column.
`columns` can be set to override object's default columns.

See `ArrayWrapper.wrap` for the pipeline.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: concat_arrs
```
Stack reduced objects along columns and wrap the final object.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: row_stack_arrs
```
Stack objects along rows and wrap the final object.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: column_stack_arrs
```
Stack objects along columns and wrap the final object.

`reindex_kwargs` will be passed to
[pandas.DataFrame.reindex](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reindex.html).
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: dummy
```
Create a dummy Series/DataFrame.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: fill
```
Fill a Series/DataFrame.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: fill_reduced
```
Fill a reduced Series/DataFrame.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: get_index_points
```
See `vectorbtpro.base.accessors.BaseIDXAccessor.get_points`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: get_index_ranges
```
See `vectorbtpro.base.accessors.BaseIDXAccessor.get_ranges`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: fill_and_set
```
Fill a new array using an index object such as `vectorbtpro.base.indexing.index_dict`.

Will be wrapped with `vectorbtpro.base.indexing.IdxSetter` if not already.

Will call `vectorbtpro.base.indexing.IdxSetter.fill_and_set`.

Usage:
    * Set a single row:

    ```pycon
    >>> from vectorbtpro import *

    >>> index = pd.date_range("2020", periods=5)
    >>> columns = pd.Index(["a", "b", "c"])
    >>> wrapper = vbt.ArrayWrapper(index, columns)

    >>> wrapper.fill_and_set(vbt.index_dict({
    ...     1: 2
    ... }))
                  a    b    c
    2020-01-01  NaN  NaN  NaN
    2020-01-02  2.0  2.0  2.0
    2020-01-03  NaN  NaN  NaN
    2020-01-04  NaN  NaN  NaN
    2020-01-05  NaN  NaN  NaN

    >>> wrapper.fill_and_set(vbt.index_dict({
    ...     "2020-01-02": 2
    ... }))
                  a    b    c
    2020-01-01  NaN  NaN  NaN
    2020-01-02  2.0  2.0  2.0
    2020-01-03  NaN  NaN  NaN
    2020-01-04  NaN  NaN  NaN
    2020-01-05  NaN  NaN  NaN

    >>> wrapper.fill_and_set(vbt.index_dict({
    ...     "2020-01-02": [1, 2, 3]
    ... }))
                  a    b    c
    2020-01-01  NaN  NaN  NaN
    2020-01-02  1.0  2.0  3.0
    2020-01-03  NaN  NaN  NaN
    2020-01-04  NaN  NaN  NaN
    2020-01-05  NaN  NaN  NaN
    ```

    * Set multiple rows:

    ```pycon
    >>> wrapper.fill_and_set(vbt.index_dict({
    ...     (1, 3): [2, 3]
    ... }))
                  a    b    c
    2020-01-01  NaN  NaN  NaN
    2020-01-02  2.0  2.0  2.0
    2020-01-03  NaN  NaN  NaN
    2020-01-04  3.0  3.0  3.0
    2020-01-05  NaN  NaN  NaN

    >>> wrapper.fill_and_set(vbt.index_dict({
    ...     ("2020-01-02", "2020-01-04"): [[1, 2, 3], [4, 5, 6]]
    ... }))
                  a    b    c
    2020-01-01  NaN  NaN  NaN
    2020-01-02  1.0  2.0  3.0
    2020-01-03  NaN  NaN  NaN
    2020-01-04  4.0  5.0  6.0
    2020-01-05  NaN  NaN  NaN

    >>> wrapper.fill_and_set(vbt.index_dict({
    ...     ("2020-01-02", "2020-01-04"): [[1, 2, 3]]
    ... }))
                  a    b    c
    2020-01-01  NaN  NaN  NaN
    2020-01-02  1.0  2.0  3.0
    2020-01-03  NaN  NaN  NaN
    2020-01-04  1.0  2.0  3.0
    2020-01-05  NaN  NaN  NaN
    ```

    * Set rows using slices:

    ```pycon
    >>> wrapper.fill_and_set(vbt.index_dict({
    ...     vbt.hslice(1, 3): 2
    ... }))
                  a    b    c
    2020-01-01  NaN  NaN  NaN
    2020-01-02  2.0  2.0  2.0
    2020-01-03  2.0  2.0  2.0
    2020-01-04  NaN  NaN  NaN
    2020-01-05  NaN  NaN  NaN

    ```pycon
    >>> wrapper.fill_and_set(vbt.index_dict({
    ...     vbt.hslice("2020-01-02", "2020-01-04"): 2
    ... }))
                  a    b    c
    2020-01-01  NaN  NaN  NaN
    2020-01-02  2.0  2.0  2.0
    2020-01-03  2.0  2.0  2.0
    2020-01-04  NaN  NaN  NaN
    2020-01-05  NaN  NaN  NaN

    >>> wrapper.fill_and_set(vbt.index_dict({
    ...     ((0, 2), (3, 5)): [[1], [2]]
    ... }))
                  a    b    c
    2020-01-01  1.0  1.0  1.0
    2020-01-02  1.0  1.0  1.0
    2020-01-03  NaN  NaN  NaN
    2020-01-04  2.0  2.0  2.0
    2020-01-05  2.0  2.0  2.0

    >>> wrapper.fill_and_set(vbt.index_dict({
    ...     ((0, 2), (3, 5)): [[1, 2, 3], [4, 5, 6]]
    ... }))
                  a    b    c
    2020-01-01  1.0  2.0  3.0
    2020-01-02  1.0  2.0  3.0
    2020-01-03  NaN  NaN  NaN
    2020-01-04  4.0  5.0  6.0
    2020-01-05  4.0  5.0  6.0
    ```

    * Set rows using index points:

    ```pycon
    >>> wrapper.fill_and_set(vbt.index_dict({
    ...     vbt.pointidx(every="2D"): 2
    ... }))
                  a    b    c
    2020-01-01  2.0  2.0  2.0
    2020-01-02  NaN  NaN  NaN
    2020-01-03  2.0  2.0  2.0
    2020-01-04  NaN  NaN  NaN
    2020-01-05  2.0  2.0  2.0
    ```

    * Set rows using index ranges:

    ```pycon
    >>> wrapper.fill_and_set(vbt.index_dict({
    ...     vbt.rangeidx(
    ...         start=("2020-01-01", "2020-01-03"),
    ...         end=("2020-01-02", "2020-01-05")
    ...     ): 2
    ... }))
                  a    b    c
    2020-01-01  2.0  2.0  2.0
    2020-01-02  NaN  NaN  NaN
    2020-01-03  2.0  2.0  2.0
    2020-01-04  2.0  2.0  2.0
    2020-01-05  NaN  NaN  NaN
    ```

    * Set column indices:

    ```pycon
    >>> wrapper.fill_and_set(vbt.index_dict({
    ...     vbt.colidx("a"): 2
    ... }))
                  a   b   c
    2020-01-01  2.0 NaN NaN
    2020-01-02  2.0 NaN NaN
    2020-01-03  2.0 NaN NaN
    2020-01-04  2.0 NaN NaN
    2020-01-05  2.0 NaN NaN

    >>> wrapper.fill_and_set(vbt.index_dict({
    ...     vbt.colidx(("a", "b")): [1, 2]
    ... }))
                  a    b   c
    2020-01-01  1.0  2.0 NaN
    2020-01-02  1.0  2.0 NaN
    2020-01-03  1.0  2.0 NaN
    2020-01-04  1.0  2.0 NaN
    2020-01-05  1.0  2.0 NaN

    >>> multi_columns = pd.MultiIndex.from_arrays(
    ...     [["a", "a", "b", "b"], [1, 2, 1, 2]],
    ...     names=["c1", "c2"]
    ... )
    >>> multi_wrapper = vbt.ArrayWrapper(index, multi_columns)

    >>> multi_wrapper.fill_and_set(vbt.index_dict({
    ...     vbt.colidx(("a", 2)): 2
    ... }))
    c1           a        b
    c2           1    2   1   2
    2020-01-01 NaN  2.0 NaN NaN
    2020-01-02 NaN  2.0 NaN NaN
    2020-01-03 NaN  2.0 NaN NaN
    2020-01-04 NaN  2.0 NaN NaN
    2020-01-05 NaN  2.0 NaN NaN

    >>> multi_wrapper.fill_and_set(vbt.index_dict({
    ...     vbt.colidx("b", level="c1"): [3, 4]
    ... }))
    c1           a        b
    c2           1   2    1    2
    2020-01-01 NaN NaN  3.0  4.0
    2020-01-02 NaN NaN  3.0  4.0
    2020-01-03 NaN NaN  3.0  4.0
    2020-01-04 NaN NaN  3.0  4.0
    2020-01-05 NaN NaN  3.0  4.0
    ```

    * Set row and column indices:

    ```pycon
    >>> wrapper.fill_and_set(vbt.index_dict({
    ...     vbt.idx(2, 2): 2
    ... }))
                 a   b    c
    2020-01-01 NaN NaN  NaN
    2020-01-02 NaN NaN  NaN
    2020-01-03 NaN NaN  2.0
    2020-01-04 NaN NaN  NaN
    2020-01-05 NaN NaN  NaN

    >>> wrapper.fill_and_set(vbt.index_dict({
    ...     vbt.idx(("2020-01-01", "2020-01-03"), 2): [1, 2]
    ... }))
                 a   b    c
    2020-01-01 NaN NaN  1.0
    2020-01-02 NaN NaN  NaN
    2020-01-03 NaN NaN  2.0
    2020-01-04 NaN NaN  NaN
    2020-01-05 NaN NaN  NaN

    >>> wrapper.fill_and_set(vbt.index_dict({
    ...     vbt.idx(("2020-01-01", "2020-01-03"), (0, 2)): [[1, 2], [3, 4]]
    ... }))
                  a   b    c
    2020-01-01  1.0 NaN  2.0
    2020-01-02  NaN NaN  NaN
    2020-01-03  3.0 NaN  4.0
    2020-01-04  NaN NaN  NaN
    2020-01-05  NaN NaN  NaN

    >>> multi_wrapper.fill_and_set(vbt.index_dict({
    ...     vbt.idx(
    ...         vbt.pointidx(every="2d"),
    ...         vbt.colidx(1, level="c2")
    ...     ): [[1, 2]]
    ... }))
    c1            a        b
    c2            1   2    1   2
    2020-01-01  1.0 NaN  2.0 NaN
    2020-01-02  NaN NaN  NaN NaN
    2020-01-03  1.0 NaN  2.0 NaN
    2020-01-04  NaN NaN  NaN NaN
    2020-01-05  1.0 NaN  2.0 NaN

    >>> multi_wrapper.fill_and_set(vbt.index_dict({
    ...     vbt.idx(
    ...         vbt.pointidx(every="2d"),
    ...         vbt.colidx(1, level="c2")
    ...     ): [[1], [2], [3]]
    ... }))
    c1            a        b
    c2            1   2    1   2
    2020-01-01  1.0 NaN  1.0 NaN
    2020-01-02  NaN NaN  NaN NaN
    2020-01-03  2.0 NaN  2.0 NaN
    2020-01-04  NaN NaN  NaN NaN
    2020-01-05  3.0 NaN  3.0 NaN
    ```

    * Set rows using a template:

    ```pycon
    >>> wrapper.fill_and_set(vbt.index_dict({
    ...     vbt.RepEval("index.day % 2 == 0"): 2
    ... }))
                  a    b    c
    2020-01-01  NaN  NaN  NaN
    2020-01-02  2.0  2.0  2.0
    2020-01-03  NaN  NaN  NaN
    2020-01-04  2.0  2.0  2.0
    2020-01-05  NaN  NaN  NaN
    ```
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: select_col
```
Select one column/group.

`column` can be a label-based position as well as an integer position (if label fails).
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: select_col_from_obj
```
Select one column/group from a Pandas object.

`column` can be a label-based position as well as an integer position (if label fails).
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: split
```
Split using `vectorbtpro.generic.splitting.base.Splitter.split_and_take`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: split_apply
```
Split using `vectorbtpro.generic.splitting.base.Splitter.split_and_apply`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: ArrayWrapper
#### Função: items
```
Iterate over columns or groups (if grouped and `Wrapping.group_select` is True).

If `apply_group_by` is False, `group_by` becomes a grouping instruction for the iteration,
not for the final object. In this case, will raise an error if the instance is grouped
and that grouping must be changed.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
#### Função: resolve_row_stack_kwargs
```
Resolve keyword arguments for initializing `Wrapping` after stacking along rows.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
#### Função: resolve_column_stack_kwargs
```
Resolve keyword arguments for initializing `Wrapping` after stacking along columns.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
#### Função: resolve_stack_kwargs
```
Resolve keyword arguments for initializing `Wrapping` after stacking.

Should be called after `Wrapping.resolve_row_stack_kwargs` or `Wrapping.resolve_column_stack_kwargs`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
#### Função: row_stack
```
Stack multiple `Wrapping` instances along rows.

Should use `ArrayWrapper.row_stack`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
#### Função: column_stack
```
Stack multiple `Wrapping` instances along columns.

Should use `ArrayWrapper.column_stack`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
#### Função: indexing_func
```
Perform indexing on `Wrapping`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
#### Função: resample
```
Perform resampling on `Wrapping`.

When overriding, make sure to create a resampler by passing `*args` and `**kwargs`
to `ArrayWrapper.get_resampler`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
#### Função: wrapper
```
Array wrapper of the type `ArrayWrapper`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
#### Função: column_only_select
```
Overrides `ArrayWrapper.column_only_select`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
#### Função: range_only_select
```
Overrides `ArrayWrapper.range_only_select`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
#### Função: group_select
```
Overrides `ArrayWrapper.group_select`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
#### Função: regroup
```
Regroup this object.

Only creates a new instance if grouping has changed, otherwise returns itself.

`**kwargs` will be passed to `ArrayWrapper.regroup`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
#### Função: resolve_self
```
Resolve self.

Creates a copy of this instance if a different `freq` can be found in `cond_kwargs`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
#### Função: select_col
```
Select one column/group.

`column` can be a label-based position as well as an integer position (if label fails).
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
#### Função: select_col_from_obj
```
See `ArrayWrapper.select_col_from_obj`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
#### Função: split
```
Split using `vectorbtpro.generic.splitting.base.Splitter.split_and_take`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
#### Função: split_apply
```
Split using `vectorbtpro.generic.splitting.base.Splitter.split_and_apply`.
```

---

# Pasta: base
### Arquivo: wrapping.py
#### Classe: Wrapping
#### Função: items
```
Iterate over columns or groups (if grouped and `Wrapping.group_select` is True).

If `apply_group_by` is False, `group_by` becomes a grouping instruction for the iteration,
not for the final object. In this case, will raise an error if the instance is grouped
and that grouping must be changed.
```

---

# Pasta: base
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules with base classes and utilities for pandas objects, such as broadcasting.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Docstring do Módulo
```
Base classes and functions for grouping.

Class `Grouper` stores metadata related to grouping index. It can return, for example,
the number of groups, the start indices of groups, and other information useful for reducing
operations that utilize grouping. It also allows to dynamically enable/disable/modify groups
and checks whether a certain operation is permitted.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
```
Class that exposes methods to group index.

`group_by` can be:

* boolean (False for no grouping, True for one group),
* integer (level by position),
* string (level by name),
* sequence of integers or strings that is shorter than `index` (multiple levels),
* any other sequence that has the same length as `index` (group per index).

Set `allow_enable` to False to prohibit grouping if `Grouper.group_by` is None.
Set `allow_disable` to False to prohibit disabling of grouping if `Grouper.group_by` is not None.
Set `allow_modify` to False to prohibit modifying groups (you can still change their labels).

All properties are read-only to enable caching.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: group_by_to_index
```
Convert mapper `group_by` to `pd.Index`.

!!! note
    Index and mapper must have the same length.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: group_by_to_groups_and_index
```
Return array of group indices pointing to the original index, and grouped index.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: iter_group_lens
```
Iterate over indices of each group in group lengths.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: iter_group_map
```
Iterate over indices of each group in a group map.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: from_pd_group_by
```
Build a `Grouper` instance from a pandas `GroupBy` object.

Indices are stored under `index` and group labels under `group_by`.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: index
```
Original index.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: group_by
```
Mapper for grouping.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: def_lvl_name
```
Default level name.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: allow_enable
```
Whether to allow enabling grouping.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: allow_disable
```
Whether to allow disabling grouping.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: allow_modify
```
Whether to allow changing groups.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: is_grouped
```
Check whether index are grouped.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: is_grouping_enabled
```
Check whether grouping has been enabled.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: is_grouping_disabled
```
Check whether grouping has been disabled.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: is_grouping_modified
```
Check whether grouping has been modified.

Doesn't care if grouping labels have been changed.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: is_grouping_changed
```
Check whether grouping has been changed in any way.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: is_group_count_changed
```
Check whether the number of groups has changed.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: check_group_by
```
Check passed `group_by` object against restrictions.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: resolve_group_by
```
Resolve `group_by` from either object variable or keyword argument.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: get_groups_and_index
```
See `Grouper.group_by_to_groups_and_index`.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: get_groups
```
Return groups array.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: get_index
```
Return grouped index.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: grouped_index
```
Grouped index.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: get_stretched_index
```
Return stretched index.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: get_group_count
```
Get number of groups.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: is_sorted
```
Return whether groups are monolithic, sorted.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: get_group_lens
```
See `vectorbtpro.base.grouping.nb.get_group_lens_nb`.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: get_group_start_idxs
```
Get first index of each group as an array.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: get_group_end_idxs
```
Get end index of each group as an array.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: get_group_map
```
See get_group_map_nb.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: iter_group_idxs
```
Iterate over indices of each group.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: iter_groups
```
Iterate over groups and their indices.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: base.py
#### Classe: Grouper
#### Função: select_groups
```
Select groups.

Returns indices and new group array. Automatically decides whether to use group lengths or group map.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: nb.py
#### Docstring do Módulo
```
Numba-compiled functions for grouping.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: nb.py
#### Função: get_group_lens_nb
```
Return the count per group.

!!! note
    Columns must form monolithic, sorted groups. For unsorted groups, use `get_group_map_nb`.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: nb.py
#### Função: get_group_map_nb
```
Build the map between groups and indices.

Returns an array with indices segmented by group and an array with group lengths.

Works well for unsorted group arrays.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: nb.py
#### Função: group_lens_select_nb
```
Perform indexing on a sorted array using group lengths.

Returns indices of elements corresponding to groups in `new_groups` and a new group array.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: nb.py
#### Função: group_map_select_nb
```
Perform indexing using group map.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: nb.py
#### Função: group_by_evenly_nb
```
Get `group_by` from evenly splitting a space of values.
```

---

# Pasta: base
## Subpasta: grouping
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules with classes and utilities for grouping.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Docstring do Módulo
```
Base classes and functions for resampling.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
```
Class that exposes methods to resample index.

Args:
    source_index (index_like): Index being resampled.
    target_index (index_like): Index resulted from resampling.
    source_freq (frequency_like or bool): Frequency or date offset of the source index.

        Set to False to force-set the frequency to None.
    target_freq (frequency_like or bool): Frequency or date offset of the target index.

        Set to False to force-set the frequency to None.
    silence_warnings (bool): Whether to silence all warnings.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: from_pd_resampler
```
Build `Resampler` from
[pandas.core.resample.Resampler](https://pandas.pydata.org/docs/reference/resampling.html).
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: from_pd_resample
```
Build `Resampler` from
[pandas.DataFrame.resample](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html).
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: from_date_range
```
Build `Resampler` from `vectorbtpro.utils.datetime_.date_range`.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: source_index
```
Index being resampled.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: target_index
```
Index resulted from resampling.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: source_freq
```
Frequency or date offset of the source index.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: target_freq
```
Frequency or date offset of the target index.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: silence_warnings
```
Frequency or date offset of the target index.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: get_np_source_freq
```
Frequency or date offset of the source index in NumPy format.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: get_np_target_freq
```
Frequency or date offset of the target index in NumPy format.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: get_lbound_index
```
Get the left bound of a datetime index.

If `freq` is None, calculates the leftmost bound.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: get_rbound_index
```
Get the right bound of a datetime index.

If `freq` is None, calculates the rightmost bound.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: source_lbound_index
```
Get the left bound of the source datetime index.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: source_rbound_index
```
Get the right bound of the source datetime index.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: target_lbound_index
```
Get the left bound of the target datetime index.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: target_rbound_index
```
Get the right bound of the target datetime index.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: map_to_target_index
```
See `vectorbtpro.base.resampling.nb.map_to_target_index_nb`.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: index_difference
```
See `vectorbtpro.base.resampling.nb.index_difference_nb`.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: map_index_to_source_ranges
```
See `vectorbtpro.base.resampling.nb.map_index_to_source_ranges_nb`.

If `Resampler.target_freq` is a date offset, sets is to None and gives a warning.
Raises another warning is `target_freq` is None.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: map_bounds_to_source_ranges
```
See `vectorbtpro.base.resampling.nb.map_bounds_to_source_ranges_nb`.

Either `target_lbound_index` or `target_rbound_index` must be set.
Set `target_lbound_index` and `target_rbound_index` to 'pandas' to use
`Resampler.get_lbound_index` and `Resampler.get_rbound_index` respectively.
Also, both allow providing a single datetime string and will automatically broadcast
to the `Resampler.target_index`.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: resample_source_mask
```
See `vectorbtpro.base.resampling.nb.resample_source_mask_nb`.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: base.py
#### Classe: Resampler
#### Função: last_before_target_index
```
See `vectorbtpro.base.resampling.nb.last_before_target_index_nb`.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: nb.py
#### Docstring do Módulo
```
Numba-compiled functions for resampling.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: nb.py
#### Função: date_range_nb
```
Generate a datetime index with nanosecond precision from a date range.

Inspired by [pandas.date_range](https://pandas.pydata.org/docs/reference/api/pandas.date_range.html).
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: nb.py
#### Função: map_to_target_index_nb
```
Get the index of each from `source_index` in `target_index`.

If `before` is True, applied on elements that come before and including that index.
Otherwise, applied on elements that come after and including that index.

If `raise_missing` is True, will throw an error if an index cannot be mapped.
Otherwise, the element for that index becomes -1.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: nb.py
#### Função: index_difference_nb
```
Get the elements in `source_index` not present in `target_index`.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: nb.py
#### Função: map_index_to_source_ranges_nb
```
Get the source bounds that correspond to each target index.

If `target_freq` is not None, the right bound is limited by the frequency in `target_freq`.
Otherwise, the right bound is the next index in `target_index`.

Returns a 2-dim array where the first column is the absolute start index (including) and
the second column is the absolute end index (excluding).

If an element cannot be mapped, the start and end of the range becomes -1.

!!! note
    Both index arrays must be increasing. Repeating values are allowed.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: nb.py
#### Função: map_bounds_to_source_ranges_nb
```
Get the source bounds that correspond to the target bounds.

Returns a 2-dim array where the first column is the absolute start index (including) nad
the second column is the absolute end index (excluding).

If an element cannot be mapped, the start and end of the range becomes -1.

!!! note
    Both index arrays must be increasing. Repeating values are allowed.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: nb.py
#### Função: resample_source_mask_nb
```
Resample a source mask to the target index.

Becomes True only if the target bar is fully contained in the source bar. The source bar
is represented by a non-interrupting sequence of True values in the source mask.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: nb.py
#### Função: last_before_target_index_nb
```
For each source index, find the position of the last source index between the original
source index and the corresponding target index.
```

---

# Pasta: base
## Subpasta: resampling
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules with classes and utilities for resampling.
```

---

# Pasta: data
### Arquivo: base.py
#### Docstring do Módulo
```
Base class for working with data sources.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: key_dict
```
Dict that contains features or symbols as keys.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: feature_dict
```
Dict that contains features as keys.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: symbol_dict
```
Dict that contains symbols as keys.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: run_func_dict
```
Dict that contains function names as keys for `Data.run`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: run_arg_dict
```
Dict that contains argument names as keys for `Data.run`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
```
Base mixin class for working with data.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
```
Mixin class for working with OHLC data.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: MetaFeatures
```
Meta class that exposes a read-only class property `MetaFeatures.feature_config`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: DataWithFeatures
```
Class exposes a read-only class property `DataWithFeatures.field_config`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
```
Class that downloads, updates, and manages data coming from a data source.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: feature_wrapper
```
Column wrapper.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: symbol_wrapper
```
Symbol wrapper.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: features
```
List of features.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: symbols
```
List of symbols.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: has_multiple_keys
```
Check whether there are one or multiple keys.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: prepare_key
```
Prepare a key.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: get_feature_idx
```
Return the index of a feature.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: get_symbol_idx
```
Return the index of a symbol.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: select_feature_idxs
```
Select one or more features by index.

Returns a new instance.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: select_symbol_idxs
```
Select one or more symbols by index.

Returns a new instance.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: select_features
```
Select one or more features.

Returns a new instance.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: select_symbols
```
Select one or more symbols.

Returns a new instance.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: get
```
Get one or more features of one or more symbols of data.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: has_feature
```
Whether feature exists.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: has_symbol
```
Whether symbol exists.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: assert_has_feature
```
Assert that feature exists.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: assert_has_symbol
```
Assert that symbol exists.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: get_feature
```
Get feature that match a feature index or label.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: BaseDataMixin
#### Função: get_symbol
```
Get symbol that match a symbol index or label.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: open
```
Open.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: high
```
High.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: low
```
Low.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: close
```
Close.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: volume
```
Volume.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: trade_count
```
Trade count.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: vwap
```
VWAP.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: hlc3
```
HLC/3.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: ohlc4
```
OHLC/4.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: has_any_ohlc
```
Whether the instance has any of the OHLC features.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: has_ohlc
```
Whether the instance has all the OHLC features.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: has_any_ohlcv
```
Whether the instance has any of the OHLCV features.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: has_ohlcv
```
Whether the instance has all the OHLCV features.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: ohlc
```
Return a `OHLCDataMixin` instance with the OHLC features only.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: ohlcv
```
Return a `OHLCDataMixin` instance with the OHLCV features only.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: get_returns_acc
```
Return accessor of type `vectorbtpro.returns.accessors.ReturnsAccessor`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: returns_acc
```
`OHLCDataMixin.get_returns_acc` with default arguments.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: get_returns
```
Returns.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: returns
```
`OHLCDataMixin.get_returns` with default arguments.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: get_log_returns
```
Log returns.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: log_returns
```
`OHLCDataMixin.get_log_returns` with default arguments.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: get_daily_returns
```
Daily returns.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: daily_returns
```
`OHLCDataMixin.get_daily_returns` with default arguments.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: get_daily_log_returns
```
Daily log returns.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: daily_log_returns
```
`OHLCDataMixin.get_daily_log_returns` with default arguments.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: get_drawdowns
```
Generate drawdown records.

See `vectorbtpro.generic.drawdowns.Drawdowns`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: OHLCDataMixin
#### Função: drawdowns
```
`OHLCDataMixin.get_drawdowns` with default arguments.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: MetaFeatures
#### Função: feature_config
```
Column config.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: DataWithFeatures
#### Função: feature_config
```
Column config of `${cls_name}`.

```python
${feature_config}
```
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: feature_config
```
Column config of `${cls_name}`.

```python
${feature_config}
```

Returns `${cls_name}._feature_config`, which gets (hybrid-) copied upon creation of each instance.
Thus, changing this config won't affect the class.

To change fields, you can either change the config in-place, override this property,
or overwrite the instance variable `${cls_name}._feature_config`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: use_feature_config_of
```
Copy feature config from another `Data` class.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: fix_data_dict_type
```
Fix dict type for data.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: fix_dict_types_in_kwargs
```
Fix dict types in keyword arguments.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: row_stack
```
Stack multiple `Data` instances along rows.

Uses `vectorbtpro.base.wrapping.ArrayWrapper.row_stack` to stack the wrappers.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: column_stack
```
Stack multiple `Data` instances along columns.

Uses `vectorbtpro.base.wrapping.ArrayWrapper.column_stack` to stack the wrappers.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: replace
```
See `vectorbtpro.utils.config.Configured.replace`.

Replaces the data's index and/or columns if they were changed in the wrapper.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: indexing_func
```
Perform indexing on `Data`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: data
```
Data dictionary.

Has the type `feature_dict` for feature-oriented data or `symbol_dict` for symbol-oriented data.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: dict_type
```
Return the dict type.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: column_type
```
Return the column type.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: feature_oriented
```
Whether data has features as keys.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: symbol_oriented
```
Whether data has symbols as keys.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: get_keys
```
Get keys depending on the provided dict type.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: keys
```
Keys in data.

Features if `feature_dict` and symbols if `symbol_dict`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: single_key
```
Whether there is only one key in `Data.data`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: single_feature
```
Whether there is only one feature in `Data.data`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: single_symbol
```
Whether there is only one symbol in `Data.data`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: classes
```
Key classes.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: feature_classes
```
Feature classes.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: symbol_classes
```
Symbol classes.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: get_level_name
```
Get level name(s) for keys.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: level_name
```
Level name(s) for keys.

Keys are symbols or features depending on the data dict type.

Must be a sequence if keys are tuples, otherwise a hashable.
If False, no level names will be used.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: get_key_index
```
Get key index.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: key_index
```
Key index.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: fetch_kwargs
```
Keyword arguments of type `symbol_dict` initially passed to `Data.fetch_symbol`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: returned_kwargs
```
Keyword arguments of type `symbol_dict` returned by `Data.fetch_symbol`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: last_index
```
Last fetched index per symbol of type `symbol_dict`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: delisted
```
Delisted flag per symbol of type `symbol_dict`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: tz_localize
```
Timezone to localize a datetime-naive index to, which is initially passed to `Data.pull`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: tz_convert
```
Timezone to convert a datetime-aware to, which is initially passed to `Data.pull`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: missing_index
```
Argument `missing` passed to `Data.align_index`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: missing_columns
```
Argument `missing` passed to `Data.align_columns`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: get_base_settings
```
`CustomData.get_settings` with `path_id="base"`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: has_base_settings
```
`CustomData.has_settings` with `path_id="base"`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: get_base_setting
```
`CustomData.get_setting` with `path_id="base"`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: has_base_setting
```
`CustomData.has_setting` with `path_id="base"`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: resolve_base_setting
```
`CustomData.resolve_setting` with `path_id="base"`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: set_base_settings
```
`CustomData.set_settings` with `path_id="base"`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: items
```
Iterate over columns (or groups if grouped and `Wrapping.group_select` is True), keys,
features, or symbols. The respective mode can be selected with `over`.

See `vectorbtpro.base.wrapping.Wrapping.items` for iteration over columns.
Iteration over keys supports `group_by` but doesn't support `apply_group_by`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: get_key_wrapper
```
Get wrapper with keys as columns.

If `attach_classes` is True, attaches `Data.classes` by stacking them over
the keys using `vectorbtpro.base.indexes.stack_indexes`.

Other keyword arguments are passed to the constructor of the wrapper.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: key_wrapper
```
Key wrapper.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: get_feature_wrapper
```
Get wrapper with features as columns.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: get_symbol_wrapper
```
Get wrapper with symbols as columns.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: ndim
```
Number of dimensions.

Based on the default symbol wrapper.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: shape
```
Shape.

Based on the default symbol wrapper.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: shape_2d
```
Shape as if the object was two-dimensional.

Based on the default symbol wrapper.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: columns
```
Columns.

Based on the default symbol wrapper.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: index
```
Index.

Based on the default symbol wrapper.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: freq
```
Frequency.

Based on the default symbol wrapper.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: resolve_features
```
Return the features of this instance that match the provided features.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: resolve_symbols
```
Return the symbols of this instance that match the provided symbols.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: resolve_keys
```
Return the keys of this instance that match the provided keys.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: resolve_columns
```
Return the columns of this instance that match the provided columns.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: concat
```
Concatenate keys along columns.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: get
```
Get one or more features of one or more symbols of data.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: prepare_dt_index
```
Prepare datetime index.

If `parse_dates` is True, will try to convert the index with an object data type
into a datetime format using `vectorbtpro.utils.datetime_.prepare_dt_index`.

If `tz_localize` is not None, will localize a datetime-naive index into this timezone.

If `tz_convert` is not None, will convert a datetime-aware index into this timezone.
If `force_tz_convert` is True, will convert regardless of whether the index is datetime-aware.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: prepare_dt_column
```
Prepare datetime column.

See `Data.prepare_dt_index` for arguments.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: prepare_dt
```
Prepare datetime index and columns.

If `parse_dates` is True, will try to convert any index and column with object data type
into a datetime format using `vectorbtpro.utils.datetime_.prepare_dt_index`.
If `parse_dates` is a list or dict, will first check whether the name of the column
is among the names that are in `parse_dates`.

If `to_utc` is True or `to_utc` is "index" or `to_utc` is a sequence and index name is in this
sequence, will localize/convert any datetime index to the UTC timezone. If `to_utc` is True or
`to_utc` is "columns" or `to_utc` is a sequence and column name is in this sequence, will
localize/convert any datetime column to the UTC timezone.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: prepare_tzaware_index
```
Prepare a timezone-aware index of a Pandas object.

Uses `Data.prepare_dt_index` with `parse_dates=True` and `force_tz_convert=True`.

For defaults, see `vectorbtpro._settings.data`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: align_index
```
Align data to have the same index.

The argument `missing` accepts the following values:

* 'nan': set missing data points to NaN
* 'drop': remove missing data points
* 'raise': raise an error

For defaults, see `vectorbtpro._settings.data`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: align_columns
```
Align data to have the same columns.

See `Data.align_index` for `missing`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: switch_class
```
Switch the class of the data instance.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: invert_data
```
Invert data by swapping keys and columns.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: align_data
```
Align data.

Removes any index duplicates, prepares the datetime index, and aligns the index and columns.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: from_data
```
Create a new `Data` instance from data.

Args:
    data (dict): Dictionary of array-like objects keyed by symbol.
    columns_are_symbols (bool): Whether columns in each DataFrame are symbols.
    invert_data (bool): Whether to invert the data dictionary with `Data.invert_data`.
    single_key (bool): See `Data.single_key`.
    classes (feature_dict or symbol_dict): See `Data.classes`.
    level_name (bool, hashable or iterable of hashable): See `Data.level_name`.
    tz_localize (timezone_like): See `Data.prepare_tzaware_index`.
    tz_convert (timezone_like): See `Data.prepare_tzaware_index`.
    missing_index (str): See `Data.align_index`.
    missing_columns (str): See `Data.align_columns`.
    wrapper_kwargs (dict): Keyword arguments passed to `vectorbtpro.base.wrapping.ArrayWrapper`.
    fetch_kwargs (feature_dict or symbol_dict): Keyword arguments initially passed to `Data.fetch_symbol`.
    returned_kwargs (feature_dict or symbol_dict): Keyword arguments returned by `Data.fetch_symbol`.
    last_index (feature_dict or symbol_dict): Last fetched index per symbol.
    delisted (feature_dict or symbol_dict): Whether symbol has been delisted.
    silence_warnings (bool): Whether to silence all warnings.
    **kwargs: Keyword arguments passed to the `__init__` method.

For defaults, see `vectorbtpro._settings.data`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: invert
```
Invert data and return a new instance.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: to_feature_oriented
```
Convert this instance to the feature-oriented format.

Returns self if the data is already properly formatted.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: to_symbol_oriented
```
Convert this instance to the symbol-oriented format.

Returns self if the data is already properly formatted.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: has_key_dict
```
Check whether the argument contains any data dictionary.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: check_dict_type
```
Check whether the argument conforms to a data dictionary.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: select_key_kwargs
```
Select the keyword arguments belonging to a feature or symbol.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: select_feature_kwargs
```
Select the keyword arguments belonging to a feature.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: select_symbol_kwargs
```
Select the keyword arguments belonging to a symbol.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: select_key_from_dict
```
Select the dictionary value belonging to a feature or symbol.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: select_feature_from_dict
```
Select the dictionary value belonging to a feature.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: select_symbol_from_dict
```
Select the dictionary value belonging to a symbol.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: select_from_dict
```
Select keys from a dict.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: get_intersection_dict
```
Get sub-keys and corresponding sub-values that are the same for all keys.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: select_keys
```
Create a new `Data` instance with one or more keys selected from this instance.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: select_columns
```
Create a new `Data` instance with one or more columns selected from this instance.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: select
```
Create a new `Data` instance with one or more features or symbols selected from this instance.

Will try to determine the orientation automatically.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: add_feature
```
Create a new `Data` instance with a new feature added to this instance.

If `data` is None, uses `Data.run`. If in addition `pull_feature` is True, uses `Data.pull` instead.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: add_symbol
```
Create a new `Data` instance with a new symbol added to this instance.

If `data` is None, uses `Data.pull`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: add_key
```
Create a new `Data` instance with a new key added to this instance.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: add_column
```
Create a new `Data` instance with a new column added to this instance.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: add
```
Create a new `Data` instance with a new feature or symbol added to this instance.

Will try to determine the orientation automatically.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: rename_in_dict
```
Rename keys in a dict.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: rename_keys
```
Create a new `Data` instance with keys renamed.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: rename_columns
```
Create a new `Data` instance with columns renamed.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: rename_features
```
Create a new `Data` instance with features renamed.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: rename_symbols
```
Create a new `Data` instance with symbols renamed.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: rename
```
Create a new `Data` instance with features or symbols renamed.

Will try to determine the orientation automatically.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: remove_features
```
Create a new `Data` instance with one or more features removed from this instance.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: remove_symbols
```
Create a new `Data` instance with one or more symbols removed from this instance.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: remove_keys
```
Create a new `Data` instance with one or more keys removed from this instance.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: remove_columns
```
Create a new `Data` instance with one or more columns removed from this instance.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: remove
```
Create a new `Data` instance with one or more features or symbols removed from this instance.

Will try to determine the orientation automatically.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: merge
```
Merge multiple `Data` instances.

Can merge both symbols and features. Data is overridden in the order as provided in `datas`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: fetch_feature
```
Fetch a feature.

Can also return a dictionary that will be accessible in `Data.returned_kwargs`.
If there are keyword arguments `tz_localize`, `tz_convert`, or `freq` in this dict,
will pop them and use them to override global settings.

This is an abstract method - override it to define custom logic.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: try_fetch_feature
```
Try to fetch a feature using `Data.fetch_feature`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: fetch_symbol
```
Fetch a symbol.

Can also return a dictionary that will be accessible in `Data.returned_kwargs`.
If there are keyword arguments `tz_localize`, `tz_convert`, or `freq` in this dict,
will pop them and use them to override global settings.

This is an abstract method - override it to define custom logic.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: try_fetch_symbol
```
Try to fetch a symbol using `Data.fetch_symbol`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: resolve_keys_meta
```
Resolve metadata for keys.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: pull
```
Pull data.

Fetches each feature/symbol with `Data.fetch_feature`/`Data.fetch_symbol` and prepares it with `Data.from_data`.

Iteration over features/symbols is done using `vectorbtpro.utils.execution.execute`.
That is, it can be distributed and parallelized when needed.

Args:
    keys (hashable, sequence of hashable, or dict): One or multiple keys.

        Depending on `keys_are_features` will be set to `features` or `symbols`.
    keys_are_features (bool): Whether `keys` are considered features.
    features (hashable, sequence of hashable, or dict): One or multiple features.

        If provided as a dictionary, will use keys as features and values as keyword arguments.

        !!! note
            Tuple is considered as a single feature (tuple is a hashable).
    symbols (hashable, sequence of hashable, or dict): One or multiple symbols.

        If provided as a dictionary, will use keys as symbols and values as keyword arguments.

        !!! note
            Tuple is considered as a single symbol (tuple is a hashable).
    classes (feature_dict or symbol_dict): See `Data.classes`.

        Can be a hashable (single value), a dictionary (class names as keys and
        class values as values), or a sequence of such.

        !!! note
            Tuple is considered as a single class (tuple is a hashable).
    level_name (bool, hashable or iterable of hashable): See `Data.level_name`.
    tz_localize (any): See `Data.from_data`.
    tz_convert (any): See `Data.from_data`.
    missing_index (str): See `Data.from_data`.
    missing_columns (str): See `Data.from_data`.
    wrapper_kwargs (dict): See `Data.from_data`.
    skip_on_error (bool): Whether to skip the feature/symbol when an exception is raised.
    silence_warnings (bool): Whether to silence all warnings.

        Will also forward this argument to `Data.fetch_feature`/`Data.fetch_symbol` if in the signature.
    execute_kwargs (dict): Keyword arguments passed to `vectorbtpro.utils.execution.execute`.
    return_raw (bool): Whether to return the raw outputs.
    **kwargs: Passed to `Data.fetch_feature`/`Data.fetch_symbol`.

        If two features/symbols require different keyword arguments, pass
        `key_dict` or `feature_dict`/`symbol_dict` for each argument.

For defaults, see `vectorbtpro._settings.data`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: fetch
```
Exists for backward compatibility. Use `Data.pull` instead.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: from_data_str
```
Parse a `Data` instance from a string.

For example: `YFData:BTC-USD` or just `BTC-USD` where the data class is
`vectorbtpro.data.custom.yf.YFData` by default.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: update_feature
```
Update a feature.

Can also return a dictionary that will be accessible in `Data.returned_kwargs`.

This is an abstract method - override it to define custom logic.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: try_update_feature
```
Try to update a feature using `Data.update_feature`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: update_symbol
```
Update a symbol.

Can also return a dictionary that will be accessible in `Data.returned_kwargs`.

This is an abstract method - override it to define custom logic.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: try_update_symbol
```
Try to update a symbol using `Data.update_symbol`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: update
```
Update data.

Fetches new data for each feature/symbol using `Data.update_feature`/`Data.update_symbol`.

Args:
    concat (bool): Whether to concatenate existing and updated/new data.
    skip_on_error (bool): Whether to skip the feature/symbol when an exception is raised.
    silence_warnings (bool): Whether to silence all warnings.

        Will also forward this argument to `Data.update_feature`/`Data.update_symbol`
        if accepted by `Data.fetch_feature`/`Data.fetch_symbol`.
    execute_kwargs (dict): Keyword arguments passed to `vectorbtpro.utils.execution.execute`.
    return_raw (bool): Whether to return the raw outputs.
    **kwargs: Passed to `Data.update_feature`/`Data.update_symbol`.

        If two features/symbols require different keyword arguments,
        pass `key_dict` or `feature_dict`/`symbol_dict` for each argument.

!!! note
    Returns a new `Data` instance instead of changing the data in place.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: transform
```
Transform data.

If one key (i.e., feature or symbol), passes the entire Series/DataFrame. If `per_feature` is True,
passes the Series/DataFrame of each feature. If `per_symbol` is True, passes the Series/DataFrame
of each symbol. If both are True, passes each feature and symbol combination as a Series
if `pass_frame` is False or as a DataFrame with one column if `pass_frame` is True.
If both are False, concatenates all features and symbols into a single DataFrame
and calls `transform_func` on it. Then, splits the data by key and builds a new `Data` instance.
Keyword arguments `key_wrapper_kwargs` are passed to `Data.get_key_wrapper` to control,
for example, attachment of classes.

After the transformation, the new data is aligned using `Data.align_data`.

!!! note
    The returned object must have the same type and dimensionality as the input object.

    Number of columns (i.e., features and symbols) and their names must stay the same.
    To remove columns, use either indexing or `Data.select` (depending on the data orientation).
    To add new columns, use either column stacking or `Data.merge`.

    Index, on the other hand, can be changed freely.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: dropna
```
Drop missing values.

Keyword arguments are passed to `Data.transform` and then to `pd.Series.dropna`
or `pd.DataFrame.dropna`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: resample
```
Perform resampling on `Data`.

Features "open", "high", "low", "close", "volume", "trade count", and "vwap" (case-insensitive)
are recognized and resampled automatically.

Looks for `resample_func` of each feature in `Data.feature_config`. The function must
accept the `Data` instance, object, and resampler.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: realign
```
Perform realigning on `Data`.

Looks for `realign_func` of each feature in `Data.feature_config`. If no function provided,
resamples feature "open" with `vectorbtpro.generic.accessors.GenericAccessor.realign_opening`
and other features with `vectorbtpro.generic.accessors.GenericAccessor.realign_closing`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: try_run
```
Try to run a function on data.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: select_run_func_args
```
Select positional arguments that correspond to a runnable function index or name.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: select_run_func_kwargs
```
Select keyword arguments that correspond to a runnable function index or name.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: run
```
Run a function on data.

Looks into the signature of the function and searches for arguments with the name `data` or
those found among features or attributes.

For example, the argument `open` will be substituted by `Data.open`.

`func` can be one of the following:

* Location to compute all indicators from. See `vectorbtpro.indicators.factory.IndicatorFactory.list_locations`.
* Indicator name. See `vectorbtpro.indicators.factory.IndicatorFactory.get_indicator`.
* Simulation method. See `vectorbtpro.portfolio.base.Portfolio`.
* Any callable object
* Iterable with any of the above. Will be stacked as columns into a DataFrame.

Use `magnet_kwargs` to provide keyword arguments that will be passed only if found
in the signature of the function.

Use `rename_args` to rename arguments. For example, in `vectorbtpro.portfolio.base.Portfolio`,
data can be passed instead of `close`.

Set `unpack` to True, "dict", or "frame" to use
`vectorbtpro.indicators.factory.IndicatorBase.unpack`,
`vectorbtpro.indicators.factory.IndicatorBase.to_dict`, and
`vectorbtpro.indicators.factory.IndicatorBase.to_frame` respectively.

Any argument in `*args` and `**kwargs` can be wrapped with `run_func_dict`/`run_arg_dict`
to specify the value per function/argument name or index when `func` is iterable.

Multiple function calls are executed with `vectorbtpro.utils.execution.execute`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: resolve_key_arg
```
Resolve argument.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: to_csv
```
Save data to CSV file(s).

Uses https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html

Any argument can be provided per feature using `feature_dict` or per symbol using `symbol_dict`,
depending on the format of the data dictionary.

If `path_or_buf` is a path to a directory, will save each feature/symbol to a separate file.
If there's only one file, you can specify the file path via `path_or_buf`. If there are
multiple files, use the same argument but wrap the multiple paths with `key_dict`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: from_csv
```
Use `vectorbtpro.data.custom.csv.CSVData` to load data from CSV and switch the class back to this class.

Use `fetch_kwargs` to provide keyword arguments that were originally used in fetching.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: to_hdf
```
Save data to an HDF file using PyTables.

Uses https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_hdf.html

Any argument can be provided per feature using `feature_dict` or per symbol using `symbol_dict`,
depending on the format of the data dictionary.

If `path_or_buf` exists and it's a directory, will create inside it a file named after this class.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: from_hdf
```
Use `vectorbtpro.data.custom.hdf.HDFData` to load data from HDF and switch the class back to this class.

Use `fetch_kwargs` to provide keyword arguments that were originally used in fetching.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: to_feather
```
Save data to Feather file(s) using PyArrow.

Uses https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_feather.html

Any argument can be provided per feature using `feature_dict` or per symbol using `symbol_dict`,
depending on the format of the data dictionary.

If `path_or_buf` is a path to a directory, will save each feature/symbol to a separate file.
If there's only one file, you can specify the file path via `path_or_buf`. If there are
multiple files, use the same argument but wrap the multiple paths with `key_dict`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: from_feather
```
Use `vectorbtpro.data.custom.feather.FeatherData` to load data from Feather and
switch the class back to this class.

Use `fetch_kwargs` to provide keyword arguments that were originally used in fetching.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: to_parquet
```
Save data to Parquet file(s) using PyArrow or FastParquet.

Uses https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_parquet.html

Any argument can be provided per feature using `feature_dict` or per symbol using `symbol_dict`,
depending on the format of the data dictionary.

If `path_or_buf` is a path to a directory, will save each feature/symbol to a separate file.
If there's only one file, you can specify the file path via `path_or_buf`. If there are
multiple files, use the same argument but wrap the multiple paths with `key_dict`.

If `partition_cols` and `partition_by` are None, `path_or_buf` must be a file, otherwise
it must be a directory. If `partition_by` is not None, will group the index by using
`vectorbtpro.base.wrapping.ArrayWrapper.get_index_grouper` with `**groupby_kwargs` and
put it inside `partition_cols`. In this case, `partition_cols` must be None.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: from_parquet
```
Use `vectorbtpro.data.custom.parquet.ParquetData` to load data from Parquet and
switch the class back to this class.

Use `fetch_kwargs` to provide keyword arguments that were originally used in fetching.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: to_sql
```
Save data to a SQL database using SQLAlchemy.

Uses https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_sql.html

Any argument can be provided per feature using `feature_dict` or per symbol using `symbol_dict`,
depending on the format of the data dictionary.

Each feature/symbol gets saved to a separate table.

If `engine` is None or a string, will resolve an engine with
`vectorbtpro.data.custom.sql.SQLData.resolve_engine` and dispose it afterward if `dispose_engine`
is None or True. It can additionally return the engine if `return_engine` is True or entire
metadata (all passed arguments as `feature_dict` or `symbol_dict`). In this case, the engine
won't be disposed by default.

If `schema` is not None and it doesn't exist, will create a new schema.

For `to_utc` and `remove_utc_tz`, see `Data.prepare_dt`. If `to_utc` is None, uses the
corresponding setting of `vectorbtpro.data.custom.sql.SQLData`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: from_sql
```
Use `vectorbtpro.data.custom.sql.SQLData` to load data from a SQL database and switch the class
back to this class.

Use `fetch_kwargs` to provide keyword arguments that were originally used in fetching.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: to_duckdb
```
Save data to a DuckDB database.

Any argument can be provided per feature using `feature_dict` or per symbol using `symbol_dict`,
depending on the format of the data dictionary.

If `connection` is None or a string, will resolve a connection with
`vectorbtpro.data.custom.duckdb.DuckDBData.resolve_connection`. It can additionally return the
connection if `return_connection` is True or entire metadata (all passed arguments as `feature_dict`
or `symbol_dict`). In this case, the engine won't be disposed by default.

If `write_format` is None and `write_path` is a directory (default), will persist each feature/symbol
to a table (see https://duckdb.org/docs/guides/python/import_pandas).
If `catalog` is not None, will make it default for this connection. If `schema` is not None,
and it doesn't exist, will create a new schema in the current catalog and make it default
for this connection. Any new table will be automatically created under this schema.

If `if_exists` is "fail", will raise an error if a table with the same name already exists.
If `if_exists` is "replace", will drop the existing table first. If `if_exists` is "append",
will append the new table to the existing one.

If `write_format` is not None, it must be either "csv", "parquet", or "json". If `write_path` is
a directory or has no suffix (meaning it's not a file), each feature/symbol will be saved to a
separate file under that path and with the provided `write_format` as extension. The data will be
saved using a `COPY` mechanism (see https://duckdb.org/docs/sql/statements/copy.html).
To provide options to the write operation, pass them as a dictionary or an already formatted
string (without brackets). For example, `dict(compression="gzip")` is same as "COMPRESSION 'gzip'".

For `to_utc` and `remove_utc_tz`, see `Data.prepare_dt`. If `to_utc` is None, uses the
corresponding setting of `vectorbtpro.data.custom.duckdb.DuckDBData`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: from_duckdb
```
Use `vectorbtpro.data.custom.duckdb.DuckDBData` to load data from a DuckDB database and
switch the class back to this class.

Use `fetch_kwargs` to provide keyword arguments that were originally used in fetching.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: sql
```
Run a SQL query on this instance using DuckDB.

First, connection gets established. Then, `Data.get` gets invoked with `**kwargs` passed as
keyword arguments and `as_dict=True`. Then, each returned object gets registered within the
database. Finally, the query gets executed with `duckdb.sql` and the relation as a DataFrame
gets returned. If `squeeze` is True, a DataFrame with one column will be converted into a Series.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: stats_defaults
```
Defaults for `Data.stats`.

Merges `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats_defaults` and
`stats` from `vectorbtpro._settings.data`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: plot
```
Plot either one feature of multiple symbols, or OHLC(V) of one symbol.

Args:
    column (hashable): Name of the feature or symbol to plot.

        Depends on the data orientation.
    feature (hashable): Name of the feature to plot.
    symbol (hashable): Name of the symbol to plot.
    feature_map (sequence of str): Dictionary mapping the feature names to OHLCV.

        Applied only if OHLC(V) is plotted.
    plot_volume (bool): Whether to plot volume beneath.

        Applied only if OHLC(V) is plotted.
    base (float): Rebase all series of a feature to a given initial base.

        !!! note
            The feature must contain prices.

        Applied only if lines are plotted.
    kwargs (dict): Keyword arguments passed to `vectorbtpro.generic.accessors.GenericAccessor.plot`
        for lines and to `vectorbtpro.ohlcv.accessors.OHLCVDFAccessor.plot` for OHLC(V).

Usage:
    * Plot the lines of one feature across all symbols:

    ```pycon
    >>> from vectorbtpro import *

    >>> start = '2021-01-01 UTC'  # crypto is in UTC
    >>> end = '2021-06-01 UTC'
    >>> data = vbt.YFData.pull(['BTC-USD', 'ETH-USD', 'ADA-USD'], start=start, end=end)
    ```

    [=100% "100%"]{: .candystripe .candystripe-animate }

    ```pycon
    >>> data.plot(feature='Close', base=1).show()
    ```

    * Plot OHLC(V) of one symbol (only if data contains the respective features):

    ![](/assets/images/api/data_plot.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/data_plot.dark.svg#only-dark){: .iimg loading=lazy }

    ```pycon
    >>> data.plot(symbol='BTC-USD').show()
    ```

    ![](/assets/images/api/data_plot_ohlcv.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/data_plot_ohlcv.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: plots_defaults
```
Defaults for `Data.plots`.

Merges `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots_defaults` and
`plots` from `vectorbtpro._settings.data`.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: build_feature_config_doc
```
Build feature config documentation.
```

---

# Pasta: data
### Arquivo: base.py
#### Classe: Data
#### Função: override_feature_config_doc
```
Call this method on each subclass that overrides `Data.feature_config`.
```

---

# Pasta: data
### Arquivo: decorators.py
#### Docstring do Módulo
```
Class decorators for data.
```

---

# Pasta: data
### Arquivo: decorators.py
#### Função: attach_symbol_dict_methods
```
Class decorator to attach methods for updating symbol dictionaries.
```

---

# Pasta: data
### Arquivo: nb.py
#### Docstring do Módulo
```
Numba-compiled functions for generating data.

Provides an arsenal of Numba-compiled functions that are used to generate data.
These only accept NumPy arrays and other Numba-compatible types.
```

---

# Pasta: data
### Arquivo: nb.py
#### Função: generate_random_data_1d_nb
```
Generate data using cumulative product of returns drawn from normal (Gaussian) distribution.

Turn on `symmetric` to diminish negative returns and make them symmetric to positive ones.
Otherwise, the majority of generated paths will go downward.
```

---

# Pasta: data
### Arquivo: nb.py
#### Função: generate_random_data_nb
```
2-dim version of `generate_random_data_1d_nb`.

Each argument can be provided per column thanks to flexible indexing.
```

---

# Pasta: data
### Arquivo: nb.py
#### Função: generate_gbm_data_1d_nb
```
Generate data using Geometric Brownian Motion (GBM).
```

---

# Pasta: data
### Arquivo: nb.py
#### Função: generate_gbm_data_nb
```
2-dim version of `generate_gbm_data_1d_nb`.

Each argument can be provided per column thanks to flexible indexing.
```

---

# Pasta: data
### Arquivo: saver.py
#### Docstring do Módulo
```
Classes for scheduling data saves.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: DataSaver
```
Base class for scheduling data saves.

Subclasses `vectorbtpro.data.updater.DataUpdater`.

Args:
    data (Data): Data instance.
    save_kwargs (dict): Default keyword arguments for `DataSaver.init_save_data` and `DataSaver.save_data`.
    init_save_kwargs (dict): Default keyword arguments overriding `save_kwargs` for `DataSaver.init_save_data`.
    **kwargs: Keyword arguments passed to the constructor of `DataUpdater`.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: CSVDataSaver
```
Subclass of `DataSaver` for saving data with `vectorbtpro.data.base.Data.to_csv`.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: HDFDataSaver
```
Subclass of `DataSaver` for saving data with `vectorbtpro.data.base.Data.to_hdf`.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: SQLDataSaver
```
Subclass of `DataSaver` for saving data with `vectorbtpro.data.base.Data.to_sql`.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: DuckDBDataSaver
```
Subclass of `DataSaver` for saving data with `vectorbtpro.data.base.Data.to_duckdb`.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: DataSaver
#### Função: save_kwargs
```
Keyword arguments passed to `DataSaver.save_data`.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: DataSaver
#### Função: init_save_kwargs
```
Keyword arguments passed to `DataSaver.init_save_data`.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: DataSaver
#### Função: init_save_data
```
Save initial data.

This is an abstract method - override it to define custom logic.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: DataSaver
#### Função: save_data
```
Save data.

This is an abstract method - override it to define custom logic.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: DataSaver
#### Função: update
```
Update and save data using `DataSaver.save_data`.

Override to do pre- and postprocessing.

To stop this method from running again, raise `vectorbtpro.utils.schedule_.CancelledError`.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: DataSaver
#### Função: update_every
```
Overrides `vectorbtpro.data.updater.DataUpdater` to save initial data prior to updating.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: CSVDataSaver
#### Função: init_save_data
```
Save initial data.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: CSVDataSaver
#### Função: save_data
```
Save data.

By default, appends new data without header.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: HDFDataSaver
#### Função: init_save_data
```
Save initial data.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: HDFDataSaver
#### Função: save_data
```
Save data.

By default, appends new data in a table format.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: SQLDataSaver
#### Função: init_save_data
```
Save initial data.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: SQLDataSaver
#### Função: save_data
```
Save data.

By default, appends new data without header.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: DuckDBDataSaver
#### Função: init_save_data
```
Save initial data.
```

---

# Pasta: data
### Arquivo: saver.py
#### Classe: DuckDBDataSaver
#### Função: save_data
```
Save data.

By default, appends new data without header.
```

---

# Pasta: data
### Arquivo: updater.py
#### Docstring do Módulo
```
Classes for scheduling data updates.
```

---

# Pasta: data
### Arquivo: updater.py
#### Classe: DataUpdater
```
Base class for scheduling data updates.

Args:
    data (Data): Data instance.
    update_kwargs (dict): Default keyword arguments for `DataSaver.update`.
    **kwargs: Keyword arguments passed to the constructor of `Configured`.
```

---

# Pasta: data
### Arquivo: updater.py
#### Classe: DataUpdater
#### Função: data
```
Data instance.

See `vectorbtpro.data.base.Data`.
```

---

# Pasta: data
### Arquivo: updater.py
#### Classe: DataUpdater
#### Função: schedule_manager
```
Schedule manager instance.

See `vectorbtpro.utils.schedule_.ScheduleManager`.
```

---

# Pasta: data
### Arquivo: updater.py
#### Classe: DataUpdater
#### Função: update_kwargs
```
Keyword arguments passed to `DataSaver.update`.
```

---

# Pasta: data
### Arquivo: updater.py
#### Classe: DataUpdater
#### Função: update
```
Method that updates data.

Override to do pre- and postprocessing.

To stop this method from running again, raise `vectorbtpro.utils.schedule_.CancelledError`.
```

---

# Pasta: data
### Arquivo: updater.py
#### Classe: DataUpdater
#### Função: update_every
```
Schedule `DataUpdater.update` as a job.

For `*args`, `to` and `tags`, see `vectorbtpro.utils.schedule_.ScheduleManager.every`.

If `in_background` is set to True, starts in the background as an `asyncio` task.
The task can be stopped with `vectorbtpro.utils.schedule_.ScheduleManager.stop`.

If `replace` is True, will delete scheduled jobs with the same tags, or all jobs if tags are omitted.

If `start` is False, will add the job to the scheduler without starting.

`**update_kwargs` are merged over `DataUpdater.update_kwargs` and passed to `DataUpdater.update`.
```

---

# Pasta: data
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules for working with data sources.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: alpaca.py
#### Docstring do Módulo
```
Module with `AlpacaData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: alpaca.py
#### Classe: AlpacaData
```
Data class for fetching from Alpaca.

See https://github.com/alpacahq/alpaca-py for API.

See `AlpacaData.fetch_symbol` for arguments.

Usage:
    * Set up the API key globally (optional for crypto):

    ```pycon
    >>> from vectorbtpro import *

    >>> vbt.AlpacaData.set_custom_settings(
    ...     client_config=dict(
    ...         api_key="YOUR_KEY",
    ...         secret_key="YOUR_SECRET"
    ...     )
    ... )
    ```

    * Pull stock data:

    ```pycon
    >>> data = vbt.AlpacaData.pull(
    ...     "AAPL",
    ...     start="2021-01-01",
    ...     end="2022-01-01",
    ...     timeframe="1 day"
    ... )
    ```

    * Pull crypto data:

    ```pycon
    >>> data = vbt.AlpacaData.pull(
    ...     "BTCUSD",
    ...     client_type="crypto",
    ...     start="2021-01-01",
    ...     end="2022-01-01",
    ...     timeframe="1 day"
    ... )
    ```
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: alpaca.py
#### Classe: AlpacaData
#### Função: list_symbols
```
List all symbols.

Uses `vectorbtpro.data.custom.custom.CustomData.key_match` to check each symbol against `pattern`.

Arguments `status`, `asset_class`, and `exchange` can be strings, such as `asset_class="crypto"`.
For possible values, take a look into `alpaca.trading.enums`.

!!! note
    If you get an authorization error, make sure that you either enable or disable
    the `paper` flag in `client_config` depending upon the account whose credentials you used.
    By default, the credentials are assumed to be of a live trading account (`paper=False`).
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: alpaca.py
#### Classe: AlpacaData
#### Função: resolve_client
```
Resolve the client.

If provided, must be of the type `alpaca.data.historical.CryptoHistoricalDataClient`
for `client_type="crypto"` and `alpaca.data.historical.StockHistoricalDataClient` for
`client_type="stocks"`. Otherwise, will be created using `client_config`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: alpaca.py
#### Classe: AlpacaData
#### Função: fetch_symbol
```
Override `vectorbtpro.data.base.Data.fetch_symbol` to fetch a symbol from Alpaca.

Args:
    symbol (str): Symbol.
    client (alpaca.common.rest.RESTClient): Client.

        See `AlpacaData.resolve_client`.
    client_type (str): Client type.

        See `AlpacaData.resolve_client`.
    client_config (dict): Client config.

        See `AlpacaData.resolve_client`.
    start (any): Start datetime.

        See `vectorbtpro.utils.datetime_.to_tzaware_datetime`.
    end (any): End datetime.

        See `vectorbtpro.utils.datetime_.to_tzaware_datetime`.
    timeframe (str): Timeframe.

        Allows human-readable strings such as "15 minutes".
    tz (any): Timezone.

        See `vectorbtpro.utils.datetime_.to_timezone`.
    adjustment (str): Specifies the corporate action adjustment for the returned bars.

        Options are: "raw", "split", "dividend" or "all". Default is "raw".
    feed (str): The feed to pull market data from.

        This is either "iex", "otc", or "sip". Feeds "sip" and "otc" are only available to
        those with a subscription. Default is "iex" for free plans and "sip" for paid.
    limit (int): The maximum number of returned items.

For defaults, see `custom.alpaca` in `vectorbtpro._settings.data`.
Global settings can be provided per exchange id using the `exchanges` dictionary.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: av.py
#### Docstring do Módulo
```
Module with `AVData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: av.py
#### Classe: AVData
```
Data class for fetching from Alpha Vantage.

See https://www.alphavantage.co/documentation/ for API.

Apart of using https://github.com/RomelTorres/alpha_vantage package, this class can also
parse the API documentation with `AVData.parse_api_meta` using `BeautifulSoup4` and build
the API query based on this metadata (pass `use_parser=True`).

This approach is the most flexible we can get since we can instantly react to Alpha Vantage's changes
in the API. If the data provider changes its API documentation, you can always adapt the parsing
procedure by overriding `AVData.parse_api_meta`.

If parser still fails, you can disable parsing entirely and specify all information manually
by setting `function` and disabling `match_params`

See `AVData.fetch_symbol` for arguments.

Usage:
    * Set up the API key globally (optional):

    ```pycon
    >>> from vectorbtpro import *

    >>> vbt.AVData.set_custom_settings(
    ...     apikey="YOUR_KEY"
    ... )
    ```

    * Pull data:

    ```pycon
    >>> data = vbt.AVData.pull(
    ...     "GOOGL",
    ...     timeframe="1 day",
    ... )

    >>> data = vbt.AVData.pull(
    ...     "BTC_USD",
    ...     timeframe="30 minutes",  # premium?
    ...     category="digital-currency",
    ...     outputsize="full"
    ... )

    >>> data = vbt.AVData.pull(
    ...     "REAL_GDP",
    ...     category="economic-indicators"
    ... )

    >>> data = vbt.AVData.pull(
    ...     "IBM",
    ...     category="technical-indicators",
    ...     function="STOCHRSI",
    ...     params=dict(fastkperiod=14)
    ... )
    ```
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: av.py
#### Classe: AVData
#### Função: list_symbols
```
List all symbols.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: av.py
#### Classe: AVData
#### Função: parse_api_meta
```
Parse API metadata from the documentation at https://www.alphavantage.co/documentation

Cached class method. To avoid re-parsing the same metadata in different runtimes, save it manually.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: av.py
#### Classe: AVData
#### Função: fetch_symbol
```
Fetch a symbol from Alpha Vantage.

If `use_parser` is False, or None and `alpha_vantage` is installed, uses the package.
Otherwise, parses the API documentation and pulls data directly.

See https://www.alphavantage.co/documentation/ for API endpoints and their parameters.

!!! note
    Supports the CSV format only.

Args:
    symbol (str): Symbol.

        May combine symbol/from_currency and market/to_currency using an underscore.
    use_parser (bool): Whether to use the parser instead of the `alpha_vantage` package.
    apikey (str): API key.
    api_meta (dict): API meta.

        If None, will use `AVData.parse_api_meta` if `function` is not provided
        or `match_params` is True.
    category (str or AlphaVantage): API category of your choice.

        Used if `function` is not provided or `match_params` is True.

        Supported are:

        * `alpha_vantage.alphavantage.AlphaVantage` instance, class, or class name
        * "time-series-data" or "time-series"
        * "fundamental-data" or "fundamentals"
        * "foreign-exchange", "forex", or "fx"
        * "digital-currency", "cryptocurrencies", "cryptocurrency", or "crypto"
        * "commodities"
        * "economic-indicators"
        * "technical-indicators" or "indicators"
    function (str or callable): API function of your choice.

        If None, will try to resolve it based on other arguments, such as `timeframe`,
        `adjusted`, and `extended`. Required for technical indicators, economic indicators,
        and fundamental data.

        See the keys in sub-dictionaries returned by `AVData.parse_api_meta`.
    timeframe (str): Timeframe.

        Allows human-readable strings such as "15 minutes".

        For time series, forex, and crypto, looks for interval type in the function's name.
        Defaults to "60min" if extended, otherwise to "daily".
    tz (any): Timezone.

        See `vectorbtpro.utils.datetime_.to_timezone`.
    adjusted (bool): Whether to return time series adjusted by historical split and dividend events.
    extended (bool): Whether to return historical intraday time series for the trailing 2 years.
    slice (str): Slice of the trailing 2 years.
    series_type (str): The desired price type in the time series.
    time_period (int): Number of data points used to calculate each window value.
    outputsize (str): Output size.

        Supported are

        * "compact" that returns only the latest 100 data points
        * "full" that returns the full-length time series
    match_params (bool): Whether to match parameters with the ones required by the endpoint.

        Otherwise, uses only (resolved) `function`, `apikey`, `datatype="csv"`, and `params`.
    params: Additional keyword arguments passed as key/value pairs in the URL.
    read_csv_kwargs (dict): Keyword arguments passed to `pd.read_csv`.
    silence_warnings (bool): Whether to silence all warnings.

For defaults, see `custom.av` in `vectorbtpro._settings.data`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: bento.py
#### Docstring do Módulo
```
Module with `BentoData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: bento.py
#### Classe: BentoData
```
Data class for fetching from Databento.

See https://github.com/databento/databento-python for API.

See `BentoData.fetch_symbol` for arguments.

Usage:
    * Set up the API key globally (optional):

    ```pycon
    >>> from vectorbtpro import *

    >>> vbt.BentoData.set_custom_settings(
    ...     client_config=dict(
    ...         key="YOUR_KEY"
    ...     )
    ... )
    ```

    * Pull data:

    ```pycon
    >>> data = vbt.BentoData.pull(
    ...     "AAPL",
    ...     dataset="XNAS.ITCH"
    ... )
    ```

    ```pycon
    >>> data = vbt.BentoData.pull(
    ...     "AAPL",
    ...     dataset="XNAS.ITCH",
    ...     timeframe="hourly",
    ...     start="one week ago"
    ... )
    ```

    ```pycon
    >>> data = vbt.BentoData.pull(
    ...     "ES.FUT",
    ...     dataset="GLBX.MDP3",
    ...     stype_in="parent",
    ...     schema="mbo",
    ...     start="2022-06-10T14:30",
    ...     end="2022-06-11",
    ...     limit=1000
    ... )
    ```
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: bento.py
#### Classe: BentoData
#### Função: resolve_client
```
Resolve the client.

If provided, must be of the type `databento.historical.client.Historical`.
Otherwise, will be created using `client_config`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: bento.py
#### Classe: BentoData
#### Função: get_cost
```
Get the cost of calling `BentoData.fetch_symbol` on one or more symbols.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: bento.py
#### Classe: BentoData
#### Função: fetch_symbol
```
Override `vectorbtpro.data.base.Data.fetch_symbol` to fetch a symbol from Databento.

Args:
    symbol (str): Symbol.

        Symbol can be in the `DATASET:SYMBOL` format if `dataset` is None.
    client (binance.client.Client): Client.

        See `BentoData.resolve_client`.
    client_config (dict): Client config.

        See `BentoData.resolve_client`.
    start (any): Start datetime.

        See `vectorbtpro.utils.datetime_.to_tzaware_datetime`.
    end (any): End datetime.

        See `vectorbtpro.utils.datetime_.to_tzaware_datetime`.
    resolve_dates (bool): Whether to resolve `start` and `end`, or pass them as they are.
    timeframe (str): Timeframe to create `schema` from.

        Allows human-readable strings such as "1 minute".

        If `timeframe` and `schema` are both not None, will raise an error.
    tz (any): Timezone.

        See `vectorbtpro.utils.datetime_.to_timezone`.
    dataset (str): See `databento.historical.client.Historical.get_range`.
    schema (str): See `databento.historical.client.Historical.get_range`.
    return_params (bool): Whether to return the client and (final) parameters instead of data.

        Used by `BentoData.get_cost`.
    df_kwargs (dict): Keyword arguments passed to `databento.common.dbnstore.DBNStore.to_df`.
    **params: Keyword arguments passed to `databento.historical.client.Historical.get_range`.

For defaults, see `custom.bento` in `vectorbtpro._settings.data`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: binance.py
#### Docstring do Módulo
```
Module with `BinanceData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: binance.py
#### Classe: BinanceData
```
Data class for fetching from Binance.

See https://github.com/sammchardy/python-binance for API.

See `BinanceData.fetch_symbol` for arguments.

!!! note
    If you are using an exchange from the US, Japan or other TLD then make sure pass `tld="us"`
    in `client_config` when creating the client.

Usage:
    * Set up the API key globally (optional):

    ```pycon
    >>> from vectorbtpro import *

    >>> vbt.BinanceData.set_custom_settings(
    ...     client_config=dict(
    ...         api_key="YOUR_KEY",
    ...         api_secret="YOUR_SECRET"
    ...     )
    ... )
    ```

    * Pull data:

    ```pycon
    >>> data = vbt.BinanceData.pull(
    ...     "BTCUSDT",
    ...     start="2020-01-01",
    ...     end="2021-01-01",
    ...     timeframe="1 day"
    ... )
    ```
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: binance.py
#### Classe: BinanceData
#### Função: resolve_client
```
Resolve the client.

If provided, must be of the type `binance.client.Client`.
Otherwise, will be created using `client_config`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: binance.py
#### Classe: BinanceData
#### Função: list_symbols
```
List all symbols.

Uses `vectorbtpro.data.custom.custom.CustomData.key_match` to check each symbol against `pattern`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: binance.py
#### Classe: BinanceData
#### Função: fetch_symbol
```
Override `vectorbtpro.data.base.Data.fetch_symbol` to fetch a symbol from Binance.

Args:
    symbol (str): Symbol.
    client (binance.client.Client): Client.

        See `BinanceData.resolve_client`.
    client_config (dict): Client config.

        See `BinanceData.resolve_client`.
    start (any): Start datetime.

        See `vectorbtpro.utils.datetime_.to_tzaware_datetime`.
    end (any): End datetime.

        See `vectorbtpro.utils.datetime_.to_tzaware_datetime`.
    timeframe (str): Timeframe.

        Allows human-readable strings such as "15 minutes".
    tz (any): Timezone.

        See `vectorbtpro.utils.datetime_.to_timezone`.
    klines_type (int or str): Kline type.

        See `binance.enums.HistoricalKlinesType`. Supports strings.
    limit (int): The maximum number of returned items.
    delay (float): Time to sleep after each request (in seconds).
    show_progress (bool): Whether to show the progress bar.
    pbar_kwargs (dict): Keyword arguments passed to `vectorbtpro.utils.pbar.ProgressBar`.
    silence_warnings (bool): Whether to silence all warnings.
    **get_klines_kwargs: Keyword arguments passed to `binance.client.Client.get_klines`.

For defaults, see `custom.binance` in `vectorbtpro._settings.data`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: ccxt.py
#### Docstring do Módulo
```
Module with `CCXTData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: ccxt.py
#### Classe: CCXTData
```
Data class for fetching using CCXT.

See https://github.com/ccxt/ccxt for API.

See `CCXTData.fetch_symbol` for arguments.

Usage:
    * Set up the API key globally (optional):

    ```pycon
    >>> from vectorbtpro import *

    >>> vbt.CCXTData.set_exchange_settings(
    ...     exchange_name="binance",
    ...     populate_=True,
    ...     exchange_config=dict(
    ...         apiKey="YOUR_KEY",
    ...         secret="YOUR_SECRET"
    ...     )
    ... )
    ```

    * Pull data:

    ```pycon
    >>> data = vbt.CCXTData.pull(
    ...     "BTCUSDT",
    ...     exchange="binance",
    ...     start="2020-01-01",
    ...     end="2021-01-01",
    ...     timeframe="1 day"
    ... )
    ```
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: ccxt.py
#### Classe: CCXTData
#### Função: get_exchange_settings
```
`CCXTData.get_custom_settings` with `sub_path=exchange_name`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: ccxt.py
#### Classe: CCXTData
#### Função: has_exchange_settings
```
`CCXTData.has_custom_settings` with `sub_path=exchange_name`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: ccxt.py
#### Classe: CCXTData
#### Função: get_exchange_setting
```
`CCXTData.get_custom_setting` with `sub_path=exchange_name`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: ccxt.py
#### Classe: CCXTData
#### Função: has_exchange_setting
```
`CCXTData.has_custom_setting` with `sub_path=exchange_name`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: ccxt.py
#### Classe: CCXTData
#### Função: resolve_exchange_setting
```
`CCXTData.resolve_custom_setting` with `sub_path=exchange_name`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: ccxt.py
#### Classe: CCXTData
#### Função: set_exchange_settings
```
`CCXTData.set_custom_settings` with `sub_path=exchange_name`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: ccxt.py
#### Classe: CCXTData
#### Função: list_symbols
```
List all symbols.

Uses `vectorbtpro.data.custom.custom.CustomData.key_match` to check each symbol against `pattern`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: ccxt.py
#### Classe: CCXTData
#### Função: resolve_exchange
```
Resolve the exchange.

If provided, must be of the type `ccxt.base.exchange.Exchange`.
Otherwise, will be created using `exchange_config`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: ccxt.py
#### Classe: CCXTData
#### Função: _find_earliest_date
```
Find the earliest date using binary search.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: ccxt.py
#### Classe: CCXTData
#### Função: find_earliest_date
```
Find the earliest date using binary search.

See `CCXTData.fetch_symbol` for arguments.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: ccxt.py
#### Classe: CCXTData
#### Função: fetch_symbol
```
Override `vectorbtpro.data.base.Data.fetch_symbol` to fetch a symbol from CCXT.

Args:
    symbol (str): Symbol.

        Symbol can be in the `EXCHANGE:SYMBOL` format, in this case `exchange` argument will be ignored.
    exchange (str or object): Exchange identifier or an exchange object.

        See `CCXTData.resolve_exchange`.
    exchange_config (dict): Exchange config.

        See `CCXTData.resolve_exchange`.
    start (any): Start datetime.

        See `vectorbtpro.utils.datetime_.to_tzaware_datetime`.
    end (any): End datetime.

        See `vectorbtpro.utils.datetime_.to_tzaware_datetime`.
    timeframe (str): Timeframe.

        Allows human-readable strings such as "15 minutes".
    tz (any): Timezone.

        See `vectorbtpro.utils.datetime_.to_timezone`.
    find_earliest_date (bool): Whether to find the earliest date using `CCXTData.find_earliest_date`.
    limit (int): The maximum number of returned items.
    delay (float): Time to sleep after each request (in seconds).

        !!! note
            Use only if `enableRateLimit` is not set.
    retries (int): The number of retries on failure to fetch data.
    fetch_params (dict): Exchange-specific keyword arguments passed to `fetch_ohlcv`.
    show_progress (bool): Whether to show the progress bar.
    pbar_kwargs (dict): Keyword arguments passed to `vectorbtpro.utils.pbar.ProgressBar`.
    silence_warnings (bool): Whether to silence all warnings.
    return_fetch_method (bool): Required by `CCXTData.find_earliest_date`.

For defaults, see `custom.ccxt` in `vectorbtpro._settings.data`.
Global settings can be provided per exchange id using the `exchanges` dictionary.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: csv.py
#### Docstring do Módulo
```
Module with `CSVData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: csv.py
#### Classe: CSVData
```
Data class for fetching CSV data.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: csv.py
#### Classe: CSVData
#### Função: is_csv_file
```
Return whether the path is a CSV/TSV file.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: csv.py
#### Classe: CSVData
#### Função: fetch_key
```
Fetch the CSV file of a feature or symbol.

Args:
    key (hashable): Feature or symbol.
    path (str): Path.

        If `path` is None, uses `key` as the path to the CSV file.
    start (any): Start datetime.

        Will use the timezone of the object. See `vectorbtpro.utils.datetime_.to_timestamp`.
    end (any): End datetime.

        Will use the timezone of the object. See `vectorbtpro.utils.datetime_.to_timestamp`.
    tz (any): Target timezone.

        See `vectorbtpro.utils.datetime_.to_timezone`.
    start_row (int): Start row (inclusive).

        Must exclude header rows.
    end_row (int): End row (exclusive).

        Must exclude header rows.
    header (int or sequence of int): See `pd.read_csv`.
    index_col (int): See `pd.read_csv`.

         If False, will pass None.
    parse_dates (bool): See `pd.read_csv`.
    chunk_func (callable): Function to select and concatenate chunks from `TextFileReader`.

        Gets called only if `iterator` or `chunksize` are set.
    squeeze (int): Whether to squeeze a DataFrame with one column into a Series.
    **read_kwargs: Other keyword arguments passed to `pd.read_csv`.

`skiprows` and `nrows` will be automatically calculated based on `start_row` and `end_row`.

When either `start` or `end` is provided, will fetch the entire data first and filter it thereafter.

See https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html for other arguments.

For defaults, see `custom.csv` in `vectorbtpro._settings.data`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: csv.py
#### Classe: CSVData
#### Função: fetch_feature
```
Fetch the CSV file of a feature.

Uses `CSVData.fetch_key`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: csv.py
#### Classe: CSVData
#### Função: fetch_symbol
```
Fetch the CSV file of a symbol.

Uses `CSVData.fetch_key`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: csv.py
#### Classe: CSVData
#### Função: update_key
```
Update data of a feature or symbol.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: csv.py
#### Classe: CSVData
#### Função: update_feature
```
Update data of a feature.

Uses `CSVData.update_key` with `key_is_feature=True`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: csv.py
#### Classe: CSVData
#### Função: update_symbol
```
Update data for a symbol.

Uses `CSVData.update_key` with `key_is_feature=False`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: custom.py
#### Docstring do Módulo
```
Module with `CustomData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: custom.py
#### Classe: CustomData
```
Data class for fetching custom data.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: custom.py
#### Classe: CustomData
#### Função: get_custom_settings
```
`CustomData.get_settings` with `path_id="custom"`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: custom.py
#### Classe: CustomData
#### Função: has_custom_settings
```
`CustomData.has_settings` with `path_id="custom"`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: custom.py
#### Classe: CustomData
#### Função: get_custom_setting
```
`CustomData.get_setting` with `path_id="custom"`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: custom.py
#### Classe: CustomData
#### Função: has_custom_setting
```
`CustomData.has_setting` with `path_id="custom"`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: custom.py
#### Classe: CustomData
#### Função: resolve_custom_setting
```
`CustomData.resolve_setting` with `path_id="custom"`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: custom.py
#### Classe: CustomData
#### Função: set_custom_settings
```
`CustomData.set_settings` with `path_id="custom"`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: custom.py
#### Classe: CustomData
#### Função: key_match
```
Return whether key matches pattern.

If `use_regex` is True, checks against a regular expression.
Otherwise, checks against a glob-style pattern.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: db.py
#### Docstring do Módulo
```
Module with `DBData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: db.py
#### Classe: DBData
```
Data class for fetching database data.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: duckdb.py
#### Docstring do Módulo
```
Module with `DuckDBData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: duckdb.py
#### Classe: DuckDBData
```
Data class for fetching data using DuckDB.

See `DuckDBData.pull` and `DuckDBData.fetch_key` for arguments.

Usage:
    * Set up the connection settings globally (optional):

    ```pycon
    >>> from vectorbtpro import *

    >>> vbt.DuckDBData.set_custom_settings(connection="database.duckdb")
    ```

    * Pull tables:

    ```pycon
    >>> data = vbt.DuckDBData.pull(["TABLE1", "TABLE2"])
    ```

    * Rename tables:

    ```pycon
    >>> data = vbt.DuckDBData.pull(
    ...     ["SYMBOL1", "SYMBOL2"],
    ...     table=vbt.key_dict({
    ...         "SYMBOL1": "TABLE1",
    ...         "SYMBOL2": "TABLE2"
    ...     })
    ... )
    ```

    * Pull queries:

    ```pycon
    >>> data = vbt.DuckDBData.pull(
    ...     ["SYMBOL1", "SYMBOL2"],
    ...     query=vbt.key_dict({
    ...         "SYMBOL1": "SELECT * FROM TABLE1",
    ...         "SYMBOL2": "SELECT * FROM TABLE2"
    ...     })
    ... )
    ```

    * Pull Parquet files:

    ```pycon
    >>> data = vbt.DuckDBData.pull(
    ...     ["SYMBOL1", "SYMBOL2"],
    ...     read_path=vbt.key_dict({
    ...         "SYMBOL1": "s1.parquet",
    ...         "SYMBOL2": "s2.parquet"
    ...     })
    ... )
    ```
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: duckdb.py
#### Classe: DuckDBData
#### Função: resolve_connection
```
Resolve the connection.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: duckdb.py
#### Classe: DuckDBData
#### Função: list_catalogs
```
List all catalogs.

Catalogs "system" and "temp" are skipped if `incl_system` is False.

Uses `vectorbtpro.data.custom.custom.CustomData.key_match` to check each symbol against `pattern`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: duckdb.py
#### Classe: DuckDBData
#### Função: list_schemas
```
List all schemas.

If `catalog` is None, searches for all catalog names in the database and prefixes each schema
with the respective catalog name. If `catalog` is provided, returns the schemas corresponding
to this catalog without a prefix. Schemas "information_schema" and "pg_catalog" are skipped
if `incl_system` is False.

Uses `vectorbtpro.data.custom.custom.CustomData.key_match` to check each symbol against `pattern`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: duckdb.py
#### Classe: DuckDBData
#### Função: get_current_schema
```
Get the current schema.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: duckdb.py
#### Classe: DuckDBData
#### Função: list_tables
```
List all tables and views.

If `schema` is None, searches for all schema names in the database and prefixes each table
with the respective catalog and schema name (unless there's only one schema which is the current
schema or `schema` is `current_schema`). If `schema` is provided, returns the tables corresponding
to this schema without a prefix.

Uses `vectorbtpro.data.custom.custom.CustomData.key_match` to check each schema against
`schema_pattern` and each table against `table_pattern`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: duckdb.py
#### Classe: DuckDBData
#### Função: pull
```
Override `vectorbtpro.data.base.Data.pull` to resolve and share the connection among the keys
and use the table names available in the database in case no keys were provided.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: duckdb.py
#### Classe: DuckDBData
#### Função: format_write_option
```
Format a write option.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: duckdb.py
#### Classe: DuckDBData
#### Função: format_write_options
```
Format write options.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: duckdb.py
#### Classe: DuckDBData
#### Função: format_read_option
```
Format a read option.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: duckdb.py
#### Classe: DuckDBData
#### Função: format_read_options
```
Format read options.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: duckdb.py
#### Classe: DuckDBData
#### Função: fetch_key
```
Fetch a feature or symbol from a DuckDB database.

Can use a table name (which defaults to the key) or a custom query.

Args:
    key (str): Feature or symbol.

        If `table` and `query` are both None, becomes the table name.

        Key can be in the `SCHEMA:TABLE` format, in this case `schema` argument will be ignored.
    table (str): Table name.

        Cannot be used together with `file` or `query`.
    schema (str): Schema name.

        Cannot be used together with `file` or `query`.
    catalog (str): Catalog name.

        Cannot be used together with ``file` or query`.
    read_path (path_like): Path to a file to read.

        Cannot be used together with `table`, `schema`, `catalog`, or `query`.
    read_format (str): Format of the file to read.

        Allowed values are "csv", "parquet", and "json".

        Requires `read_path` to be set.
    read_options (str or dict): Options used to read the file.

        Requires `read_path` and `read_format` to be set.

        Uses `DuckDBData.format_read_options` to transform a dictionary to a string.
    query (str or DuckDBPyRelation): Custom query.

        Cannot be used together with `catalog`, `schema`, and `table`.
    connection (str or object): See `DuckDBData.resolve_connection`.
    connection_config (dict): See `DuckDBData.resolve_connection`.
    start (any): Start datetime (if datetime index) or any other start value.

        Will parse with `vectorbtpro.utils.datetime_.to_timestamp` if `align_dates` is True
        and the index is a datetime index. Otherwise, you must ensure the correct type is provided.

        Cannot be used together with `query`. Include the condition into the query.
    end (any): End datetime (if datetime index) or any other end value.

        Will parse with `vectorbtpro.utils.datetime_.to_timestamp` if `align_dates` is True
        and the index is a datetime index. Otherwise, you must ensure the correct type is provided.

        Cannot be used together with `query`. Include the condition into the query.
    align_dates (bool): Whether to align `start` and `end` to the timezone of the index.

        Will pull one row (using `LIMIT 1`) and use `SQLData.prepare_dt` to get the index.
    parse_dates (bool or sequence of str): See `DuckDBData.prepare_dt`.
    to_utc (bool, str, or sequence of str): See `DuckDBData.prepare_dt`.
    tz (any): Timezone.

        See `vectorbtpro.utils.datetime_.to_timezone`.
    index_col (int, str, or list): One or more columns that should become the index.
    squeeze (int): Whether to squeeze a DataFrame with one column into a Series.
    df_kwargs (dict): Keyword arguments passed to `relation.df` to convert a relation to a DataFrame.
    **sql_kwargs: Other keyword arguments passed to `connection.execute` to run a SQL query.

For defaults, see `custom.duckdb` in `vectorbtpro._settings.data`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: duckdb.py
#### Classe: DuckDBData
#### Função: fetch_feature
```
Fetch the table of a feature.

Uses `DuckDBData.fetch_key`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: duckdb.py
#### Classe: DuckDBData
#### Função: fetch_symbol
```
Fetch the table for a symbol.

Uses `DuckDBData.fetch_key`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: duckdb.py
#### Classe: DuckDBData
#### Função: update_key
```
Update data of a feature or symbol.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: duckdb.py
#### Classe: DuckDBData
#### Função: update_feature
```
Update data of a feature.

Uses `DuckDBData.update_key`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: duckdb.py
#### Classe: DuckDBData
#### Função: update_symbol
```
Update data for a symbol.

Uses `DuckDBData.update_key`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: feather.py
#### Docstring do Módulo
```
Module with `FeatherData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: feather.py
#### Classe: FeatherData
```
Data class for fetching Feather data using PyArrow.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: feather.py
#### Classe: FeatherData
#### Função: fetch_key
```
Fetch the Feather file of a feature or symbol.

Args:
    key (hashable): Feature or symbol.
    path (str): Path.

        If `path` is None, uses `key` as the path to the Feather file.
    tz (any): Target timezone.

        See `vectorbtpro.utils.datetime_.to_timezone`.
    index_col (int, str, or sequence): Position(s) or name(s) of column(s) that should become the index.

        Will only apply if the fetched object has a default index.
    squeeze (int): Whether to squeeze a DataFrame with one column into a Series.
    **read_kwargs: Other keyword arguments passed to `pd.read_feather`.

See https://pandas.pydata.org/docs/reference/api/pandas.read_feather.html for other arguments.

For defaults, see `custom.feather` in `vectorbtpro._settings.data`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: feather.py
#### Classe: FeatherData
#### Função: fetch_feature
```
Fetch the Feather file of a feature.

Uses `FeatherData.fetch_key`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: feather.py
#### Classe: FeatherData
#### Função: fetch_symbol
```
Fetch the Feather file of a symbol.

Uses `FeatherData.fetch_key`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: feather.py
#### Classe: FeatherData
#### Função: update_key
```
Update data of a feature or symbol.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: feather.py
#### Classe: FeatherData
#### Função: update_feature
```
Update data of a feature.

Uses `FeatherData.update_key` with `key_is_feature=True`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: feather.py
#### Classe: FeatherData
#### Função: update_symbol
```
Update data for a symbol.

Uses `FeatherData.update_key` with `key_is_feature=False`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: file.py
#### Docstring do Módulo
```
Module with `FileData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: file.py
#### Classe: FileData
```
Data class for fetching file data.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: file.py
#### Classe: FileData
#### Função: is_dir_match
```
Return whether a directory is a valid match.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: file.py
#### Classe: FileData
#### Função: is_file_match
```
Return whether a file is a valid match.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: file.py
#### Classe: FileData
#### Função: match_path
```
Get the list of all paths matching a path.

If `FileData.is_dir_match` returns True for a directory, it gets returned as-is.
Otherwise, iterates through all files in that directory and invokes `FileData.is_file_match`.
If a pattern was provided, these methods aren't invoked.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: file.py
#### Classe: FileData
#### Função: list_paths
```
List all features or symbols under a path.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: file.py
#### Classe: FileData
#### Função: path_to_key
```
Convert a path into a feature or symbol.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: file.py
#### Classe: FileData
#### Função: pull
```
Override `vectorbtpro.data.base.Data.pull` to take care of paths.

Use either features, symbols, or `paths` to specify the path to one or multiple files.
Allowed are paths in a string or `pathlib.Path` format, or string expressions accepted by `glob.glob`.

Set `match_paths` to False to not parse paths and behave like a regular
`vectorbtpro.data.base.Data` instance.

For defaults, see `custom.local` in `vectorbtpro._settings.data`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: finpy.py
#### Docstring do Módulo
```
Module with `FinPyData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: finpy.py
#### Classe: FinPyData
```
Data class for fetching using findatapy.

See https://github.com/cuemacro/findatapy for API.

See `FinPyData.fetch_symbol` for arguments.

Usage:
    * Pull data (keyword argument format):

    ```pycon
    >>> data = vbt.FinPyData.pull(
    ...     "EURUSD",
    ...     start="14 June 2016",
    ...     end="15 June 2016",
    ...     timeframe="tick",
    ...     category="fx",
    ...     fields=["bid", "ask"],
    ...     data_source="dukascopy"
    ... )
    ```

    * Pull data (string format):

    ```pycon
    >>> data = vbt.FinPyData.pull(
    ...     "fx.dukascopy.tick.NYC.EURUSD.bid,ask",
    ...     start="14 June 2016",
    ...     end="15 June 2016",
    ... )
    ```
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: finpy.py
#### Classe: FinPyData
#### Função: resolve_market
```
Resolve the market.

If provided, must be of the type `findatapy.market.market.Market`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: finpy.py
#### Classe: FinPyData
#### Função: resolve_config_manager
```
Resolve the config manager.

If provided, must be of the type `findatapy.util.ConfigManager`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: finpy.py
#### Classe: FinPyData
#### Função: list_symbols
```
List all symbols.

Passes most arguments to `findatapy.util.ConfigManager.free_form_tickers_regex_query`.

Uses `vectorbtpro.data.custom.custom.CustomData.key_match` to check each symbol against `pattern`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: finpy.py
#### Classe: FinPyData
#### Função: fetch_symbol
```
Override `vectorbtpro.data.base.Data.fetch_symbol` to fetch a symbol from findatapy.

Args:
    symbol (str): Symbol.

        Also accepts the format such as "fx.bloomberg.daily.NYC.EURUSD.close".
        The fields `freq`, `cut`, `tickers`, and `fields` here are optional.
    market (findatapy.market.market.Market): Market.

        See `FinPyData.resolve_market`.
    market_config (dict): Client config.

        See `FinPyData.resolve_market`.
    start (any): Start datetime.

        See `vectorbtpro.utils.datetime_.to_tzaware_datetime`.
    end (any): End datetime.

        See `vectorbtpro.utils.datetime_.to_tzaware_datetime`.
    timeframe (str): Timeframe.

        Allows human-readable strings such as "15 minutes".
    tz (any): Timezone.

        See `vectorbtpro.utils.datetime_.to_timezone`.
    **request_kwargs: Other keyword arguments passed to `findatapy.market.marketdatarequest.MarketDataRequest`.

For defaults, see `custom.finpy` in `vectorbtpro._settings.data`.
Global settings can be provided per exchange id using the `exchanges` dictionary.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: gbm.py
#### Docstring do Módulo
```
Module with `GBMData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: gbm.py
#### Classe: GBMData
```
`SyntheticData` for data generated using `vectorbtpro.data.nb.generate_gbm_data_nb`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: gbm.py
#### Classe: GBMData
#### Função: generate_key
```
Generate a feature or symbol.

Args:
    key (hashable): Feature or symbol.
    index (pd.Index): Pandas index.
    columns (hashable or index_like): Column names.

        Provide a single value (hashable) to make a Series.
    start_value (float): Value at time 0.

        Does not appear as the first value in the output data.
    mean (float): Drift, or mean of the percentage change.
    std (float): Standard deviation of the percentage change.
    dt (float): Time change (one period of time).
    seed (int): Seed to make output deterministic.
    jitted (any): See `vectorbtpro.utils.jitting.resolve_jitted_option`.

For defaults, see `custom.gbm` in `vectorbtpro._settings.data`.

!!! note
    When setting a seed, remember to pass a seed per feature/symbol using
    `vectorbtpro.data.base.feature_dict`/`vectorbtpro.data.base.symbol_dict` or generally
    `vectorbtpro.data.base.key_dict`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: gbm_ohlc.py
#### Docstring do Módulo
```
Module with `GBMOHLCData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: gbm_ohlc.py
#### Classe: GBMOHLCData
```
`SyntheticData` for data generated using `vectorbtpro.data.nb.generate_gbm_data_1d_nb`
and then resampled using `vectorbtpro.ohlcv.nb.ohlc_every_1d_nb`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: gbm_ohlc.py
#### Classe: GBMOHLCData
#### Função: generate_symbol
```
Generate a symbol.

Args:
    symbol (hashable): Symbol.
    index (pd.Index): Pandas index.
    n_ticks (int or array_like): Number of ticks per bar.

        Flexible argument. Can be a template with a context containing `symbol` and `index`.
    start_value (float): Value at time 0.

        Does not appear as the first value in the output data.
    mean (float): Drift, or mean of the percentage change.
    std (float): Standard deviation of the percentage change.
    dt (float): Time change (one period of time).
    seed (int): Seed to make output deterministic.
    jitted (any): See `vectorbtpro.utils.jitting.resolve_jitted_option`.
    template_context (dict): Context used to substitute templates.

For defaults, see `custom.gbm` in `vectorbtpro._settings.data`.

!!! note
    When setting a seed, remember to pass a seed per symbol using `vectorbtpro.data.base.symbol_dict`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: hdf.py
#### Docstring do Módulo
```
Module with `HDFData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: hdf.py
#### Classe: HDFPathNotFoundError
```
Gets raised if the path to an HDF file could not be found.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: hdf.py
#### Classe: HDFKeyNotFoundError
```
Gets raised if the key to an HDF object could not be found.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: hdf.py
#### Classe: HDFData
```
Data class for fetching HDF data using PyTables.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: hdf.py
#### Classe: HDFData
#### Função: is_hdf_file
```
Return whether the path is an HDF file.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: hdf.py
#### Classe: HDFData
#### Função: split_hdf_path
```
Split the path to an HDF object into the path to the file and the key.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: hdf.py
#### Classe: HDFData
#### Função: match_path
```
Override `FileData.match_path` to return a list of HDF paths
(path to file + key) matching a path.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: hdf.py
#### Classe: HDFData
#### Função: fetch_key
```
Fetch the HDF object of a feature or symbol.

Args:
    key (hashable): Feature or symbol.
    path (str): Path.

        Will be resolved with `HDFData.split_hdf_path`.

        If `path` is None, uses `key` as the path to the HDF file.
    start (any): Start datetime.

        Will extract the object's index and compare the index to the date.
        Will use the timezone of the object. See `vectorbtpro.utils.datetime_.to_timestamp`.

        !!! note
            Can only be used if the object was saved in the table format!
    end (any): End datetime.

        Will extract the object's index and compare the index to the date.
        Will use the timezone of the object. See `vectorbtpro.utils.datetime_.to_timestamp`.

        !!! note
            Can only be used if the object was saved in the table format!
    tz (any): Target timezone.

        See `vectorbtpro.utils.datetime_.to_timezone`.
    start_row (int): Start row (inclusive).

        Will use it when querying index as well.
    end_row (int): End row (exclusive).

        Will use it when querying index as well.
    chunk_func (callable): Function to select and concatenate chunks from `TableIterator`.

        Gets called only if `iterator` or `chunksize` are set.
    **read_kwargs: Other keyword arguments passed to `pd.read_hdf`.

See https://pandas.pydata.org/docs/reference/api/pandas.read_hdf.html for other arguments.

For defaults, see `custom.hdf` in `vectorbtpro._settings.data`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: hdf.py
#### Classe: HDFData
#### Função: fetch_feature
```
Fetch the HDF object of a feature.

Uses `HDFData.fetch_key`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: hdf.py
#### Classe: HDFData
#### Função: fetch_symbol
```
Load the HDF object for a symbol.

Uses `HDFData.fetch_key`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: hdf.py
#### Classe: HDFData
#### Função: update_key
```
Update data of a feature or symbol.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: hdf.py
#### Classe: HDFData
#### Função: update_feature
```
Update data of a feature.

Uses `HDFData.update_key` with `key_is_feature=True`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: hdf.py
#### Classe: HDFData
#### Função: update_symbol
```
Update data for a symbol.

Uses `HDFData.update_key` with `key_is_feature=False`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: local.py
#### Docstring do Módulo
```
Module with `LocalData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: local.py
#### Classe: LocalData
```
Data class for fetching local data.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: ndl.py
#### Docstring do Módulo
```
Module with `NDLData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: ndl.py
#### Classe: NDLData
```
Data class for fetching from Nasdaq Data Link.

See https://github.com/Nasdaq/data-link-python for API.

See `NDLData.fetch_symbol` for arguments.

Usage:
    * Set up the API key globally (optional):

    ```pycon
    >>> from vectorbtpro import *

    >>> vbt.NDLData.set_custom_settings(
    ...     api_key="YOUR_KEY"
    ... )
    ```

    * Pull a dataset:

    ```pycon
    >>> data = vbt.NDLData.pull(
    ...     "FRED/GDP",
    ...     start="2001-12-31",
    ...     end="2005-12-31"
    ... )
    ```

    * Pull a datatable:

    ```pycon
    >>> data = vbt.NDLData.pull(
    ...     "MER/F1",
    ...     data_format="datatable",
    ...     compnumber="39102",
    ...     paginate=True
    ... )
    ```
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: ndl.py
#### Classe: NDLData
#### Função: fetch_symbol
```
Override `vectorbtpro.data.base.Data.fetch_symbol` to fetch a symbol from Nasdaq Data Link.

Args:
    symbol (str): Symbol.
    api_key (str): API key.
    data_format (str): Data format.

        Supported are "dataset" and "datatable".
    start (any): Retrieve data rows on and after the specified start date.

        See `vectorbtpro.utils.datetime_.to_tzaware_datetime`.
    end (any): Retrieve data rows up to and including the specified end date.

        See `vectorbtpro.utils.datetime_.to_tzaware_datetime`.
    tz (any): Timezone.

        See `vectorbtpro.utils.datetime_.to_timezone`.
    column_indices (int or iterable): Request one or more specific columns.

        Column 0 is the date column and is always returned. Data begins at column 1.
    **params: Keyword arguments sent as field/value params to Nasdaq Data Link with no interference.

For defaults, see `custom.ndl` in `vectorbtpro._settings.data`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: parquet.py
#### Docstring do Módulo
```
Module with `ParquetData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: parquet.py
#### Classe: ParquetData
```
Data class for fetching Parquet data using PyArrow or FastParquet.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: parquet.py
#### Classe: ParquetData
#### Função: is_parquet_file
```
Return whether the path is a Parquet file.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: parquet.py
#### Classe: ParquetData
#### Função: is_parquet_group_dir
```
Return whether the path is a directory that is a group of Parquet partitions.

!!! note
    Assumes the Hive partitioning scheme.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: parquet.py
#### Classe: ParquetData
#### Função: is_parquet_dir
```
Return whether the path is a directory that is a group itself or
contains groups of Parquet partitions.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: parquet.py
#### Classe: ParquetData
#### Função: list_partition_cols
```
List partitioning columns under a path.

!!! note
    Assumes the Hive partitioning scheme.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: parquet.py
#### Classe: ParquetData
#### Função: is_default_partition_col
```
Return whether a partitioning column is a default partitioning column.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: parquet.py
#### Classe: ParquetData
#### Função: fetch_key
```
Fetch the Parquet file of a feature or symbol.

Args:
    key (hashable): Feature or symbol.
    path (str): Path.

        If `path` is None, uses `key` as the path to the Parquet file.
    tz (any): Target timezone.

        See `vectorbtpro.utils.datetime_.to_timezone`.
    squeeze (int): Whether to squeeze a DataFrame with one column into a Series.
    keep_partition_cols (bool): Whether to return partitioning columns (if any).

        If None, will remove any partitioning column that is "group" or "group_{index}".

        Retrieves the list of partitioning columns with `ParquetData.list_partition_cols`.
    engine (str): See `pd.read_parquet`.
    **read_kwargs: Other keyword arguments passed to `pd.read_parquet`.

See https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html for other arguments.

For defaults, see `custom.parquet` in `vectorbtpro._settings.data`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: parquet.py
#### Classe: ParquetData
#### Função: fetch_feature
```
Fetch the Parquet file of a feature.

Uses `ParquetData.fetch_key`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: parquet.py
#### Classe: ParquetData
#### Função: fetch_symbol
```
Fetch the Parquet file of a symbol.

Uses `ParquetData.fetch_key`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: parquet.py
#### Classe: ParquetData
#### Função: update_key
```
Update data of a feature or symbol.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: parquet.py
#### Classe: ParquetData
#### Função: update_feature
```
Update data of a feature.

Uses `ParquetData.update_key` with `key_is_feature=True`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: parquet.py
#### Classe: ParquetData
#### Função: update_symbol
```
Update data for a symbol.

Uses `ParquetData.update_key` with `key_is_feature=False`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: polygon.py
#### Docstring do Módulo
```
Module with `PolygonData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: polygon.py
#### Classe: PolygonData
```
Data class for fetching from Polygon.

See https://github.com/polygon-io/client-python for API.

See `PolygonData.fetch_symbol` for arguments.

Usage:
    * Set up the API key globally:

    ```pycon
    >>> from vectorbtpro import *

    >>> vbt.PolygonData.set_custom_settings(
    ...     client_config=dict(
    ...         api_key="YOUR_KEY"
    ...     )
    ... )
    ```

    * Pull stock data:

    ```pycon
    >>> data = vbt.PolygonData.pull(
    ...     "AAPL",
    ...     start="2021-01-01",
    ...     end="2022-01-01",
    ...     timeframe="1 day"
    ... )
    ```

    * Pull crypto data:

    ```pycon
    >>> data = vbt.PolygonData.pull(
    ...     "X:BTCUSD",
    ...     start="2021-01-01",
    ...     end="2022-01-01",
    ...     timeframe="1 day"
    ... )
    ```
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: polygon.py
#### Classe: PolygonData
#### Função: list_symbols
```
List all symbols.

Uses `vectorbtpro.data.custom.custom.CustomData.key_match` to check each symbol against `pattern`.

For supported keyword arguments, see `polygon.RESTClient.list_tickers`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: polygon.py
#### Classe: PolygonData
#### Função: resolve_client
```
Resolve the client.

If provided, must be of the type `polygon.rest.RESTClient`.
Otherwise, will be created using `client_config`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: polygon.py
#### Classe: PolygonData
#### Função: fetch_symbol
```
Override `vectorbtpro.data.base.Data.fetch_symbol` to fetch a symbol from Polygon.

Args:
    symbol (str): Symbol.

        Supports the following APIs:

        * Stocks and equities
        * Currencies - symbol must have the prefix `C:`
        * Crypto - symbol must have the prefix `X:`
    client (polygon.rest.RESTClient): Client.

        See `PolygonData.resolve_client`.
    client_config (dict): Client config.

        See `PolygonData.resolve_client`.
    start (any): The start of the aggregate time window.

        See `vectorbtpro.utils.datetime_.to_tzaware_datetime`.
    end (any): The end of the aggregate time window.

        See `vectorbtpro.utils.datetime_.to_tzaware_datetime`.
    timeframe (str): Timeframe.

        Allows human-readable strings such as "15 minutes".
    tz (any): Timezone.

        See `vectorbtpro.utils.datetime_.to_timezone`.
    adjusted (str): Whether the results are adjusted for splits.

        By default, results are adjusted.
        Set this to False to get results that are NOT adjusted for splits.
    limit (int): Limits the number of base aggregates queried to create the aggregate results.

        Max 50000 and Default 5000.
    params (dict): Any additional query params.
    delay (float): Time to sleep after each request (in seconds).
    retries (int): The number of retries on failure to fetch data.
    show_progress (bool): Whether to show the progress bar.
    pbar_kwargs (dict): Keyword arguments passed to `vectorbtpro.utils.pbar.ProgressBar`.
    silence_warnings (bool): Whether to silence all warnings.

For defaults, see `custom.polygon` in `vectorbtpro._settings.data`.

!!! note
    If you're using a free plan that has an API rate limit of several requests per minute,
    make sure to set `delay` to a higher number, such as 12 (which makes 5 requests per minute).
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: random.py
#### Docstring do Módulo
```
Module with `RandomData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: random.py
#### Classe: RandomData
```
`SyntheticData` for data generated using `vectorbtpro.data.nb.generate_random_data_nb`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: random.py
#### Classe: RandomData
#### Função: generate_key
```
Generate a feature or symbol.

Args:
    key (hashable): Feature or symbol.
    index (pd.Index): Pandas index.
    columns (hashable or index_like): Column names.

        Provide a single value (hashable) to make a Series.
    start_value (float): Value at time 0.

        Does not appear as the first value in the output data.
    mean (float): Drift, or mean of the percentage change.
    std (float): Standard deviation of the percentage change.
    symmetric (bool): Whether to diminish negative returns and make them symmetric to positive ones.
    seed (int): Seed to make output deterministic.
    jitted (any): See `vectorbtpro.utils.jitting.resolve_jitted_option`.

For defaults, see `custom.random` in `vectorbtpro._settings.data`.

!!! note
    When setting a seed, remember to pass a seed per feature/symbol using
    `vectorbtpro.data.base.feature_dict`/`vectorbtpro.data.base.symbol_dict` or generally
    `vectorbtpro.data.base.key_dict`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: random_ohlc.py
#### Docstring do Módulo
```
Module with `RandomOHLCData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: random_ohlc.py
#### Classe: RandomOHLCData
```
`SyntheticData` for data generated using `vectorbtpro.data.nb.generate_random_data_1d_nb`
and then resampled using `vectorbtpro.ohlcv.nb.ohlc_every_1d_nb`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: random_ohlc.py
#### Classe: RandomOHLCData
#### Função: generate_symbol
```
Generate a symbol.

Args:
    symbol (hashable): Symbol.
    index (pd.Index): Pandas index.
    n_ticks (int or array_like): Number of ticks per bar.

        Flexible argument. Can be a template with a context containing `symbol` and `index`.
    start_value (float): Value at time 0.

        Does not appear as the first value in the output data.
    mean (float): Drift, or mean of the percentage change.
    std (float): Standard deviation of the percentage change.
    symmetric (bool): Whether to diminish negative returns and make them symmetric to positive ones.
    seed (int): Seed to make output deterministic.
    jitted (any): See `vectorbtpro.utils.jitting.resolve_jitted_option`.
    template_context (dict): Template context.

For defaults, see `custom.random_ohlc` in `vectorbtpro._settings.data`.

!!! note
    When setting a seed, remember to pass a seed per symbol using `vectorbtpro.data.base.symbol_dict`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: remote.py
#### Docstring do Módulo
```
Module with `RemoteData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: remote.py
#### Classe: RemoteData
```
Data class for fetching remote data.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Docstring do Módulo
```
Module with `SQLData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
```
Data class for fetching data from a database using SQLAlchemy.

See https://www.sqlalchemy.org/ for the SQLAlchemy's API.

See https://pandas.pydata.org/docs/reference/api/pandas.read_sql_query.html for the read method.

See `SQLData.pull` and `SQLData.fetch_key` for arguments.

Usage:
    * Set up the engine settings globally (optional):

    ```pycon
    >>> from vectorbtpro import *

    >>> vbt.SQLData.set_engine_settings(
    ...     engine_name="postgresql",
    ...     populate_=True,
    ...     engine="postgresql+psycopg2://...",
    ...     engine_config=dict(),
    ...     schema="public"
    ... )
    ```

    * Pull tables:

    ```pycon
    >>> data = vbt.SQLData.pull(
    ...     ["TABLE1", "TABLE2"],
    ...     engine="postgresql",
    ...     start="2020-01-01",
    ...     end="2021-01-01"
    ... )
    ```

    * Pull queries:

    ```pycon
    >>> data = vbt.SQLData.pull(
    ...     ["SYMBOL1", "SYMBOL2"],
    ...     query=vbt.key_dict({
    ...         "SYMBOL1": "SELECT * FROM TABLE1",
    ...         "SYMBOL2": "SELECT * FROM TABLE2"
    ...     }),
    ...     engine="postgresql"
    ... )
    ```
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: get_engine_settings
```
`SQLData.get_custom_settings` with `sub_path=engine_name`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: has_engine_settings
```
`SQLData.has_custom_settings` with `sub_path=engine_name`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: get_engine_setting
```
`SQLData.get_custom_setting` with `sub_path=engine_name`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: has_engine_setting
```
`SQLData.has_custom_setting` with `sub_path=engine_name`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: resolve_engine_setting
```
`SQLData.resolve_custom_setting` with `sub_path=engine_name`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: set_engine_settings
```
`SQLData.set_custom_settings` with `sub_path=engine_name`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: resolve_engine
```
Resolve the engine.

Argument `engine` can be

1) an object of the type `sqlalchemy.engine.base.Engine`,
2) a URL of the engine as a string, which will be used to create an engine with
`sqlalchemy.engine.create.create_engine` and `engine_config` passed as keyword arguments
(you should not include `url` in the `engine_config`), or
3) an engine name, which is the name of a sub-config with engine settings under `custom.sql.engines`
in `vectorbtpro._settings.data`. Such a sub-config can then contain the actual engine as an object or a URL.

Argument `engine_name` can be provided instead of `engine`, or also together with `engine`
to pull other settings from a sub-config. URLs can also be used as engine names, but not the
other way around.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: list_schemas
```
List all schemas.

Uses `vectorbtpro.data.custom.custom.CustomData.key_match` to check each symbol against `pattern`.

Keyword arguments `**kwargs` are passed to `inspector.get_schema_names`.

If `dispose_engine` is None, disposes the engine if it wasn't provided.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: list_tables
```
List all tables and views.

If `schema` is None, searches for all schema names in the database and prefixes each table
with the respective schema name (unless there's only one schema "main"). If `schema` is False,
sets the schema to None. If `schema` is provided, returns the tables corresponding to this
schema without a prefix.

Uses `vectorbtpro.data.custom.custom.CustomData.key_match` to check each schema against
`schema_pattern` and each table against `table_pattern`.

Keyword arguments `**kwargs` are passed to `inspector.get_table_names`.

If `dispose_engine` is None, disposes the engine if it wasn't provided.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: has_schema
```
Check whether the database has a schema.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: create_schema
```
Create a schema if it doesn't exist yet.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: has_table
```
Check whether the database has a table.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: get_table_relation
```
Get table relation.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: get_last_row_number
```
Get last row number.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: pull
```
Override `vectorbtpro.data.base.Data.pull` to resolve and share the engine among the keys
and use the table names available in the database in case no keys were provided.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: fetch_key
```
Fetch a feature or symbol from a SQL database.

Can use a table name (which defaults to the key) or a custom query.

Args:
    key (str): Feature or symbol.

        If `table` and `query` are both None, becomes the table name.

        Key can be in the `SCHEMA:TABLE` format, in this case `schema` argument will be ignored.
    table (str or Table): Table name or actual object.

        Cannot be used together with `query`.
    schema (str): Schema.

        Cannot be used together with `query`.
    query (str or Selectable): Custom query.

        Cannot be used together with `table` and `schema`.
    engine (str or object): See `SQLData.resolve_engine`.
    engine_name (str): See `SQLData.resolve_engine`.
    engine_config (dict): See `SQLData.resolve_engine`.
    dispose_engine (bool): See `SQLData.resolve_engine`.
    start (any): Start datetime (if datetime index) or any other start value.

        Will parse with `vectorbtpro.utils.datetime_.to_timestamp` if `align_dates` is True
        and the index is a datetime index. Otherwise, you must ensure the correct type is provided.

        If the index is a multi-index, start value must be a tuple.

        Cannot be used together with `query`. Include the condition into the query.
    end (any): End datetime (if datetime index) or any other end value.

        Will parse with `vectorbtpro.utils.datetime_.to_timestamp` if `align_dates` is True
        and the index is a datetime index. Otherwise, you must ensure the correct type is provided.

        If the index is a multi-index, end value must be a tuple.

        Cannot be used together with `query`. Include the condition into the query.
    align_dates (bool): Whether to align `start` and `end` to the timezone of the index.

        Will pull one row (using `LIMIT 1`) and use `SQLData.prepare_dt` to get the index.
    parse_dates (bool, list, or dict): Whether to parse dates and how to do it.

        If `query` is not used, will get mapped into column names. Otherwise,
        usage of integers is not allowed and column names directly must be used.
        If enabled, will also try to parse the datetime columns that couldn't be parsed
        by Pandas after the object has been fetched.

        For dict format, see `pd.read_sql_query`.
    to_utc (bool, str, or sequence of str): See `SQLData.prepare_dt`.
    tz (any): Timezone.

        See `vectorbtpro.utils.datetime_.to_timezone`.
    start_row (int): Start row.

        Table must contain the column defined in `row_number_column`.

        Cannot be used together with `query`. Include the condition into the query.
    end_row (int): End row.

        Table must contain the column defined in `row_number_column`.

        Cannot be used together with `query`. Include the condition into the query.
    keep_row_number (bool): Whether to return the column defined in `row_number_column`.
    row_number_column (str): Name of the column with row numbers.
    index_col (int, str, or list): One or more columns that should become the index.

        If `query` is not used, will get mapped into column names. Otherwise,
        usage of integers is not allowed and column names directly must be used.
    columns (int, str, or list): One or more columns to select.

        Will get mapped into column names. Cannot be used together with `query`.
    dtype (dtype_like or dict): Data type of each column.

        If `query` is not used, will get mapped into column names. Otherwise,
        usage of integers is not allowed and column names directly must be used.

        For dict format, see `pd.read_sql_query`.
    chunksize (int): See `pd.read_sql_query`.
    chunk_func (callable): Function to select and concatenate chunks from `Iterator`.

        Gets called only if `chunksize` is set.
    squeeze (int): Whether to squeeze a DataFrame with one column into a Series.
    **read_sql_kwargs: Other keyword arguments passed to `pd.read_sql_query`.

See https://pandas.pydata.org/docs/reference/api/pandas.read_sql_query.html for other arguments.

For defaults, see `custom.sql` in `vectorbtpro._settings.data`.
Global settings can be provided per engine name using the `engines` dictionary.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: fetch_feature
```
Fetch the table of a feature.

Uses `SQLData.fetch_key`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: fetch_symbol
```
Fetch the table for a symbol.

Uses `SQLData.fetch_key`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: update_key
```
Update data of a feature or symbol.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: update_feature
```
Update data of a feature.

Uses `SQLData.update_key`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: sql.py
#### Classe: SQLData
#### Função: update_symbol
```
Update data for a symbol.

Uses `SQLData.update_key`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: synthetic.py
#### Docstring do Módulo
```
Module with `SyntheticData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: synthetic.py
#### Classe: SyntheticData
```
Data class for fetching synthetic data.

Exposes an abstract class method `SyntheticData.generate_symbol`.
Everything else is taken care of.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: synthetic.py
#### Classe: SyntheticData
#### Função: generate_key
```
Abstract method to generate data of a feature or symbol.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: synthetic.py
#### Classe: SyntheticData
#### Função: generate_feature
```
Abstract method to generate data of a feature.

Uses `SyntheticData.generate_key` with `key_is_feature=True`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: synthetic.py
#### Classe: SyntheticData
#### Função: generate_symbol
```
Abstract method to generate data for a symbol.

Uses `SyntheticData.generate_key` with `key_is_feature=False`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: synthetic.py
#### Classe: SyntheticData
#### Função: fetch_key
```
Generate data of a feature or symbol.

Generates datetime index using `vectorbtpro.utils.datetime_.date_range` and passes it to
`SyntheticData.generate_key` to fill the Series/DataFrame with generated data.

For defaults, see `custom.synthetic` in `vectorbtpro._settings.data`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: synthetic.py
#### Classe: SyntheticData
#### Função: fetch_feature
```
Generate data of a feature.

Uses `SyntheticData.fetch_key` with `key_is_feature=True`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: synthetic.py
#### Classe: SyntheticData
#### Função: fetch_symbol
```
Generate data for a symbol.

Uses `SyntheticData.fetch_key` with `key_is_feature=False`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: synthetic.py
#### Classe: SyntheticData
#### Função: update_key
```
Update data of a feature or symbol.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: synthetic.py
#### Classe: SyntheticData
#### Função: update_feature
```
Update data of a feature.

Uses `SyntheticData.update_key` with `key_is_feature=True`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: synthetic.py
#### Classe: SyntheticData
#### Função: update_symbol
```
Update data for a symbol.

Uses `SyntheticData.update_key` with `key_is_feature=False`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Docstring do Módulo
```
Module with `TVData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
```
Client for TradingView.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVData
```
Data class for fetching from TradingView.

See `TVData.fetch_symbol` for arguments.

!!! note
    If you're getting the error "Please confirm that you are not a robot by clicking the captcha box."
    when attempting to authenticate, use `auth_token` instead of `username` and `password`.
    To get the authentication token, go to TradingView, log in, visit any chart, open your console's
    developer tools, and search for "auth_token".

Usage:
    * Set up the credentials globally (optional):

    ```pycon
    >>> from vectorbtpro import *

    >>> vbt.TVData.set_custom_settings(
    ...     client_config=dict(
    ...         username="YOUR_USERNAME",
    ...         password="YOUR_PASSWORD",
    ...         auth_token="YOUR_AUTH_TOKEN",  # optional, instead of username and password
    ...     )
    ... )
    ```

    * Pull data:

    ```pycon
    >>> data = vbt.TVData.pull(
    ...     "NASDAQ:AAPL",
    ...     timeframe="1 hour"
    ... )
    ```
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: __init__
```
Client for TradingView.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: auth_token
```
Authentication token.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: ws
```
Instance of `websocket.Websocket`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: session
```
Session.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: chart_session
```
Chart session.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: auth
```
Authenticate.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: generate_session
```
Generate session.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: generate_chart_session
```
Generate chart session.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: create_connection
```
Create a websocket connection.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: filter_raw_message
```
Filter raw message.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: prepend_header
```
Prepend a header.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: construct_message
```
Construct a message.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: create_message
```
Create a message.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: send_message
```
Send a message.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: convert_raw_data
```
Process raw data into a DataFrame.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: format_symbol
```
Format a symbol.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: get_hist
```
Get historical data.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: search_symbol
```
Search for a symbol.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVClient
#### Função: scan_symbols
```
Scan symbols in a region/market.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVData
#### Função: list_symbols
```
List all symbols.

Uses symbol search when either `text` or `exchange` is provided (returns a subset of symbols).
Otherwise, uses the market scanner (returns all symbols, big payload).

When using the market scanner, use `market` to filter by one or multiple markets. For the list
of available markets, see `MARKET_LIST`.

Use `fields` to make the market scanner return additional information that can be used for
filtering with `filter_by`. Such information is passed to the function as a dictionary where
fields are keys. The function can also be a template that can use the same information provided
as a context, or a list of values that should be matched against the values corresponding to their fields.
For the list of available fields, see `FIELD_LIST`. Argument `fields` can also be "all".
Set `return_field_data` to True to return a list with (filtered) field data.

Use `groups` to provide a single dictionary or a list of dictionaries with groups.
Each dictionary can be provided either in a compressed format, such as `dict(index=index)`,
or in a full format, such as `dict(type="index", values=[index])`.

Keyword arguments `scanner_kwargs` are encoded and passed directly to the market scanner.

Uses `vectorbtpro.data.custom.custom.CustomData.key_match` to check each exchange against
`exchange_pattern` and each symbol against `symbol_pattern`.

Usage:
    * List all symbols (market scanner):

    ```pycon
    >>> from vectorbtpro import *

    >>> vbt.TVData.list_symbols()
    ```

    * Search for symbols matching a pattern (market scanner, client-side):

    ```pycon
    >>> vbt.TVData.list_symbols(symbol_pattern="BTC*")
    ```

    * Search for exchanges matching a pattern (market scanner, client-side):

    ```pycon
    >>> vbt.TVData.list_symbols(exchange_pattern="NASDAQ")
    ```

    * Search for symbols containing a text (symbol search, server-side):

    ```pycon
    >>> vbt.TVData.list_symbols(text="BTC")
    ```

    * List symbols from an exchange (symbol search):

    ```pycon
    >>> vbt.TVData.list_symbols(exchange="NASDAQ")
    ```

    * List symbols from a market (market scanner):

    ```pycon
    >>> vbt.TVData.list_symbols(market="poland")
    ```

    * List index constituents (market scanner):

    ```pycon
    >>> vbt.TVData.list_symbols(groups=dict(index="NASDAQ:NDX"))
    ```

    * Filter symbols by fields using a function (market scanner):

    ```pycon
    >>> vbt.TVData.list_symbols(
    ...     market="america",
    ...     fields=["sector"],
    ...     filter_by=lambda context: context["sector"] == "Technology Services"
    ... )
    ```

    * Filter symbols by fields using a template (market scanner):

    ```pycon
    >>> vbt.TVData.list_symbols(
    ...     market="america",
    ...     fields=["sector"],
    ...     filter_by=vbt.RepEval("sector == 'Technology Services'")
    ... )
    ```
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVData
#### Função: resolve_client
```
Resolve the client.

If provided, must be of the type `TVClient`. Otherwise, will be created using `client_config`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: tv.py
#### Classe: TVData
#### Função: fetch_symbol
```
Override `vectorbtpro.data.base.Data.fetch_symbol` to fetch a symbol from TradingView.

Args:
    symbol (str): Symbol.

        Symbol must be in the `EXCHANGE:SYMBOL` format if `exchange` is None.
    client (TVClient): Client.

        See `TVData.resolve_client`.
    client_config (dict): Client config.

        See `TVData.resolve_client`.
    exchange (str): Exchange.

        Can be omitted if already provided via `symbol`.
    timeframe (str): Timeframe.

        Allows human-readable strings such as "15 minutes".
    tz (any): Timezone.

        See `vectorbtpro.utils.datetime_.to_timezone`.
    fut_contract (int): None for cash, 1 for continuous current contract in front,
        2 for continuous next contract in front.
    adjustment (str): Adjustment.

        Either "splits" (default) or "dividends".
    extended_session (bool): Regular session if False, extended session if True.
    pro_data (bool): Whether to use pro data.
    limit (int): The maximum number of returned items.
    delay (float): Time to sleep after each request (in seconds).
    retries (int): The number of retries on failure to fetch data.

For defaults, see `custom.tv` in `vectorbtpro._settings.data`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: yf.py
#### Docstring do Módulo
```
Module with `YFData`.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: yf.py
#### Classe: YFData
```
Data class for fetching from Yahoo Finance.

See https://github.com/ranaroussi/yfinance for API.

See `YFData.fetch_symbol` for arguments.

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> data = vbt.YFData.pull(
    ...     "BTC-USD",
    ...     start="2020-01-01",
    ...     end="2021-01-01",
    ...     timeframe="1 day"
    ... )
    ```
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: yf.py
#### Classe: YFData
#### Função: fetch_symbol
```
Override `vectorbtpro.data.base.Data.fetch_symbol` to fetch a symbol from Yahoo Finance.

Args:
    symbol (str): Symbol.
    period (str): Period.
    start (any): Start datetime.

        See `vectorbtpro.utils.datetime_.to_tzaware_datetime`.
    end (any): End datetime.

        See `vectorbtpro.utils.datetime_.to_tzaware_datetime`.
    timeframe (str): Timeframe.

        Allows human-readable strings such as "15 minutes".
    tz (any): Timezone.

        See `vectorbtpro.utils.datetime_.to_timezone`.
    **history_kwargs: Keyword arguments passed to `yfinance.base.TickerBase.history`.

For defaults, see `custom.yf` in `vectorbtpro._settings.data`.

!!! warning
    Data coming from Yahoo is not the most stable data out there. Yahoo may manipulate data
    how they want, add noise, return missing data points (see volume in the example below), etc.
    It's only used in vectorbt for demonstration purposes.
```

---

# Pasta: data
## Subpasta: custom
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules with custom data classes.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Docstring do Módulo
```
Custom Pandas accessors for generic data.

Methods can be accessed as follows:

* `GenericSRAccessor` -> `pd.Series.vbt.*`
* `GenericDFAccessor` -> `pd.DataFrame.vbt.*`

```pycon
>>> from vectorbtpro import *

>>> # vectorbtpro.generic.accessors.GenericAccessor.rolling_mean
>>> pd.Series([1, 2, 3, 4]).vbt.rolling_mean(2)
0    NaN
1    1.5
2    2.5
3    3.5
dtype: float64
```

The accessors inherit `vectorbtpro.base.accessors` and are inherited by more
specialized accessors, such as `vectorbtpro.signals.accessors` and `vectorbtpro.returns.accessors`.

!!! note
    Grouping is only supported by the methods that accept the `group_by` argument.

    Accessors do not utilize caching.

Run for the examples below:
    
```pycon
>>> df = pd.DataFrame({
...     'a': [1, 2, 3, 4, 5],
...     'b': [5, 4, 3, 2, 1],
...     'c': [1, 2, 3, 2, 1]
... }, index=pd.Index(pd.date_range("2020", periods=5)))
>>> df
            a  b  c
2020-01-01  1  5  1
2020-01-02  2  4  2
2020-01-03  3  3  3
2020-01-04  4  2  2
2020-01-05  5  1  1

>>> sr = pd.Series(np.arange(10), index=pd.date_range("2020", periods=10))
>>> sr
2020-01-01    0
2020-01-02    1
2020-01-03    2
2020-01-04    3
2020-01-05    4
2020-01-06    5
2020-01-07    6
2020-01-08    7
2020-01-09    8
2020-01-10    9
dtype: int64
```

## Stats

!!! hint
    See `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats` and `GenericAccessor.metrics`.

```pycon
>>> df2 = pd.DataFrame({
...     'a': [np.nan, 2, 3],
...     'b': [4, np.nan, 5],
...     'c': [6, 7, np.nan]
... }, index=['x', 'y', 'z'])

>>> df2.vbt(freq='d').stats(column='a')
Start                      x
End                        z
Period       3 days 00:00:00
Count                      2
Mean                     2.5
Std                 0.707107
Min                      2.0
Median                   2.5
Max                      3.0
Min Index                  y
Max Index                  z
Name: a, dtype: object
```

### Mapping

Mapping can be set both in `GenericAccessor` (preferred) and `GenericAccessor.stats`:

```pycon
>>> mapping = {x: 'test_' + str(x) for x in pd.unique(df2.values.flatten())}
>>> df2.vbt(freq='d', mapping=mapping).stats(column='a')
Start                                   x
End                                     z
Period                    3 days 00:00:00
Count                                   2
Value Counts: test_2.0                  1
Value Counts: test_3.0                  1
Value Counts: test_4.0                  0
Value Counts: test_5.0                  0
Value Counts: test_6.0                  0
Value Counts: test_7.0                  0
Value Counts: test_nan                  1
Name: a, dtype: object

>>> df2.vbt(freq='d').stats(column='a', settings=dict(mapping=mapping))
UserWarning: Changing the mapping will create a copy of this object.
Consider setting it upon object creation to re-use existing cache.

Start                                   x
End                                     z
Period                    3 days 00:00:00
Count                                   2
Value Counts: test_2.0                  1
Value Counts: test_3.0                  1
Value Counts: test_4.0                  0
Value Counts: test_5.0                  0
Value Counts: test_6.0                  0
Value Counts: test_7.0                  0
Value Counts: test_nan                  1
Name: a, dtype: object
```

Selecting a column before calling `stats` will consider uniques from this column only:

```pycon
>>> df2['a'].vbt(freq='d', mapping=mapping).stats()
Start                                   x
End                                     z
Period                    3 days 00:00:00
Count                                   2
Value Counts: test_2.0                  1
Value Counts: test_3.0                  1
Value Counts: test_nan                  1
Name: a, dtype: object
```

To include all keys from `mapping`, pass `incl_all_keys=True`:

```pycon
>>> df2['a'].vbt(freq='d', mapping=mapping).stats(settings=dict(incl_all_keys=True))
Start                                   x
End                                     z
Period                    3 days 00:00:00
Count                                   2
Value Counts: test_2.0                  1
Value Counts: test_3.0                  1
Value Counts: test_4.0                  0
Value Counts: test_5.0                  0
Value Counts: test_6.0                  0
Value Counts: test_7.0                  0
Value Counts: test_nan                  1
Name: a, dtype: object
```

`GenericAccessor.stats` also supports (re-)grouping:

```pycon
>>> df2.vbt(freq='d').stats(column=0, group_by=[0, 0, 1])
Start                      x
End                        z
Period       3 days 00:00:00
Count                      4
Mean                     3.5
Std                 1.290994
Min                      2.0
Median                   3.5
Max                      5.0
Min Index                  y
Max Index                  z
Name: 0, dtype: object
```

## Plots

!!! hint
    See `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots` and `GenericAccessor.subplots`.

`GenericAccessor` class has a single subplot based on `GenericAccessor.plot`:

```pycon
>>> df2.vbt.plots().show()
```

![](/assets/images/api/generic_plots.light.svg#only-light){: .iimg loading=lazy }
![](/assets/images/api/generic_plots.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
```
Accessor on top of data of any type. For both, Series and DataFrames.

Accessible via `pd.Series.vbt` and `pd.DataFrame.vbt`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericSRAccessor
```
Accessor on top of data of any type. For Series only.

Accessible via `pd.Series.vbt`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericDFAccessor
```
Accessor on top of data of any type. For DataFrames only.

Accessible via `pd.DataFrame.vbt`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: sr_accessor_cls
```
Accessor class for `pd.Series`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: df_accessor_cls
```
Accessor class for `pd.DataFrame`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: mapping
```
Mapping.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: resolve_mapping
```
Resolve mapping.

Set `mapping` to False to disable mapping completely.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: apply_mapping
```
See `vectorbtpro.utils.mapping.apply_mapping`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: ago
```
For each value, get the value `n` periods ago.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: any_ago
```
For each value, check whether any value within a window of `n` last periods is True.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: all_ago
```
For each value, check whether all values within a window of `n` last periods are True.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: rolling_idxmin
```
See `vectorbtpro.generic.nb.rolling.rolling_argmin_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: expanding_idxmin
```
Expanding version of `GenericAccessor.rolling_idxmin`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: rolling_idxmax
```
See `vectorbtpro.generic.nb.rolling.rolling_argmax_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: expanding_idxmax
```
Expanding version of `GenericAccessor.rolling_idxmax`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: rolling_mean
```
See `vectorbtpro.generic.nb.rolling.rolling_mean_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: expanding_mean
```
Expanding version of `GenericAccessor.rolling_mean`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: rolling_std
```
See `vectorbtpro.generic.nb.rolling.rolling_std_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: expanding_std
```
Expanding version of `GenericAccessor.rolling_std`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: rolling_zscore
```
See `vectorbtpro.generic.nb.rolling.rolling_zscore_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: expanding_zscore
```
Expanding version of `GenericAccessor.rolling_zscore`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: wm_mean
```
See `vectorbtpro.generic.nb.rolling.wm_mean_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: ewm_mean
```
See `vectorbtpro.generic.nb.rolling.ewm_mean_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: ewm_std
```
See `vectorbtpro.generic.nb.rolling.ewm_std_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: wwm_mean
```
See `vectorbtpro.generic.nb.rolling.wwm_mean_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: wwm_std
```
See `vectorbtpro.generic.nb.rolling.wwm_std_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: vidya
```
See `vectorbtpro.generic.nb.rolling.vidya_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: ma
```
See `vectorbtpro.generic.nb.rolling.ma_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: msd
```
See `vectorbtpro.generic.nb.rolling.msd_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: rolling_cov
```
See `vectorbtpro.generic.nb.rolling.rolling_cov_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: expanding_cov
```
Expanding version of `GenericAccessor.rolling_cov`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: rolling_corr
```
See `vectorbtpro.generic.nb.rolling.rolling_corr_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: expanding_corr
```
Expanding version of `GenericAccessor.rolling_corr`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: rolling_ols
```
See `vectorbtpro.generic.nb.rolling.rolling_ols_nb`.

Returns two arrays: slope and intercept.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: expanding_ols
```
Expanding version of `GenericAccessor.rolling_ols`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: rolling_rank
```
See `vectorbtpro.generic.nb.rolling.rolling_rank_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: expanding_rank
```
Expanding version of `GenericAccessor.rolling_rank`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: rolling_pattern_similarity
```
See `vectorbtpro.generic.nb.rolling.rolling_pattern_similarity_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: map
```
See `vectorbtpro.generic.nb.apply_reduce.map_nb`.

For details on the meta version, see `vectorbtpro.generic.nb.apply_reduce.map_meta_nb`.

Usage:
    * Using regular function:

    ```pycon
    >>> prod_nb = njit(lambda a, x: a * x)

    >>> df.vbt.map(prod_nb, 10)
                 a   b   c
    2020-01-01  10  50  10
    2020-01-02  20  40  20
    2020-01-03  30  30  30
    2020-01-04  40  20  20
    2020-01-05  50  10  10
    ```

    * Using meta function:

    ```pycon
    >>> diff_meta_nb = njit(lambda i, col, a, b: a[i, col] / b[i, col])

    >>> vbt.pd_acc.map(
    ...     diff_meta_nb,
    ...     df.vbt.to_2d_array() - 1,
    ...     df.vbt.to_2d_array() + 1,
    ...     wrapper=df.vbt.wrapper
    ... )
                       a         b         c
    2020-01-01  0.000000  0.666667  0.000000
    2020-01-02  0.333333  0.600000  0.333333
    2020-01-03  0.500000  0.500000  0.500000
    2020-01-04  0.600000  0.333333  0.333333
    2020-01-05  0.666667  0.000000  0.000000
    ```

    * Using templates and broadcasting:

    ```pycon
    >>> vbt.pd_acc.map(
    ...     diff_meta_nb,
    ...     vbt.Rep('a'),
    ...     vbt.Rep('b'),
    ...     broadcast_named_args=dict(
    ...         a=pd.Series([1, 2, 3, 4, 5], index=df.index),
    ...         b=pd.DataFrame([[1, 2, 3]], columns=['a', 'b', 'c'])
    ...     )
    ... )
                  a    b         c
    2020-01-01  1.0  0.5  0.333333
    2020-01-02  2.0  1.0  0.666667
    2020-01-03  3.0  1.5  1.000000
    2020-01-04  4.0  2.0  1.333333
    2020-01-05  5.0  2.5  1.666667
    ```
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: apply_along_axis
```
See `vectorbtpro.generic.nb.apply_reduce.apply_nb` for `axis=1` and
`vectorbtpro.generic.nb.apply_reduce.row_apply_nb` for `axis=0`.

For details on the meta version, see `vectorbtpro.generic.nb.apply_reduce.apply_meta_nb`
for `axis=1` and `vectorbtpro.generic.nb.apply_reduce.row_apply_meta_nb` for `axis=0`.

Usage:
    * Using regular function:

    ```pycon
    >>> power_nb = njit(lambda a: np.power(a, 2))

    >>> df.vbt.apply_along_axis(power_nb)
                 a   b  c
    2020-01-01   1  25  1
    2020-01-02   4  16  4
    2020-01-03   9   9  9
    2020-01-04  16   4  4
    2020-01-05  25   1  1
    ```

    * Using meta function:

    ```pycon
    >>> ratio_meta_nb = njit(lambda col, a, b: a[:, col] / b[:, col])

    >>> vbt.pd_acc.apply_along_axis(
    ...     ratio_meta_nb,
    ...     df.vbt.to_2d_array() - 1,
    ...     df.vbt.to_2d_array() + 1,
    ...     wrapper=df.vbt.wrapper
    ... )
                       a         b         c
    2020-01-01  0.000000  0.666667  0.000000
    2020-01-02  0.333333  0.600000  0.333333
    2020-01-03  0.500000  0.500000  0.500000
    2020-01-04  0.600000  0.333333  0.333333
    2020-01-05  0.666667  0.000000  0.000000
    ```

    * Using templates and broadcasting:

    ```pycon
    >>> vbt.pd_acc.apply_along_axis(
    ...     ratio_meta_nb,
    ...     vbt.Rep('a'),
    ...     vbt.Rep('b'),
    ...     broadcast_named_args=dict(
    ...         a=pd.Series([1, 2, 3, 4, 5], index=df.index),
    ...         b=pd.DataFrame([[1, 2, 3]], columns=['a', 'b', 'c'])
    ...     )
    ... )
                  a    b         c
    2020-01-01  1.0  0.5  0.333333
    2020-01-02  2.0  1.0  0.666667
    2020-01-03  3.0  1.5  1.000000
    2020-01-04  4.0  2.0  1.333333
    2020-01-05  5.0  2.5  1.666667
    ```
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: row_apply
```
`GenericAccessor.apply_along_axis` with `axis=0`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: column_apply
```
`GenericAccessor.apply_along_axis` with `axis=1`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: rolling_apply
```
See `vectorbtpro.generic.nb.apply_reduce.rolling_reduce_nb` for integer windows
and `vectorbtpro.generic.nb.apply_reduce.rolling_freq_reduce_nb` for frequency windows.

For details on the meta version, see `vectorbtpro.generic.nb.apply_reduce.rolling_reduce_meta_nb`
for integer windows and `vectorbtpro.generic.nb.apply_reduce.rolling_freq_reduce_meta_nb` for
frequency windows.

If `window` is None, it will become an expanding window.

Usage:
    * Using regular function:

    ```pycon
    >>> mean_nb = njit(lambda a: np.nanmean(a))

    >>> df.vbt.rolling_apply(3, mean_nb)
                  a    b         c
    2020-01-01  NaN  NaN       NaN
    2020-01-02  NaN  NaN       NaN
    2020-01-03  2.0  4.0  2.000000
    2020-01-04  3.0  3.0  2.333333
    2020-01-05  4.0  2.0  2.000000
    ```

    * Using a frequency-based window:

    ```pycon
    >>> df.vbt.rolling_apply("3d", mean_nb)
                  a    b         c
    2020-01-01  1.0  5.0  1.000000
    2020-01-02  1.5  4.5  1.500000
    2020-01-03  2.0  4.0  2.000000
    2020-01-04  3.0  3.0  2.333333
    2020-01-05  4.0  2.0  2.000000
    ```

    * Using meta function:

    ```pycon
    >>> mean_ratio_meta_nb = njit(lambda from_i, to_i, col, a, b: \
    ...     np.mean(a[from_i:to_i, col]) / np.mean(b[from_i:to_i, col]))

    >>> vbt.pd_acc.rolling_apply(
    ...     3,
    ...     mean_ratio_meta_nb,
    ...     df.vbt.to_2d_array() - 1,
    ...     df.vbt.to_2d_array() + 1,
    ...     wrapper=df.vbt.wrapper,
    ... )
                       a         b         c
    2020-01-01       NaN       NaN       NaN
    2020-01-02       NaN       NaN       NaN
    2020-01-03  0.333333  0.600000  0.333333
    2020-01-04  0.500000  0.500000  0.400000
    2020-01-05  0.600000  0.333333  0.333333
    ```

    * Using templates and broadcasting:

    ```pycon
    >>> vbt.pd_acc.rolling_apply(
    ...     2,
    ...     mean_ratio_meta_nb,
    ...     vbt.Rep('a'),
    ...     vbt.Rep('b'),
    ...     broadcast_named_args=dict(
    ...         a=pd.Series([1, 2, 3, 4, 5], index=df.index),
    ...         b=pd.DataFrame([[1, 2, 3]], columns=['a', 'b', 'c'])
    ...     )
    ... )
                  a     b         c
    2020-01-01  NaN   NaN       NaN
    2020-01-02  1.5  0.75  0.500000
    2020-01-03  2.5  1.25  0.833333
    2020-01-04  3.5  1.75  1.166667
    2020-01-05  4.5  2.25  1.500000
    ```
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: expanding_apply
```
`GenericAccessor.rolling_apply` but expanding.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: groupby_apply
```
See `vectorbtpro.generic.nb.apply_reduce.groupby_reduce_nb`.

For details on the meta version, see `vectorbtpro.generic.nb.apply_reduce.groupby_reduce_meta_nb`.

Argument `by` can be an instance of `vectorbtpro.base.grouping.base.Grouper`,
`pandas.core.groupby.GroupBy`, `pandas.core.resample.Resampler`, or any other groupby-like
object that can be accepted by `vectorbtpro.base.grouping.base.Grouper`, or if it fails,
then by `pd.DataFrame.groupby` with `groupby_kwargs` passed as keyword arguments.

Usage:
    * Using regular function:

    ```pycon
    >>> mean_nb = njit(lambda a: np.nanmean(a))

    >>> df.vbt.groupby_apply([1, 1, 2, 2, 3], mean_nb)
         a    b    c
    1  1.5  4.5  1.5
    2  3.5  2.5  2.5
    3  5.0  1.0  1.0
    ```

    * Using meta function:

    ```pycon
    >>> mean_ratio_meta_nb = njit(lambda idxs, group, col, a, b: \
    ...     np.mean(a[idxs, col]) / np.mean(b[idxs, col]))

    >>> vbt.pd_acc.groupby_apply(
    ...     [1, 1, 2, 2, 3],
    ...     mean_ratio_meta_nb,
    ...     df.vbt.to_2d_array() - 1,
    ...     df.vbt.to_2d_array() + 1,
    ...     wrapper=df.vbt.wrapper
    ... )
              a         b         c
    1  0.200000  0.636364  0.200000
    2  0.555556  0.428571  0.428571
    3  0.666667  0.000000  0.000000
    ```

    * Using templates and broadcasting, let's split both input arrays into 2 groups of rows and
    run the calculation function on each group:

    ```pycon
    >>> from vectorbtpro.base.grouping.nb import group_by_evenly_nb

    >>> vbt.pd_acc.groupby_apply(
    ...     vbt.RepEval('group_by_evenly_nb(wrapper.shape[0], 2)'),
    ...     mean_ratio_meta_nb,
    ...     vbt.Rep('a'),
    ...     vbt.Rep('b'),
    ...     broadcast_named_args=dict(
    ...         a=pd.Series([1, 2, 3, 4, 5], index=df.index),
    ...         b=pd.DataFrame([[1, 2, 3]], columns=['a', 'b', 'c'])
    ...     ),
    ...     template_context=dict(group_by_evenly_nb=group_by_evenly_nb)
    ... )
         a     b         c
    0  2.0  1.00  0.666667
    1  4.5  2.25  1.500000
    ```

    The advantage of the approach above is in the flexibility: we can pass two arrays of
    any broadcastable shapes and everything else is done for us.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: groupby_transform
```
See `vectorbtpro.generic.nb.apply_reduce.groupby_transform_nb`.

For details on the meta version, see `vectorbtpro.generic.nb.apply_reduce.groupby_transform_meta_nb`.

For argument `by`, see `GenericAccessor.groupby_apply`.

Usage:
    * Using regular function:

    ```pycon
    >>> zscore_nb = njit(lambda a: (a - np.nanmean(a)) / np.nanstd(a))

    >>> df.vbt.groupby_transform([1, 1, 2, 2, 3], zscore_nb)
                       a         b         c
    2020-01-01 -1.000000  1.666667 -1.000000
    2020-01-02 -0.333333  1.000000 -0.333333
    2020-01-03  0.242536  0.242536  0.242536
    2020-01-04  1.697749 -1.212678 -1.212678
    2020-01-05  1.414214 -0.707107 -0.707107
    ```

    * Using meta function:

    ```pycon
    >>> zscore_ratio_meta_nb = njit(lambda idxs, group, a, b: \
    ...     zscore_nb(a[idxs]) / zscore_nb(b[idxs]))

    >>> vbt.pd_acc.groupby_transform(
    ...     [1, 1, 2, 2, 3],
    ...     zscore_ratio_meta_nb,
    ...     df.vbt.to_2d_array(),
    ...     df.vbt.to_2d_array()[::-1],
    ...     wrapper=df.vbt.wrapper
    ... )
                       a         b    c
    2020-01-01 -0.600000 -1.666667  1.0
    2020-01-02 -0.333333 -3.000000  1.0
    2020-01-03  1.000000  1.000000  1.0
    2020-01-04 -1.400000 -0.714286  1.0
    2020-01-05 -2.000000 -0.500000  1.0
    ```
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: resample_apply
```
Resample.

Argument `rule` can be an instance of `vectorbtpro.base.resampling.base.Resampler`,
`pandas.core.resample.Resampler`, or any other frequency-like object that can be accepted by
`pd.DataFrame.resample` with `resample_kwargs` passed as keyword arguments.
If `use_groupby_apply` is True, uses `GenericAccessor.groupby_apply` (with some post-processing).
Otherwise, uses `GenericAccessor.resample_to_index`.

Usage:
    * Using regular function:

    ```pycon
    >>> mean_nb = njit(lambda a: np.nanmean(a))

    >>> df.vbt.resample_apply('2d', mean_nb)
                  a    b    c
    2020-01-01  1.5  4.5  1.5
    2020-01-03  3.5  2.5  2.5
    2020-01-05  5.0  1.0  1.0
    ```

    * Using meta function:

    ```pycon
    >>> mean_ratio_meta_nb = njit(lambda idxs, group, col, a, b: \
    ...     np.mean(a[idxs, col]) / np.mean(b[idxs, col]))

    >>> vbt.pd_acc.resample_apply(
    ...     '2d',
    ...     mean_ratio_meta_nb,
    ...     df.vbt.to_2d_array() - 1,
    ...     df.vbt.to_2d_array() + 1,
    ...     wrapper=df.vbt.wrapper
    ... )
                       a         b         c
    2020-01-01  0.200000  0.636364  0.200000
    2020-01-03  0.555556  0.428571  0.428571
    2020-01-05  0.666667  0.000000  0.000000
    ```

    * Using templates and broadcasting:

    ```pycon
    >>> vbt.pd_acc.resample_apply(
    ...     '2d',
    ...     mean_ratio_meta_nb,
    ...     vbt.Rep('a'),
    ...     vbt.Rep('b'),
    ...     broadcast_named_args=dict(
    ...         a=pd.Series([1, 2, 3, 4, 5], index=df.index),
    ...         b=pd.DataFrame([[1, 2, 3]], columns=['a', 'b', 'c'])
    ...     )
    ... )
                  a     b         c
    2020-01-01  1.5  0.75  0.500000
    2020-01-03  3.5  1.75  1.166667
    2020-01-05  5.0  2.50  1.666667
    ```
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: apply_and_reduce
```
See `vectorbtpro.generic.nb.apply_reduce.apply_and_reduce_nb`.

For details on the meta version, see `vectorbtpro.generic.nb.apply_reduce.apply_and_reduce_meta_nb`.

Usage:
    * Using regular function:

    ```pycon
    >>> greater_nb = njit(lambda a: a[a > 2])
    >>> mean_nb = njit(lambda a: np.nanmean(a))

    >>> df.vbt.apply_and_reduce(greater_nb, mean_nb)
    a    4.0
    b    4.0
    c    3.0
    Name: apply_and_reduce, dtype: float64
    ```

    * Using meta function:

    ```pycon
    >>> and_meta_nb = njit(lambda col, a, b: a[:, col] & b[:, col])
    >>> sum_meta_nb = njit(lambda col, x: np.sum(x))

    >>> vbt.pd_acc.apply_and_reduce(
    ...     and_meta_nb,
    ...     sum_meta_nb,
    ...     apply_args=(
    ...         df.vbt.to_2d_array() > 1,
    ...         df.vbt.to_2d_array() < 4
    ...     ),
    ...     wrapper=df.vbt.wrapper
    ... )
    a    2
    b    2
    c    3
    Name: apply_and_reduce, dtype: int64
    ```

    * Using templates and broadcasting:

    ```pycon
    >>> vbt.pd_acc.apply_and_reduce(
    ...     and_meta_nb,
    ...     sum_meta_nb,
    ...     apply_args=(
    ...         vbt.Rep('mask_a'),
    ...         vbt.Rep('mask_b')
    ...     ),
    ...     broadcast_named_args=dict(
    ...         mask_a=pd.Series([True, True, True, False, False], index=df.index),
    ...         mask_b=pd.DataFrame([[True, True, False]], columns=['a', 'b', 'c'])
    ...     )
    ... )
    a    3
    b    3
    c    0
    Name: apply_and_reduce, dtype: int64
    ```
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: reduce
```
Reduce by column/group.

Set `flatten` to True when working with grouped data to pass a flattened array to `reduce_func_nb`.
The order in which to flatten the array can be specified using `order`.

Set `returns_array` to True if `reduce_func_nb` returns an array.

Set `returns_idx` to True if `reduce_func_nb` returns row index/position.

Set `to_index` to True to return labels instead of positions.

For implementation details, see

* `vectorbtpro.generic.nb.apply_reduce.reduce_flat_grouped_to_array_nb` if grouped, `returns_array` is True, and `flatten` is True
* `vectorbtpro.generic.nb.apply_reduce.reduce_flat_grouped_nb` if grouped, `returns_array` is False, and `flatten` is True
* `vectorbtpro.generic.nb.apply_reduce.reduce_grouped_to_array_nb` if grouped, `returns_array` is True, and `flatten` is False
* `vectorbtpro.generic.nb.apply_reduce.reduce_grouped_nb` if grouped, `returns_array` is False, and `flatten` is False
* `vectorbtpro.generic.nb.apply_reduce.reduce_to_array_nb` if not grouped and `returns_array` is True
* `vectorbtpro.generic.nb.apply_reduce.reduce_nb` if not grouped and `returns_array` is False

For implementation details on the meta versions, see

* `vectorbtpro.generic.nb.apply_reduce.reduce_grouped_to_array_meta_nb` if grouped and `returns_array` is True
* `vectorbtpro.generic.nb.apply_reduce.reduce_grouped_meta_nb` if grouped and `returns_array` is False
* `vectorbtpro.generic.nb.apply_reduce.reduce_to_array_meta_nb` if not grouped and `returns_array` is True
* `vectorbtpro.generic.nb.apply_reduce.reduce_meta_nb` if not grouped and `returns_array` is False

`reduce_func_nb` can be a string denoting the suffix of a reducing function
from `vectorbtpro.generic.nb`. For example, "sum" will refer to "sum_reduce_nb".

Usage:
    * Using regular function:

    ```pycon
    >>> mean_nb = njit(lambda a: np.nanmean(a))

    >>> df.vbt.reduce(mean_nb)
    a    3.0
    b    3.0
    c    1.8
    Name: reduce, dtype: float64

    >>> argmax_nb = njit(lambda a: np.argmax(a))

    >>> df.vbt.reduce(argmax_nb, returns_idx=True)
    a   2020-01-05
    b   2020-01-01
    c   2020-01-03
    Name: reduce, dtype: datetime64[ns]

    >>> df.vbt.reduce(argmax_nb, returns_idx=True, to_index=False)
    a    4
    b    0
    c    2
    Name: reduce, dtype: int64

    >>> min_max_nb = njit(lambda a: np.array([np.nanmin(a), np.nanmax(a)]))

    >>> df.vbt.reduce(min_max_nb, returns_array=True, wrap_kwargs=dict(name_or_index=['min', 'max']))
         a  b  c
    min  1  1  1
    max  5  5  3

    >>> group_by = pd.Series(['first', 'first', 'second'], name='group')
    >>> df.vbt.reduce(mean_nb, group_by=group_by)
    group
    first     3.0
    second    1.8
    dtype: float64
    ```

    * Using meta function:

    ```pycon
    >>> mean_meta_nb = njit(lambda col, a: np.nanmean(a[:, col]))

    >>> pd.Series.vbt.reduce(
    ...     mean_meta_nb,
    ...     df['a'].vbt.to_2d_array(),
    ...     wrapper=df['a'].vbt.wrapper
    ... )
    3.0

    >>> vbt.pd_acc.reduce(
    ...     mean_meta_nb,
    ...     df.vbt.to_2d_array(),
    ...     wrapper=df.vbt.wrapper
    ... )
    a    3.0
    b    3.0
    c    1.8
    Name: reduce, dtype: float64

    >>> grouped_mean_meta_nb = njit(lambda group_idxs, group, a: np.nanmean(a[:, group_idxs]))

    >>> group_by = pd.Series(['first', 'first', 'second'], name='group')
    >>> vbt.pd_acc.reduce(
    ...     grouped_mean_meta_nb,
    ...     df.vbt.to_2d_array(),
    ...     wrapper=df.vbt.wrapper,
    ...     group_by=group_by
    ... )
    group
    first     3.0
    second    1.8
    Name: reduce, dtype: float64
    ```

    * Using templates and broadcasting:

    ```pycon
    >>> mean_a_b_nb = njit(lambda col, a, b: \
    ...     np.array([np.nanmean(a[:, col]), np.nanmean(b[:, col])]))

    >>> vbt.pd_acc.reduce(
    ...     mean_a_b_nb,
    ...     vbt.Rep('arr1'),
    ...     vbt.Rep('arr2'),
    ...     returns_array=True,
    ...     broadcast_named_args=dict(
    ...         arr1=pd.Series([1, 2, 3, 4, 5], index=df.index),
    ...         arr2=pd.DataFrame([[1, 2, 3]], columns=['a', 'b', 'c'])
    ...     ),
    ...     wrap_kwargs=dict(name_or_index=['arr1', 'arr2'])
    ... )
            a    b    c
    arr1  3.0  3.0  3.0
    arr2  1.0  2.0  3.0
    ```
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: proximity_apply
```
See `vectorbtpro.generic.nb.apply_reduce.proximity_reduce_nb`.

For details on the meta version, see `vectorbtpro.generic.nb.apply_reduce.proximity_reduce_meta_nb`.

Usage:
    * Using regular function:

    ```pycon
    >>> mean_nb = njit(lambda a: np.nanmean(a))

    >>> df.vbt.proximity_apply(1, mean_nb)
                  a         b         c
    2020-01-01  3.0  2.500000  3.000000
    2020-01-02  3.0  2.666667  3.000000
    2020-01-03  3.0  2.777778  2.666667
    2020-01-04  3.0  2.666667  2.000000
    2020-01-05  3.0  2.500000  1.500000
    ```

    * Using meta function:

    ```pycon
    >>> @njit
    ... def mean_ratio_meta_nb(from_i, to_i, from_col, to_col, a, b):
    ...     a_mean = np.mean(a[from_i:to_i, from_col:to_col])
    ...     b_mean = np.mean(b[from_i:to_i, from_col:to_col])
    ...     return a_mean / b_mean

    >>> vbt.pd_acc.proximity_apply(
    ...     1,
    ...     mean_ratio_meta_nb,
    ...     df.vbt.to_2d_array() - 1,
    ...     df.vbt.to_2d_array() + 1,
    ...     wrapper=df.vbt.wrapper,
    ... )
                  a         b         c
    2020-01-01  0.5  0.428571  0.500000
    2020-01-02  0.5  0.454545  0.500000
    2020-01-03  0.5  0.470588  0.454545
    2020-01-04  0.5  0.454545  0.333333
    2020-01-05  0.5  0.428571  0.200000
    ```

    * Using templates and broadcasting:

    ```pycon
    >>> vbt.pd_acc.proximity_apply(
    ...     1,
    ...     mean_ratio_meta_nb,
    ...     vbt.Rep('a'),
    ...     vbt.Rep('b'),
    ...     broadcast_named_args=dict(
    ...         a=pd.Series([1, 2, 3, 4, 5], index=df.index),
    ...         b=pd.DataFrame([[1, 2, 3]], columns=['a', 'b', 'c'])
    ...     )
    ... )
                       a     b    c
    2020-01-01  1.000000  0.75  0.6
    2020-01-02  1.333333  1.00  0.8
    2020-01-03  2.000000  1.50  1.2
    2020-01-04  2.666667  2.00  1.6
    2020-01-05  3.000000  2.25  1.8
    ```
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: squeeze_grouped
```
Squeeze each group of columns into a single column.

See `vectorbtpro.generic.nb.apply_reduce.squeeze_grouped_nb`.
For details on the meta version, see `vectorbtpro.generic.nb.apply_reduce.squeeze_grouped_meta_nb`.

Usage:
    * Using regular function:

    ```pycon
    >>> mean_nb = njit(lambda a: np.nanmean(a))

    >>> group_by = pd.Series(['first', 'first', 'second'], name='group')
    >>> df.vbt.squeeze_grouped(mean_nb, group_by=group_by)
    group       first  second
    2020-01-01    3.0     1.0
    2020-01-02    3.0     2.0
    2020-01-03    3.0     3.0
    2020-01-04    3.0     2.0
    2020-01-05    3.0     1.0
    ```

    * Using meta function:

    ```pycon
    >>> mean_ratio_meta_nb = njit(lambda i, group_idxs, group, a, b: \
    ...     np.mean(a[i][group_idxs]) / np.mean(b[i][group_idxs]))

    >>> vbt.pd_acc.squeeze_grouped(
    ...     mean_ratio_meta_nb,
    ...     df.vbt.to_2d_array() - 1,
    ...     df.vbt.to_2d_array() + 1,
    ...     wrapper=df.vbt.wrapper,
    ...     group_by=group_by
    ... )
    group       first    second
    2020-01-01    0.5  0.000000
    2020-01-02    0.5  0.333333
    2020-01-03    0.5  0.500000
    2020-01-04    0.5  0.333333
    2020-01-05    0.5  0.000000
    ```

    * Using templates and broadcasting:

    ```pycon
    >>> vbt.pd_acc.squeeze_grouped(
    ...     mean_ratio_meta_nb,
    ...     vbt.Rep('a'),
    ...     vbt.Rep('b'),
    ...     broadcast_named_args=dict(
    ...         a=pd.Series([1, 2, 3, 4, 5], index=df.index),
    ...         b=pd.DataFrame([[1, 2, 3]], columns=['a', 'b', 'c'])
    ...     ),
    ...     group_by=[0, 0, 1]
    ... )
                       0         1
    2020-01-01  0.666667  0.333333
    2020-01-02  1.333333  0.666667
    2020-01-03  2.000000  1.000000
    2020-01-04  2.666667  1.333333
    2020-01-05  3.333333  1.666667
    ```
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: flatten_grouped
```
Flatten each group of columns.

See `vectorbtpro.generic.nb.apply_reduce.flatten_grouped_nb`.
If all groups have the same length, see `vectorbtpro.generic.nb.apply_reduce.flatten_uniform_grouped_nb`.

!!! warning
    Make sure that the distribution of group lengths is close to uniform, otherwise
    groups with less columns will be filled with NaN and needlessly occupy memory.

Usage:
    ```pycon
    >>> group_by = pd.Series(['first', 'first', 'second'], name='group')
    >>> df.vbt.flatten_grouped(group_by=group_by, order='C')
    group       first  second
    2020-01-01    1.0     1.0
    2020-01-01    5.0     NaN
    2020-01-02    2.0     2.0
    2020-01-02    4.0     NaN
    2020-01-03    3.0     3.0
    2020-01-03    3.0     NaN
    2020-01-04    4.0     2.0
    2020-01-04    2.0     NaN
    2020-01-05    5.0     1.0
    2020-01-05    1.0     NaN

    >>> df.vbt.flatten_grouped(group_by=group_by, order='F')
    group       first  second
    2020-01-01    1.0     1.0
    2020-01-02    2.0     2.0
    2020-01-03    3.0     3.0
    2020-01-04    4.0     2.0
    2020-01-05    5.0     1.0
    2020-01-01    5.0     NaN
    2020-01-02    4.0     NaN
    2020-01-03    3.0     NaN
    2020-01-04    2.0     NaN
    2020-01-05    1.0     NaN
    ```
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: realign
```
See `vectorbtpro.generic.nb.base.realign_nb`.

`index` can be either an instance of `vectorbtpro.base.resampling.base.Resampler`,
or any index-like object.

Gives the same results as `df.resample(closed='right', label='right').last().ffill()`
when applied on the target index of the resampler.

Usage:
    * Downsampling:

    ```pycon
    >>> h_index = pd.date_range('2020-01-01', '2020-01-05', freq='1h')
    >>> d_index = pd.date_range('2020-01-01', '2020-01-05', freq='1d')

    >>> h_sr = pd.Series(range(len(h_index)), index=h_index)
    >>> h_sr.vbt.realign(d_index)
    2020-01-01     0.0
    2020-01-02    24.0
    2020-01-03    48.0
    2020-01-04    72.0
    2020-01-05    96.0
    Freq: D, dtype: float64
    ```

    * Upsampling:

    ```pycon
    >>> d_sr = pd.Series(range(len(d_index)), index=d_index)
    >>> d_sr.vbt.realign(h_index)
    2020-01-01 00:00:00    0.0
    2020-01-01 01:00:00    0.0
    2020-01-01 02:00:00    0.0
    2020-01-01 03:00:00    0.0
    2020-01-01 04:00:00    0.0
    ...                    ...
    2020-01-04 20:00:00    3.0
    2020-01-04 21:00:00    3.0
    2020-01-04 22:00:00    3.0
    2020-01-04 23:00:00    3.0
    2020-01-05 00:00:00    4.0
    Freq: H, Length: 97, dtype: float64
    ```
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: realign_opening
```
`GenericAccessor.realign` but creating a resampler and using the left bound
of the source and target index.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: realign_closing
```
`GenericAccessor.realign` but creating a resampler and using the right bound
of the source and target index.

!!! note
    The timestamps in the source and target index should denote the open time.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: resample_to_index
```
Resample solely based on target index.

Applies `vectorbtpro.generic.nb.apply_reduce.reduce_index_ranges_nb` on index ranges
from `vectorbtpro.base.resampling.nb.map_index_to_source_ranges_nb`.

For details on the meta version, see `vectorbtpro.generic.nb.apply_reduce.reduce_index_ranges_meta_nb`.

Usage:
    * Downsampling:

    ```pycon
    >>> h_index = pd.date_range('2020-01-01', '2020-01-05', freq='1h')
    >>> d_index = pd.date_range('2020-01-01', '2020-01-05', freq='1d')

    >>> h_sr = pd.Series(range(len(h_index)), index=h_index)
    >>> h_sr.vbt.resample_to_index(d_index, njit(lambda x: x.mean()))
    2020-01-01    11.5
    2020-01-02    35.5
    2020-01-03    59.5
    2020-01-04    83.5
    2020-01-05    96.0
    Freq: D, dtype: float64

    >>> h_sr.vbt.resample_to_index(d_index, njit(lambda x: x.mean()), before=True)
    2020-01-01     0.0
    2020-01-02    12.5
    2020-01-03    36.5
    2020-01-04    60.5
    2020-01-05    84.5
    Freq: D, dtype: float64
    ```

    * Upsampling:

    ```pycon
    >>> d_sr = pd.Series(range(len(d_index)), index=d_index)
    >>> d_sr.vbt.resample_to_index(h_index, njit(lambda x: x[-1]))
    2020-01-01 00:00:00    0.0
    2020-01-01 01:00:00    NaN
    2020-01-01 02:00:00    NaN
    2020-01-01 03:00:00    NaN
    2020-01-01 04:00:00    NaN
    ...                    ...
    2020-01-04 20:00:00    NaN
    2020-01-04 21:00:00    NaN
    2020-01-04 22:00:00    NaN
    2020-01-04 23:00:00    NaN
    2020-01-05 00:00:00    4.0
    Freq: H, Length: 97, dtype: float64
    ```

    * Using meta function:

    ```pycon
    >>> mean_ratio_meta_nb = njit(lambda from_i, to_i, col, a, b: \
    ...     np.mean(a[from_i:to_i][col]) / np.mean(b[from_i:to_i][col]))

    >>> vbt.pd_acc.resample_to_index(
    ...     d_index,
    ...     mean_ratio_meta_nb,
    ...     h_sr.vbt.to_2d_array() - 1,
    ...     h_sr.vbt.to_2d_array() + 1,
    ...     wrapper=h_sr.vbt.wrapper
    ... )
    2020-01-01   -1.000000
    2020-01-02    0.920000
    2020-01-03    0.959184
    2020-01-04    0.972603
    2020-01-05    0.979381
    Freq: D, dtype: float64
    ```

    * Using templates and broadcasting:

    ```pycon
    >>> vbt.pd_acc.resample_to_index(
    ...     d_index,
    ...     mean_ratio_meta_nb,
    ...     vbt.Rep('a'),
    ...     vbt.Rep('b'),
    ...     broadcast_named_args=dict(
    ...         a=h_sr - 1,
    ...         b=h_sr + 1
    ...     )
    ... )
    2020-01-01   -1.000000
    2020-01-02    0.920000
    2020-01-03    0.959184
    2020-01-04    0.972603
    2020-01-05    0.979381
    Freq: D, dtype: float64
    ```
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: resample_between_bounds
```
Resample between target index bounds.

Applies `vectorbtpro.generic.nb.apply_reduce.reduce_index_ranges_nb` on index ranges
from `vectorbtpro.base.resampling.nb.map_bounds_to_source_ranges_nb`.

For details on the meta version, see `vectorbtpro.generic.nb.apply_reduce.reduce_index_ranges_meta_nb`.

Usage:
    * Using regular function:

    ```pycon
    >>> h_index = pd.date_range('2020-01-01', '2020-01-05', freq='1h')
    >>> d_index = pd.date_range('2020-01-01', '2020-01-05', freq='1d')

    >>> h_sr = pd.Series(range(len(h_index)), index=h_index)
    >>> h_sr.vbt.resample_between_bounds(d_index, d_index.shift(), njit(lambda x: x.mean()))
    2020-01-01    11.5
    2020-01-02    35.5
    2020-01-03    59.5
    2020-01-04    83.5
    2020-01-05    96.0
    Freq: D, dtype: float64
    ```

    * Using meta function:

    ```pycon
    >>> mean_ratio_meta_nb = njit(lambda from_i, to_i, col, a, b: \
    ...     np.mean(a[from_i:to_i][col]) / np.mean(b[from_i:to_i][col]))

    >>> vbt.pd_acc.resample_between_bounds(
    ...     d_index,
    ...     d_index.shift(),
    ...     mean_ratio_meta_nb,
    ...     h_sr.vbt.to_2d_array() - 1,
    ...     h_sr.vbt.to_2d_array() + 1,
    ...     wrapper=h_sr.vbt.wrapper
    ... )
    2020-01-01   -1.000000
    2020-01-02    0.920000
    2020-01-03    0.959184
    2020-01-04    0.972603
    2020-01-05    0.979381
    Freq: D, dtype: float64
    ```

    * Using templates and broadcasting:

    ```pycon
    >>> vbt.pd_acc.resample_between_bounds(
    ...     d_index,
    ...     d_index.shift(),
    ...     mean_ratio_meta_nb,
    ...     vbt.Rep('a'),
    ...     vbt.Rep('b'),
    ...     broadcast_named_args=dict(
    ...         a=h_sr - 1,
    ...         b=h_sr + 1
    ...     )
    ... )
    2020-01-01   -1.000000
    2020-01-02    0.920000
    2020-01-03    0.959184
    2020-01-04    0.972603
    2020-01-05    0.979381
    Freq: D, dtype: float64
    ```
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: min
```
Return min of non-NaN elements.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: max
```
Return max of non-NaN elements.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: mean
```
Return mean of non-NaN elements.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: median
```
Return median of non-NaN elements.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: std
```
Return standard deviation of non-NaN elements.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: sum
```
Return sum of non-NaN elements.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: count
```
Return count of non-NaN elements.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: cov
```
Return covariance of non-NaN elements.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: corr
```
Return correlation coefficient of non-NaN elements.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: rank
```
Compute numerical data rank.

By default, equal values are assigned a rank that is the average of the ranks of those values.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: idxmin
```
Return labeled index of min of non-NaN elements.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: idxmax
```
Return labeled index of max of non-NaN elements.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: describe
```
See `vectorbtpro.generic.nb.apply_reduce.describe_reduce_nb`.

For `percentiles`, see `pd.DataFrame.describe`.

Usage:
    ```pycon
    >>> df.vbt.describe()
                  a         b        c
    count  5.000000  5.000000  5.00000
    mean   3.000000  3.000000  1.80000
    std    1.581139  1.581139  0.83666
    min    1.000000  1.000000  1.00000
    25%    2.000000  2.000000  1.00000
    50%    3.000000  3.000000  2.00000
    75%    4.000000  4.000000  2.00000
    max    5.000000  5.000000  3.00000
    ```
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: digitize
```
Apply `np.digitize`.

Usage:
    ```pycon
    >>> df.vbt.digitize(3)
                a  b  c
    2020-01-01  1  3  1
    2020-01-02  1  3  1
    2020-01-03  2  2  2
    2020-01-04  3  1  1
    2020-01-05  3  1  1
    ```
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: value_counts
```
Return a Series/DataFrame containing counts of unique values.

Args:
    axis (int): 0 - counts per row, 1 - counts per column, and -1 - counts across the whole object.
    normalize (bool): Whether to return the relative frequencies of the unique values.
    sort_uniques (bool): Whether to sort uniques.
    sort (bool): Whether to sort by frequency.
    ascending (bool): Whether to sort in ascending order.
    dropna (bool): Whether to exclude counts of NaN.
    group_by (any): Group or ungroup columns.

        See `vectorbtpro.base.grouping.base.Grouper`.
    mapping (mapping_like): Mapping of values to labels.
    incl_all_keys (bool): Whether to include all mapping keys, no only those that are present in the array.
    jitted (any): Whether to JIT-compile `vectorbtpro.generic.nb.base.value_counts_nb` or options.
    chunked (any): Whether to chunk `vectorbtpro.generic.nb.base.value_counts_nb` or options.

        See `vectorbtpro.utils.chunking.resolve_chunked`.
    wrap_kwargs (dict): Keyword arguments passed to `vectorbtpro.base.wrapping.ArrayWrapper.wrap`.
    **kwargs: Keyword arguments passed to `vectorbtpro.utils.mapping.apply_mapping`.

Usage:
    ```pycon
    >>> df.vbt.value_counts()
       a  b  c
    1  1  1  2
    2  1  1  2
    3  1  1  1
    4  1  1  0
    5  1  1  0

    >>> df.vbt.value_counts(axis=-1)
    1    4
    2    4
    3    3
    4    2
    5    2
    Name: value_counts, dtype: int64

    >>> mapping = {x: 'test_' + str(x) for x in pd.unique(df.values.flatten())}
    >>> df.vbt.value_counts(mapping=mapping)
            a  b  c
    test_1  1  1  2
    test_2  1  1  2
    test_3  1  1  1
    test_4  1  1  0
    test_5  1  1  0

    >>> sr = pd.Series([1, 2, 2, 3, 3, 3, np.nan])
    >>> sr.vbt.value_counts(mapping=mapping)
    test_1    1
    test_2    2
    test_3    3
    NaN       1
    dtype: int64

    >>> sr.vbt.value_counts(mapping=mapping, dropna=True)
    test_1    1
    test_2    2
    test_3    3
    dtype: int64

    >>> sr.vbt.value_counts(mapping=mapping, sort=True)
    test_3    3
    test_2    2
    test_1    1
    NaN       1
    dtype: int64

    >>> sr.vbt.value_counts(mapping=mapping, sort=True, ascending=True)
    test_1    1
    NaN       1
    test_2    2
    test_3    3
    dtype: int64

    >>> sr.vbt.value_counts(mapping=mapping, incl_all_keys=True)
    test_1    1
    test_2    2
    test_3    3
    test_4    0
    test_5    0
    NaN       1
    dtype: int64
    ```
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: demean
```
See `vectorbtpro.generic.nb.base.demean_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: transform
```
Transform using a transformer.

A transformer can be any class instance that has `transform` and `fit_transform` methods,
ideally subclassing `sklearn.base.TransformerMixin` and `sklearn.base.BaseEstimator`.

Will fit `transformer` if not fitted.

`**kwargs` are passed to the `transform` or `fit_transform` method.

Usage:
    ```pycon
    >>> from sklearn.preprocessing import MinMaxScaler

    >>> df.vbt.transform(MinMaxScaler((-1, 1)))
                  a    b    c
    2020-01-01 -1.0  1.0 -1.0
    2020-01-02 -0.5  0.5  0.0
    2020-01-03  0.0  0.0  1.0
    2020-01-04  0.5 -0.5  0.0
    2020-01-05  1.0 -1.0 -1.0

    >>> fitted_scaler = MinMaxScaler((-1, 1)).fit(np.array([[2], [4]]))
    >>> df.vbt.transform(fitted_scaler)
                  a    b    c
    2020-01-01 -2.0  2.0 -2.0
    2020-01-02 -1.0  1.0 -1.0
    2020-01-03  0.0  0.0  0.0
    2020-01-04  1.0 -1.0 -1.0
    2020-01-05  2.0 -2.0 -2.0
    ```
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: zscore
```
Compute z-score using `sklearn.preprocessing.StandardScaler`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: rebase
```
Rebase all series to the given base.

This makes comparing/plotting different series together easier.
Will forward and backward fill NaN values.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: drawdown
```
Get drawdown series.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: ranges
```
`GenericAccessor.get_ranges` with default arguments.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: get_ranges
```
Generate range records.

See `vectorbtpro.generic.ranges.Ranges.from_array`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: drawdowns
```
`GenericAccessor.get_drawdowns` with default arguments.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: get_drawdowns
```
Generate drawdown records.

See `vectorbtpro.generic.drawdowns.Drawdowns.from_price`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: to_mapped
```
Convert this object into an instance of `vectorbtpro.records.mapped_array.MappedArray`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: to_returns
```
Get returns of this object.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: to_log_returns
```
Get log returns of this object.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: to_daily_returns
```
Get daily returns of this object.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: to_daily_log_returns
```
Get daily log returns of this object.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: find_pattern
```
Generate pattern range records.

See `vectorbtpro.generic.ranges.PatternRanges.from_pattern_search`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: crossed_above
```
See `vectorbtpro.generic.nb.base.crossed_above_nb`.

Usage:
    ```pycon
    >>> df['b'].vbt.crossed_above(df['c'])
    2020-01-01    False
    2020-01-02    False
    2020-01-03    False
    2020-01-04    False
    2020-01-05    False
    dtype: bool

    >>> df['a'].vbt.crossed_above(df['b'])
    2020-01-01    False
    2020-01-02    False
    2020-01-03    False
    2020-01-04     True
    2020-01-05    False
    dtype: bool

    >>> df['a'].vbt.crossed_above(df['b'], wait=1)
    2020-01-01    False
    2020-01-02    False
    2020-01-03    False
    2020-01-04    False
    2020-01-05     True
    dtype: bool
    ```
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: crossed_below
```
See `vectorbtpro.generic.nb.base.crossed_below_nb`.

Also, see `GenericAccessor.crossed_above` for similar examples.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: resolve_self
```
Resolve self.

See `vectorbtpro.base.wrapping.Wrapping.resolve_self`.

Creates a copy of this instance `mapping` is different in `cond_kwargs`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: stats_defaults
```
Defaults for `GenericAccessor.stats`.

Merges `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats_defaults` and
`stats` from `vectorbtpro._settings.generic`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: plot
```
Create `vectorbtpro.generic.plotting.Scatter` and return the figure.

Usage:
    ```pycon
    >>> df.vbt.plot().show()
    ```

    ![](/assets/images/api/df_plot.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/df_plot.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: lineplot
```
`GenericAccessor.plot` with 'lines' mode.

Usage:
    ```pycon
    >>> df.vbt.lineplot().show()
    ```

    ![](/assets/images/api/df_lineplot.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/df_lineplot.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: scatterplot
```
`GenericAccessor.plot` with 'markers' mode.

Usage:
    ```pycon
    >>> df.vbt.scatterplot().show()
    ```

    ![](/assets/images/api/df_scatterplot.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/df_scatterplot.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: barplot
```
Create `vectorbtpro.generic.plotting.Bar` and return the figure.

Usage:
    ```pycon
    >>> df.vbt.barplot().show()
    ```

    ![](/assets/images/api/df_barplot.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/df_barplot.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: histplot
```
Create `vectorbtpro.generic.plotting.Histogram` and return the figure.

Usage:
    ```pycon
    >>> df.vbt.histplot().show()
    ```

    ![](/assets/images/api/df_histplot.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/df_histplot.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: boxplot
```
Create `vectorbtpro.generic.plotting.Box` and return the figure.

Usage:
    ```pycon
    >>> df.vbt.boxplot().show()
    ```

    ![](/assets/images/api/df_boxplot.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/df_boxplot.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: plot_against
```
Plot Series as a line against another line.

Args:
    other (array_like): Second array. Will broadcast.
    column (hashable): Column to plot.
    trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter`.
    other_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `other`.

        Set to 'hidden' to hide.
    pos_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for positive line.
    neg_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for negative line.
    hidden_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for hidden lines.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> df['a'].vbt.plot_against(df['b']).show()
    ```

    ![](/assets/images/api/sr_plot_against.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/sr_plot_against.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: overlay_with_heatmap
```
Plot Series as a line and overlays it with a heatmap.

Args:
    other (array_like): Second array. Will broadcast.
    column (hashable): Column to plot.
    trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter`.
    heatmap_kwargs (dict): Keyword arguments passed to `GenericDFAccessor.heatmap`.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> df['a'].vbt.overlay_with_heatmap(df['b']).show()
    ```

    ![](/assets/images/api/sr_overlay_with_heatmap.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/sr_overlay_with_heatmap.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: heatmap
```
Create a heatmap figure based on object's multi-index and values.

If the object is two-dimensional or the index is not a multi-index, returns a regular heatmap.

If multi-index contains more than two levels or you want them in specific order,
pass `x_level` and `y_level`, each (`int` if index or `str` if name) corresponding
to an axis of the heatmap. Optionally, pass `slider_level` to use a level as a slider.

Creates `vectorbtpro.generic.plotting.Heatmap` and returns the figure.

Usage:
    * Plotting a figure based on a regular index:

    ```pycon
    >>> df = pd.DataFrame([
    ...     [0, np.nan, np.nan],
    ...     [np.nan, 1, np.nan],
    ...     [np.nan, np.nan, 2]
    ... ])
    >>> df.vbt.heatmap().show()
    ```

    ![](/assets/images/api/df_heatmap.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/df_heatmap.dark.svg#only-dark){: .iimg loading=lazy }

    * Plotting a figure based on a multi-index:

    ```pycon
    >>> multi_index = pd.MultiIndex.from_tuples([
    ...     (1, 1),
    ...     (2, 2),
    ...     (3, 3)
    ... ])
    >>> sr = pd.Series(np.arange(len(multi_index)), index=multi_index)
    >>> sr
    1  1    0
    2  2    1
    3  3    2
    dtype: int64

    >>> sr.vbt.heatmap().show()
    ```

    ![](/assets/images/api/sr_heatmap.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/sr_heatmap.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: ts_heatmap
```
Heatmap of time-series data.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: volume
```
Create a 3D volume figure based on object's multi-index and values.

If multi-index contains more than three levels or you want them in specific order, pass
`x_level`, `y_level`, and `z_level`, each (`int` if index or `str` if name) corresponding
to an axis of the volume. Optionally, pass `slider_level` to use a level as a slider.

Creates `vectorbtpro.generic.plotting.Volume` and returns the figure.

Usage:
    ```pycon
    >>> multi_index = pd.MultiIndex.from_tuples([
    ...     (1, 1, 1),
    ...     (2, 2, 2),
    ...     (3, 3, 3)
    ... ])
    >>> sr = pd.Series(np.arange(len(multi_index)), index=multi_index)
    >>> sr
    1  1  1    0
    2  2  2    1
    3  3  3    2
    dtype: int64

    >>> sr.vbt.volume().show()
    ```

    ![](/assets/images/api/sr_volume.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/sr_volume.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: qqplot
```
Plot probability plot using `scipy.stats.probplot`.

`**kwargs` are passed to `GenericAccessor.scatterplot`.

Usage:
    ```pycon
    >>> pd.Series(np.random.standard_normal(100)).vbt.qqplot().show()
    ```

    ![](/assets/images/api/sr_qqplot.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/sr_qqplot.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: areaplot
```
Plot stacked area.

Args:
    line_shape (str): Line shape.
    line_visible (bool): Whether to make line visible.
    colorway (str or sequence): Name of the built-in, qualitative colorway, or a list with colors.
    trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter`.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> df.vbt.areaplot().show()
    ```

    ![](/assets/images/api/df_areaplot.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/df_areaplot.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: plot_pattern
```
Plot pattern.

Mimics the same similarity calculation procedure as implemented in
`vectorbtpro.generic.nb.patterns.pattern_similarity_nb`.

Usage:
    ```pycon
    >>> sr = pd.Series([10, 11, 12, 13, 12, 13, 14, 15, 13, 14, 11])
    >>> sr.vbt.plot_pattern([1, 2, 3, 2, 1]).show()
    ```

    ![](/assets/images/api/sr_plot_pattern.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/sr_plot_pattern.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericAccessor
#### Função: plots_defaults
```
Defaults for `GenericAccessor.plots`.

Merges `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots_defaults` and
`plots` from `vectorbtpro._settings.generic`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericSRAccessor
#### Função: fit_pattern
```
See `vectorbtpro.generic.nb.patterns.fit_pattern_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericSRAccessor
#### Função: to_renko
```
See `vectorbtpro.generic.nb.base.to_renko_1d_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericSRAccessor
#### Função: to_renko_ohlc
```
See `vectorbtpro.generic.nb.base.to_renko_ohlc_1d_nb`.
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericDFAccessor
#### Função: band
```
Calculate the band by its name.

Examples for the band name:

* "50%": 50th quantile
* "Q=50%": 50th quantile
* "Q=0.5": 50th quantile
* "Z=1.96": Z-score of 1.96
* "P=95%": One-tailed significance level of 0.95 (translated into z-score)
* "P=0.95": One-tailed significance level of 0.95 (translated into z-score)
* "median": Median (50th quantile)
* "mean": Mean across all columns
* "min": Min across all columns
* "max": Max across all columns
* "lowest": Column with the lowest final value
* "highest": Column with the highest final value
```

---

# Pasta: generic
### Arquivo: accessors.py
#### Classe: GenericDFAccessor
#### Função: plot_projections
```
Plot a DataFrame where each column is a projection.

If `plot_projections` is True, will plot each projection as a semi-transparent line.

The arguments `plot_lower`, `plot_middle`, `plot_aux_middle`, and `plot_upper` represent
bands and accept the following:

* True: Plot the band using the default quantile (20/50/80)
* False: Do not plot the band
* callable: Custom function that accepts DataFrame and reduces it across columns
* For other options see `GenericDFAccessor.band`

!!! note
    When providing z-scores, the upper should be positive, the middle should be "mean", and
    the lower should be negative. When providing significance levels, the middle should be "mean", while
    the lower should be positive and lower than the upper, for example, 25% and 75%.

Argument `colorize` allows the following values:

* False: Do not colorize
* True or "median": Colorize by median
* "mean": Colorize by mean
* "last": Colorize by last value
* callable: Custom function that accepts (rebased to 0) Series/DataFrame with
    nans already dropped and reduces it across rows

Colorization is performed by mapping the metric value of the band to the range between the
minimum and maximum value across all projections where 0 is always the middle point.
If none of the bands is plotted, projections got colorized. Otherwise, projections stay gray.

Usage:
    ```pycon
    >>> df = pd.DataFrame({
    ...     0: [10, 11, 12, 11, 10],
    ...     1: [10, 12, 14, np.nan, np.nan],
    ...     2: [10, 12, 11, 12, np.nan],
    ...     3: [10, 9, 8, 9, 8],
    ...     4: [10, 11, np.nan, np.nan, np.nan],
    ... })
    >>> df.vbt.plot_projections().show()
    ```

    ![](/assets/images/api/df_plot_projections.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/df_plot_projections.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: analyzable.py
#### Docstring do Módulo
```
Class for analyzing data.
```

---

# Pasta: generic
### Arquivo: analyzable.py
#### Classe: Analyzable
```
Class that can be analyzed by computing and plotting attributes of any kind.
```

---

# Pasta: generic
### Arquivo: decorators.py
#### Docstring do Módulo
```
Class decorators for generic accessors.
```

---

# Pasta: generic
### Arquivo: decorators.py
#### Função: attach_nb_methods
```
Class decorator to attach Numba methods.

`config` must contain target method names (keys) and dictionaries (values) with the following keys:

* `func`: Function that must be wrapped. The first argument must expect a 2-dim array.
* `is_reducing`: Whether the function is reducing. Defaults to False.
* `disable_jitted`: Whether to disable the `jitted` option.
* `disable_chunked`: Whether to disable the `chunked` option.
* `replace_signature`: Whether to replace the target signature with the source signature. Defaults to True.
* `wrap_kwargs`: Default keyword arguments for wrapping. Will be merged with the dict supplied by the user.
    Defaults to `dict(name_or_index=target_name)` for reducing functions.

The class must be a subclass of `vectorbtpro.base.wrapping.Wrapping`.
```

---

# Pasta: generic
### Arquivo: decorators.py
#### Função: attach_transform_methods
```
Class decorator to add transformation methods.

`config` must contain target method names (keys) and dictionaries (values) with the following keys:

* `transformer`: Transformer class/object.
* `docstring`: Method docstring.
* `replace_signature`: Whether to replace the target signature. Defaults to True.

The class must be a subclass of `vectorbtpro.generic.accessors.GenericAccessor`.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Docstring do Módulo
```
Base class for working with drawdown records.

Drawdown records capture information on drawdowns. Since drawdowns are ranges,
they subclass `vectorbtpro.generic.ranges.Ranges`.

!!! warning
    `Drawdowns` return both recovered AND active drawdowns, which may skew your performance results.
    To only consider recovered drawdowns, you should explicitly query `status_recovered` attribute.

Using `Drawdowns.from_price`, you can generate drawdown records for any time series and analyze them right away.

```pycon
>>> from vectorbtpro import *

>>> price = vbt.YFData.pull(
...     "BTC-USD",
...     start="2019-10 UTC",
...     end="2020-01 UTC"
... ).get('Close')
```

[=100% "100%"]{: .candystripe .candystripe-animate }

```pycon
>>> price = price.rename(None)

>>> drawdowns = vbt.Drawdowns.from_price(price, wrapper_kwargs=dict(freq='d'))

>>> drawdowns.readable
   Drawdown Id  Column               Start Index              Valley Index  \
0            0       0 2019-10-02 00:00:00+00:00 2019-10-06 00:00:00+00:00
1            1       0 2019-10-09 00:00:00+00:00 2019-10-24 00:00:00+00:00
2            2       0 2019-10-27 00:00:00+00:00 2019-12-17 00:00:00+00:00

                  End Index   Peak Value  Valley Value    End Value     Status
0 2019-10-09 00:00:00+00:00  8393.041992   7988.155762  8595.740234  Recovered
1 2019-10-25 00:00:00+00:00  8595.740234   7493.488770  8660.700195  Recovered
2 2019-12-31 00:00:00+00:00  9551.714844   6640.515137  7193.599121     Active

>>> drawdowns.duration.max(wrap_kwargs=dict(to_timedelta=True))
Timedelta('66 days 00:00:00')
```

## From accessors

Moreover, all generic accessors have a property `drawdowns` and a method `get_drawdowns`:

```pycon
>>> # vectorbtpro.generic.accessors.GenericAccessor.drawdowns.coverage
>>> price.vbt.drawdowns.coverage
0.967391304347826
```

## Stats

!!! hint
    See `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats` and `Drawdowns.metrics`.

```pycon
>>> df = pd.DataFrame({
...     'a': [1, 2, 1, 3, 2],
...     'b': [2, 3, 1, 2, 1]
... })

>>> drawdowns = df.vbt(freq='d').drawdowns

>>> drawdowns['a'].stats()
Start                                        0
End                                          4
Period                         5 days 00:00:00
Coverage [%]                              80.0
Total Records                                2
Total Recovered Drawdowns                    1
Total Active Drawdowns                       1
Active Drawdown [%]                  33.333333
Active Duration                2 days 00:00:00
Active Recovery [%]                        0.0
Active Recovery Return [%]                 0.0
Active Recovery Duration       0 days 00:00:00
Max Drawdown [%]                          50.0
Avg Drawdown [%]                          50.0
Max Drawdown Duration          2 days 00:00:00
Avg Drawdown Duration          2 days 00:00:00
Max Recovery Return [%]                  200.0
Avg Recovery Return [%]                  200.0
Max Recovery Duration          1 days 00:00:00
Avg Recovery Duration          1 days 00:00:00
Avg Recovery Duration Ratio                1.0
Name: a, dtype: object
```

By default, the metrics `max_dd`, `avg_dd`, `max_dd_duration`, and `avg_dd_duration` do
not include active drawdowns. To change that, pass `incl_active=True`:

```pycon
>>> drawdowns['a'].stats(settings=dict(incl_active=True))
Start                                        0
End                                          4
Period                         5 days 00:00:00
Coverage [%]                              80.0
Total Records                                2
Total Recovered Drawdowns                    1
Total Active Drawdowns                       1
Active Drawdown [%]                  33.333333
Active Duration                2 days 00:00:00
Active Recovery [%]                        0.0
Active Recovery Return [%]                 0.0
Active Recovery Duration       0 days 00:00:00
Max Drawdown [%]                          50.0
Avg Drawdown [%]                     41.666667
Max Drawdown Duration          2 days 00:00:00
Avg Drawdown Duration          2 days 00:00:00
Max Recovery Return [%]                  200.0
Avg Recovery Return [%]                  200.0
Max Recovery Duration          1 days 00:00:00
Avg Recovery Duration          1 days 00:00:00
Avg Recovery Duration Ratio                1.0
Name: a, dtype: object
```

`Drawdowns.stats` also supports (re-)grouping:

```pycon
>>> drawdowns['a'].stats(group_by=True)
UserWarning: Metric 'active_dd' does not support grouped data
UserWarning: Metric 'active_duration' does not support grouped data
UserWarning: Metric 'active_recovery' does not support grouped data
UserWarning: Metric 'active_recovery_return' does not support grouped data
UserWarning: Metric 'active_recovery_duration' does not support grouped data

Start                                        0
End                                          4
Period                         5 days 00:00:00
Coverage [%]                              80.0
Total Records                                2
Total Recovered Drawdowns                    1
Total Active Drawdowns                       1
Max Drawdown [%]                          50.0
Avg Drawdown [%]                          50.0
Max Drawdown Duration          2 days 00:00:00
Avg Drawdown Duration          2 days 00:00:00
Max Recovery Return [%]                  200.0
Avg Recovery Return [%]                  200.0
Max Recovery Duration          1 days 00:00:00
Avg Recovery Duration          1 days 00:00:00
Avg Recovery Duration Ratio                1.0
Name: group, dtype: object
```

## Plots

!!! hint
    See `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots` and `Drawdowns.subplots`.

`Drawdowns` class has a single subplot based on `Drawdowns.plot`:

```pycon
>>> drawdowns['a'].plots().show()
```

![](/assets/images/api/drawdowns_plots.light.svg#only-light){: .iimg loading=lazy }
![](/assets/images/api/drawdowns_plots.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
```
Extends `vectorbtpro.generic.ranges.Ranges` for working with drawdown records.

Requires `records_arr` to have all fields defined in `vectorbtpro.generic.enums.drawdown_dt`.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: from_price
```
Build `Drawdowns` from price.

`**kwargs` will be passed to `Drawdowns.__init__`.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: get_ranges
```
Get records of type `vectorbtpro.generic.ranges.Ranges` for peak-to-end ranges.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: get_decline_ranges
```
Get records of type `vectorbtpro.generic.ranges.Ranges` for peak-to-valley ranges.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: get_recovery_ranges
```
Get records of type `vectorbtpro.generic.ranges.Ranges` for valley-to-end ranges.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: get_drawdown
```
See `vectorbtpro.generic.nb.records.dd_drawdown_nb`.

Takes into account both recovered and active drawdowns.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: get_avg_drawdown
```
Get average drawdown (ADD).

Based on `Drawdowns.drawdown`.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: get_max_drawdown
```
Get maximum drawdown (MDD).

Based on `Drawdowns.drawdown`.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: get_recovery_return
```
See `vectorbtpro.generic.nb.records.dd_recovery_return_nb`.

Takes into account both recovered and active drawdowns.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: get_avg_recovery_return
```
Get average recovery return.

Based on `Drawdowns.recovery_return`.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: get_max_recovery_return
```
Get maximum recovery return.

Based on `Drawdowns.recovery_return`.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: get_decline_duration
```
See `vectorbtpro.generic.nb.records.dd_decline_duration_nb`.

Takes into account both recovered and active drawdowns.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: get_recovery_duration
```
See `vectorbtpro.generic.nb.records.dd_recovery_duration_nb`.

A value higher than 1 means the recovery was slower than the decline.

Takes into account both recovered and active drawdowns.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: get_recovery_duration_ratio
```
See `vectorbtpro.generic.nb.records.dd_recovery_duration_ratio_nb`.

Takes into account both recovered and active drawdowns.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: get_active_drawdown
```
Get drawdown of the last active drawdown only.

Does not support grouping.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: get_active_duration
```
Get duration of the last active drawdown only.

Does not support grouping.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: get_active_recovery
```
Get recovery of the last active drawdown only.

Does not support grouping.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: get_active_recovery_return
```
Get recovery return of the last active drawdown only.

Does not support grouping.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: get_active_recovery_duration
```
Get recovery duration of the last active drawdown only.

Does not support grouping.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: stats_defaults
```
Defaults for `Drawdowns.stats`.

Merges `vectorbtpro.generic.ranges.Ranges.stats_defaults` and
`stats` from `vectorbtpro._settings.drawdowns`.
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: plot
```
Plot drawdowns.

Args:
    column (str): Name of the column to plot.
    top_n (int): Filter top N drawdown records by maximum drawdown.
    plot_ohlc (bool): Whether to plot OHLC.
    plot_close (bool): Whether to plot close.
    plot_markers (bool): Whether to plot markers.
    plot_zones (bool): Whether to plot zones.
    ohlc_type: Either 'OHLC', 'Candlestick' or Plotly trace.

        Pass None to use the default.
    ohlc_trace_kwargs (dict): Keyword arguments passed to `ohlc_type`.
    close_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `Drawdowns.close`.
    peak_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for peak values.
    valley_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for valley values.
    recovery_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for recovery values.
    active_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for active recovery values.
    decline_shape_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Figure.add_shape` for decline zones.
    recovery_shape_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Figure.add_shape` for recovery zones.
    active_shape_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Figure.add_shape` for active recovery zones.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    xref (str): X coordinate axis.
    yref (str): Y coordinate axis.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> index = pd.date_range("2020", periods=8)
    >>> price = pd.Series([1, 2, 1, 2, 3, 2, 1, 2], index=index)
    >>> vbt.Drawdowns.from_price(price, wrapper_kwargs=dict(freq='1 day')).plot().show()
    ```

    ![](/assets/images/api/drawdowns_plot.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/drawdowns_plot.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: drawdowns.py
#### Classe: Drawdowns
#### Função: plots_defaults
```
Defaults for `Drawdowns.plots`.

Merges `vectorbtpro.generic.ranges.Ranges.plots_defaults` and
`plots` from `vectorbtpro._settings.drawdowns`.
```

---

# Pasta: generic
### Arquivo: enums.py
#### Docstring do Módulo
```
Named tuples and enumerated types for generic data.

Defines enums and other schemas for `vectorbtpro.generic`.
```

---

# Pasta: generic
### Arquivo: plots_builder.py
#### Docstring do Módulo
```
Mixin for building plots out of subplots.
```

---

# Pasta: generic
### Arquivo: plots_builder.py
#### Classe: MetaPlotsBuilderMixin
```
Meta class that exposes a read-only class property `PlotsBuilderMixin.subplots`.
```

---

# Pasta: generic
### Arquivo: plots_builder.py
#### Classe: PlotsBuilderMixin
```
Mixin that implements `PlotsBuilderMixin.plots`.

Required to be a subclass of `vectorbtpro.base.wrapping.Wrapping`.
```

---

# Pasta: generic
### Arquivo: plots_builder.py
#### Classe: MetaPlotsBuilderMixin
#### Função: subplots
```
Subplots supported by `PlotsBuilderMixin.plots`.
```

---

# Pasta: generic
### Arquivo: plots_builder.py
#### Classe: PlotsBuilderMixin
#### Função: plots_defaults
```
Defaults for `PlotsBuilderMixin.plots`.

See `vectorbtpro._settings.plots_builder`.
```

---

# Pasta: generic
### Arquivo: plots_builder.py
#### Classe: PlotsBuilderMixin
#### Função: subplots
```
Subplots supported by `${cls_name}`.

```python
${subplots}
```

Returns `${cls_name}._subplots`, which gets (hybrid-) copied upon creation of each instance.
Thus, changing this config won't affect the class.

To change subplots, you can either change the config in-place, override this property,
or overwrite the instance variable `${cls_name}._subplots`.
```

---

# Pasta: generic
### Arquivo: plots_builder.py
#### Classe: PlotsBuilderMixin
#### Função: plots
```
Plot various parts of this object.

Args:
    subplots (str, tuple, iterable, or dict): Subplots to plot.

        Each element can be either:

        * Subplot name (see keys in `PlotsBuilderMixin.subplots`)
        * Tuple of a subplot name and a settings dict as in `PlotsBuilderMixin.subplots`
        * Tuple of a subplot name and a template of instance `vectorbtpro.utils.template.CustomTemplate`
        * Tuple of a subplot name and a list of settings dicts to be expanded into multiple subplots

        The settings dict can contain the following keys:

        * `title`: Title of the subplot. Defaults to the name.
        * `plot_func` (required): Plotting function for custom subplots.
            Must write the supplied figure `fig` in-place and can return anything (it won't be used).
        * `xaxis_kwargs`: Layout keyword arguments for the x-axis. Defaults to `dict(title='Index')`.
        * `yaxis_kwargs`: Layout keyword arguments for the y-axis. Defaults to empty dict.
        * `tags`, `check_{filter}`, `inv_check_{filter}`, `resolve_plot_func`, `pass_{arg}`,
            `resolve_path_{arg}`, `resolve_{arg}` and `template_context`:
            The same as in `vectorbtpro.generic.stats_builder.StatsBuilderMixin` for `calc_func`.
        * Any other keyword argument that overrides the settings or is passed directly to `plot_func`.

        If `resolve_plot_func` is True, the plotting function may "request" any of the
        following arguments by accepting them or if `pass_{arg}` was found in the settings dict:

        * Each of `vectorbtpro.utils.attr_.AttrResolverMixin.self_aliases`: original object
            (ungrouped, with no column selected)
        * `group_by`: won't be passed if it was used in resolving the first attribute of `plot_func`
            specified as a path, use `pass_group_by=True` to pass anyway
        * `column`
        * `subplot_name`
        * `trace_names`: list with the subplot name, can't be used in templates
        * `add_trace_kwargs`: dict with subplot row and column index
        * `xref`
        * `yref`
        * `xaxis`
        * `yaxis`
        * `x_domain`
        * `y_domain`
        * `fig`
        * `silence_warnings`
        * Any argument from `settings`
        * Any attribute of this object if it meant to be resolved
            (see `vectorbtpro.utils.attr_.AttrResolverMixin.resolve_attr`)

        !!! note
            Layout-related resolution arguments such as `add_trace_kwargs` are unavailable
            before filtering and thus cannot be used in any templates but can still be overridden.

        Pass `subplots='all'` to plot all supported subplots.
    tags (str or iterable): See `tags` in `vectorbtpro.generic.stats_builder.StatsBuilderMixin`.
    column (str): See `column` in `vectorbtpro.generic.stats_builder.StatsBuilderMixin`.
    group_by (any): See `group_by` in `vectorbtpro.generic.stats_builder.StatsBuilderMixin`.
    silence_warnings (bool): See `silence_warnings` in `vectorbtpro.generic.stats_builder.StatsBuilderMixin`.
    template_context (mapping): See `template_context` in `vectorbtpro.generic.stats_builder.StatsBuilderMixin`.

        Applied on `settings`, `make_subplots_kwargs`, and `layout_kwargs`, and then on each subplot settings.
    filters (dict): See `filters` in `vectorbtpro.generic.stats_builder.StatsBuilderMixin`.
    settings (dict): See `settings` in `vectorbtpro.generic.stats_builder.StatsBuilderMixin`.
    subplot_settings (dict): See `metric_settings` in `vectorbtpro.generic.stats_builder.StatsBuilderMixin`.
    show_titles (bool): Whether to show the title of each subplot.
    hide_id_labels (bool): Whether to hide identical legend labels.

        Two labels are identical if their name, marker style and line style match.
    group_id_labels (bool): Whether to group identical legend labels.
    make_subplots_kwargs (dict): Keyword arguments passed to `plotly.subplots.make_subplots`.
    **layout_kwargs: Keyword arguments used to update the layout of the figure.

!!! note
    `PlotsBuilderMixin` and `vectorbtpro.generic.stats_builder.StatsBuilderMixin` are very similar.
    Some artifacts follow the same concept, just named differently:

    * `plots_defaults` vs `stats_defaults`
    * `subplots` vs `metrics`
    * `subplot_settings` vs `metric_settings`

    See further notes under `vectorbtpro.generic.stats_builder.StatsBuilderMixin`.

Usage:
    See `vectorbtpro.portfolio.base`.
```

---

# Pasta: generic
### Arquivo: plots_builder.py
#### Classe: PlotsBuilderMixin
#### Função: build_subplots_doc
```
Build subplots documentation.
```

---

# Pasta: generic
### Arquivo: plots_builder.py
#### Classe: PlotsBuilderMixin
#### Função: override_subplots_doc
```
Call this method on each subclass that overrides `PlotsBuilderMixin.subplots`.
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Docstring do Módulo
```
Base plotting functions.

Provides functions for visualizing data in an efficient and convenient way.
Each creates a figure widget that is compatible with ipywidgets and enables interactive
data visualization in Jupyter Notebook and JupyterLab environments. For more details
on using Plotly, see [Getting Started with Plotly in Python](https://plotly.com/python/getting-started/).

!!! warning
    Errors related to plotting in Jupyter environment usually appear in the logs, not under the cell.
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Função: clean_labels
```
Clean labels.

Plotly doesn't support multi-indexes.
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: TraceUpdater
#### Função: __init__
```
Base trace updating class.
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: TraceUpdater
#### Função: fig
```
Figure.
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: TraceUpdater
#### Função: traces
```
Traces to update.
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: TraceUpdater
#### Função: update_trace
```
Update one trace.
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: TraceUpdater
#### Função: update
```
Update all traces using new data.
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: Gauge
#### Função: __init__
```
Create a gauge plot.

Args:
    value (float): The value to be displayed.
    label (str): The label to be displayed.
    value_range (tuple of float): The value range of the gauge.
    cmap_name (str): A matplotlib-compatible colormap name.

        See the [list of available colormaps](https://matplotlib.org/tutorials/colors/colormaps.html).
    trace_kwargs (dict): Keyword arguments passed to the `plotly.graph_objects.Indicator`.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    make_figure_kwargs (dict): Keyword arguments passed to `vectorbtpro.utils.figure.make_figure`.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> gauge = vbt.Gauge(
    ...     value=2,
    ...     value_range=(1, 3),
    ...     label='My Gauge'
    ... )
    >>> gauge.fig.show()
    ```

    ![](/assets/images/api/Gauge.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/Gauge.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: Gauge
#### Função: value_range
```
The value range of the gauge.
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: Gauge
#### Função: cmap_name
```
A matplotlib-compatible colormap name.
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: Bar
#### Função: __init__
```
Create a bar plot.

Args:
    data (array_like): Data in any format that can be converted to NumPy.

        Must be of shape (`x_labels`, `trace_names`).
    trace_names (str or list of str): Trace names, corresponding to columns in pandas.
    x_labels (array_like): X-axis labels, corresponding to index in pandas.
    trace_kwargs (dict or list of dict): Keyword arguments passed to `plotly.graph_objects.Bar`.

        Can be specified per trace as a sequence of dicts.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    make_figure_kwargs (dict): Keyword arguments passed to `vectorbtpro.utils.figure.make_figure`.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> bar = vbt.Bar(
    ...     data=[[1, 2], [3, 4]],
    ...     trace_names=['a', 'b'],
    ...     x_labels=['x', 'y']
    ... )
    >>> bar.fig.show()
    ```

    ![](/assets/images/api/Bar.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/Bar.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: Scatter
#### Função: __init__
```
Create a scatter plot.

Args:
    data (array_like): Data in any format that can be converted to NumPy.

        Must be of shape (`x_labels`, `trace_names`).
    trace_names (str or list of str): Trace names, corresponding to columns in pandas.
    x_labels (array_like): X-axis labels, corresponding to index in pandas.
    trace_kwargs (dict or list of dict): Keyword arguments passed to `plotly.graph_objects.Scatter`.

        Can be specified per trace as a sequence of dicts.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    make_figure_kwargs (dict): Keyword arguments passed to `vectorbtpro.utils.figure.make_figure`.
    fig (Figure or FigureWidget): Figure to add traces to.
    use_gl (bool): Whether to use `plotly.graph_objects.Scattergl`.

        Defaults to the global setting. If the global setting is None, becomes True
        if there are more than 10,000 data points.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> scatter = vbt.Scatter(
    ...     data=[[1, 2], [3, 4]],
    ...     trace_names=['a', 'b'],
    ...     x_labels=['x', 'y']
    ... )
    >>> scatter.fig.show()
    ```

    ![](/assets/images/api/Scatter.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/Scatter.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: Histogram
#### Função: __init__
```
Create a histogram plot.

Args:
    data (array_like): Data in any format that can be converted to NumPy.

        Must be of shape (any, `trace_names`).
    trace_names (str or list of str): Trace names, corresponding to columns in pandas.
    horizontal (bool): Whether to plot horizontally.
    remove_nan (bool): Whether to remove NaN values.
    from_quantile (float): Filter out data points before this quantile.

        Must be in range `[0, 1]`.
    to_quantile (float): Filter out data points after this quantile.

        Must be in range `[0, 1]`.
    trace_kwargs (dict or list of dict): Keyword arguments passed to `plotly.graph_objects.Histogram`.

        Can be specified per trace as a sequence of dicts.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    make_figure_kwargs (dict): Keyword arguments passed to `vectorbtpro.utils.figure.make_figure`.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> hist = vbt.Histogram(
    ...     data=[[1, 2], [3, 4], [2, 1]],
    ...     trace_names=['a', 'b']
    ... )
    >>> hist.fig.show()
    ```

    ![](/assets/images/api/Histogram.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/Histogram.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: Histogram
#### Função: horizontal
```
Whether to plot horizontally.
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: Histogram
#### Função: remove_nan
```
Whether to remove NaN values.
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: Histogram
#### Função: from_quantile
```
Filter out data points before this quantile.
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: Histogram
#### Função: to_quantile
```
Filter out data points after this quantile.
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: Box
#### Função: __init__
```
Create a box plot.

For keyword arguments, see `Histogram`.

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> box = vbt.Box(
    ...     data=[[1, 2], [3, 4], [2, 1]],
    ...     trace_names=['a', 'b']
    ... )
    >>> box.fig.show()
    ```

    ![](/assets/images/api/Box.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/Box.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: Box
#### Função: horizontal
```
Whether to plot horizontally.
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: Box
#### Função: remove_nan
```
Whether to remove NaN values.
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: Box
#### Função: from_quantile
```
Filter out data points before this quantile.
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: Box
#### Função: to_quantile
```
Filter out data points after this quantile.
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: Heatmap
#### Função: __init__
```
Create a heatmap plot.

Args:
    data (array_like): Data in any format that can be converted to NumPy.

        Must be of shape (`y_labels`, `x_labels`).
    x_labels (array_like): X-axis labels, corresponding to columns in pandas.
    y_labels (array_like): Y-axis labels, corresponding to index in pandas.
    is_x_category (bool): Whether X-axis is a categorical axis.
    is_y_category (bool): Whether Y-axis is a categorical axis.
    trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Heatmap`.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    make_figure_kwargs (dict): Keyword arguments passed to `vectorbtpro.utils.figure.make_figure`.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> heatmap = vbt.Heatmap(
    ...     data=[[1, 2], [3, 4]],
    ...     x_labels=['a', 'b'],
    ...     y_labels=['x', 'y']
    ... )
    >>> heatmap.fig.show()
    ```

    ![](/assets/images/api/Heatmap.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/Heatmap.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: plotting.py
#### Classe: Volume
#### Função: __init__
```
Create a volume plot.

Args:
    data (array_like): Data in any format that can be converted to NumPy.

        Must be a 3-dim array.
    x_labels (array_like): X-axis labels.
    y_labels (array_like): Y-axis labels.
    z_labels (array_like): Z-axis labels.
    trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Volume`.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    scene_name (str): Reference to the 3D scene.
    make_figure_kwargs (dict): Keyword arguments passed to `vectorbtpro.utils.figure.make_figure`.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

!!! note
    Figure widgets have currently problems displaying NaNs.
    Use `.show()` method for rendering.

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> volume = vbt.Volume(
    ...     data=np.random.randint(1, 10, size=(3, 3, 3)),
    ...     x_labels=['a', 'b', 'c'],
    ...     y_labels=['d', 'e', 'f'],
    ...     z_labels=['g', 'h', 'i']
    ... )
    >>> volume.fig.show()
    ```

    ![](/assets/images/api/Volume.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/Volume.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: price_records.py
#### Docstring do Módulo
```
Base class for working with records that can make use of OHLC data.
```

---

# Pasta: generic
### Arquivo: price_records.py
#### Classe: PriceRecords
```
Extends `vectorbtpro.records.base.Records` for records that can make use of OHLC data.
```

---

# Pasta: generic
### Arquivo: price_records.py
#### Classe: PriceRecords
#### Função: from_records
```
Build `PriceRecords` from records.
```

---

# Pasta: generic
### Arquivo: price_records.py
#### Classe: PriceRecords
#### Função: resolve_row_stack_kwargs
```
Resolve keyword arguments for initializing `PriceRecords` after stacking along columns.
```

---

# Pasta: generic
### Arquivo: price_records.py
#### Classe: PriceRecords
#### Função: resolve_column_stack_kwargs
```
Resolve keyword arguments for initializing `PriceRecords` after stacking along columns.
```

---

# Pasta: generic
### Arquivo: price_records.py
#### Classe: PriceRecords
#### Função: indexing_func_meta
```
Perform indexing on `PriceRecords` and return metadata.
```

---

# Pasta: generic
### Arquivo: price_records.py
#### Classe: PriceRecords
#### Função: indexing_func
```
Perform indexing on `PriceRecords`.
```

---

# Pasta: generic
### Arquivo: price_records.py
#### Classe: PriceRecords
#### Função: resample
```
Perform resampling on `PriceRecords`.
```

---

# Pasta: generic
### Arquivo: price_records.py
#### Classe: PriceRecords
#### Função: open
```
Open price.
```

---

# Pasta: generic
### Arquivo: price_records.py
#### Classe: PriceRecords
#### Função: high
```
High price.
```

---

# Pasta: generic
### Arquivo: price_records.py
#### Classe: PriceRecords
#### Função: low
```
Low price.
```

---

# Pasta: generic
### Arquivo: price_records.py
#### Classe: PriceRecords
#### Função: close
```
Close price.
```

---

# Pasta: generic
### Arquivo: price_records.py
#### Classe: PriceRecords
#### Função: get_bar_open_time
```
Get a mapped array with the opening time of the bar.
```

---

# Pasta: generic
### Arquivo: price_records.py
#### Classe: PriceRecords
#### Função: get_bar_close_time
```
Get a mapped array with the closing time of the bar.
```

---

# Pasta: generic
### Arquivo: price_records.py
#### Classe: PriceRecords
#### Função: get_bar_open
```
Get a mapped array with the opening price of the bar.
```

---

# Pasta: generic
### Arquivo: price_records.py
#### Classe: PriceRecords
#### Função: get_bar_high
```
Get a mapped array with the high price of the bar.
```

---

# Pasta: generic
### Arquivo: price_records.py
#### Classe: PriceRecords
#### Função: get_bar_low
```
Get a mapped array with the low price of the bar.
```

---

# Pasta: generic
### Arquivo: price_records.py
#### Classe: PriceRecords
#### Função: get_bar_close
```
Get a mapped array with the closing price of the bar.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Docstring do Módulo
```
Base class for working with range records.

Range records capture information on ranges. They are useful for analyzing duration of processes,
such as drawdowns, trades, and positions. They also come in handy when analyzing distance between events,
such as entry and exit signals.

Each range has a starting point and an ending point. For example, the points for `range(20)`
are 0 and 20 (not 19!) respectively.

!!! note
    Be aware that if a range hasn't ended in a column, its `end_idx` will point at the latest index.
    Make sure to account for this when computing custom metrics involving duration.

```pycon
>>> from vectorbtpro import *

>>> start = '2019-01-01 UTC'  # crypto is in UTC
>>> end = '2020-01-01 UTC'
>>> price = vbt.YFData.pull('BTC-USD', start=start, end=end).get('Close')
```

[=100% "100%"]{: .candystripe .candystripe-animate }

```pycon
>>> fast_ma = vbt.MA.run(price, 10)
>>> slow_ma = vbt.MA.run(price, 50)
>>> fast_below_slow = fast_ma.ma_above(slow_ma)

>>> ranges = vbt.Ranges.from_array(fast_below_slow, wrapper_kwargs=dict(freq='d'))

>>> ranges.readable
   Range Id  Column           Start Timestamp             End Timestamp  \
0         0       0 2019-02-19 00:00:00+00:00 2019-07-25 00:00:00+00:00
1         1       0 2019-08-08 00:00:00+00:00 2019-08-19 00:00:00+00:00
2         2       0 2019-11-01 00:00:00+00:00 2019-11-20 00:00:00+00:00

   Status
0  Closed
1  Closed
2  Closed

>>> ranges.duration.max(wrap_kwargs=dict(to_timedelta=True))
Timedelta('156 days 00:00:00')
```

## From accessors

Moreover, all generic accessors have a property `ranges` and a method `get_ranges`:

```pycon
>>> # vectorbtpro.generic.accessors.GenericAccessor.ranges.coverage
>>> fast_below_slow.vbt.ranges.coverage
0.5081967213114754
```

## Stats

!!! hint
    See `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats` and `Ranges.metrics`.

```pycon
>>> df = pd.DataFrame({
...     'a': [1, 2, np.nan, np.nan, 5, 6],
...     'b': [np.nan, 2, np.nan, 4, np.nan, 6]
... })
>>> ranges = df.vbt(freq='d').ranges

>>> ranges['a'].stats()
Start                             0
End                               5
Period              6 days 00:00:00
Total Records                     2
Coverage                   0.666667
Overlap Coverage                0.0
Duration: Min       2 days 00:00:00
Duration: Median    2 days 00:00:00
Duration: Max       2 days 00:00:00
Name: a, dtype: object
```

`Ranges.stats` also supports (re-)grouping:

```pycon
>>> ranges.stats(group_by=True)
Start                                       0
End                                         5
Period                        6 days 00:00:00
Total Records                               5
Coverage                             0.416667
Overlap Coverage                          0.4
Duration: Min                 1 days 00:00:00
Duration: Median              1 days 00:00:00
Duration: Max                 2 days 00:00:00
Name: group, dtype: object
```

## Plots

!!! hint
    See `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots` and `Ranges.subplots`.

`Ranges` class has a single subplot based on `Ranges.plot`:

```pycon
>>> ranges['a'].plots().show()
```

![](/assets/images/api/ranges_plots.light.svg#only-light){: .iimg loading=lazy }
![](/assets/images/api/ranges_plots.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
```
Extends `vectorbtpro.generic.price_records.PriceRecords` for working with range records.

Requires `records_arr` to have all fields defined in `vectorbtpro.generic.enums.range_dt`.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: PSC
```
Class that represents a pattern search config.

Every field will be resolved into the format suitable for Numba.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: PatternRanges
```
Extends `Ranges` for working with range records generated from pattern search.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: from_array
```
Build `Ranges` from an array.

Searches for sequences of

* True values in boolean data (False acts as a gap),
* positive values in integer data (-1 acts as a gap), and
* non-NaN values in any other data (NaN acts as a gap).

If `attach_as_close` is True, will attach `arr` as `close`.

`**kwargs` will be passed to `Ranges.__init__`.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: from_delta
```
Build `Ranges` from a record/mapped array with a timedelta applied on its index field.

See `vectorbtpro.generic.nb.records.get_ranges_from_delta_nb`.

Set `delta` to an integer to wait a certain amount of rows. Set it to anything else to
wait a timedelta. The conversion is done using `vectorbtpro.utils.datetime_.to_timedelta64`.
The second option requires the index to be datetime-like, or at least the frequency to be set.

`**kwargs` will be passed to `Ranges.__init__`.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: with_delta
```
Pass self to `Ranges.from_delta`.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: crop
```
Remove any data outside the minimum start index and the maximum end index.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: filter_min_duration
```
Filter out ranges that last less than a minimum duration.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: filter_max_duration
```
Filter out ranges that last more than a maximum duration.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: get_first_pd_mask
```
Get mask from `Ranges.get_first_idx`.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: get_last_pd_mask
```
Get mask from `Ranges.get_last_idx`.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: get_ranges_pd_mask
```
Get mask from ranges.

See `vectorbtpro.generic.nb.records.ranges_to_mask_nb`.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: get_valid
```
Get valid ranges.

A valid range doesn't have the start and end index set to -1.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: get_invalid
```
Get invalid ranges.

An invalid range has the start and/or end index set to -1.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: get_first_idx
```
Get the first index in each range.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: get_last_idx
```
Get the last index in each range.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: get_duration
```
Get the effective duration of each range in integer format.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: get_real_duration
```
Get the real duration of each range in timedelta format.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: get_avg_duration
```
Get average range duration (as timedelta).
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: get_max_duration
```
Get maximum range duration (as timedelta).
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: get_coverage
```
Get coverage, that is, the number of steps that are covered by all ranges.

See `vectorbtpro.generic.nb.records.range_coverage_nb`.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: get_projections
```
Generate a projection for each range record.

See `vectorbtpro.generic.nb.records.map_ranges_to_projections_nb`.

Set `proj_start` to an integer to generate a projection after a certain row
after the start row. Set it to anything else to wait a timedelta.
The conversion is done using `vectorbtpro.utils.datetime_.to_timedelta64`.
The second option requires the index to be datetime-like, or at least the frequency to be set.

Set `proj_period` the same way as `proj_start` to generate a projection of a certain length.
Unless `extend` is True, it still respects the duration of the range.

Set `extend` to True to extend the projection even after the end of the range.
The extending period is taken from the longest range duration if `proj_period` is None,
and from the longest `proj_period` if it's not None.

Set `rebase` to True to make each projection start with 1, otherwise, each projection
will consist of original `close` values during the projected period. Use `start_value`
to replace 1 with another start value. It can also be a flexible array with elements per column.
If `start_value` is -1, will set it to the latest row in `close`.

Set `ffill` to True to forward fill NaN values, even if they are NaN in `close` itself.

Set `remove_empty` to True to remove projections that are either NaN or with only one element.
The index of each projection is still being tracked and will appear in the multi-index of the
returned DataFrame.

!!! note
    As opposed to the Numba-compiled function, the returned DataFrame will have
    projections stacked along columns rather than rows. Set `return_raw` to True
    to return them in the original format.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: stats_defaults
```
Defaults for `Ranges.stats`.

Merges `vectorbtpro.records.base.Records.stats_defaults` and
`stats` from `vectorbtpro._settings.ranges`.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: plot_projections
```
Plot projections.

Combines generation of projections using `Ranges.get_projections` and
their plotting using `vectorbtpro.generic.accessors.GenericDFAccessor.plot_projections`.

Args:
    column (str): Name of the column to plot.
    min_duration (str, int, or frequency_like): Filter range records by minimum duration.
    max_duration (str, int, or frequency_like): Filter range records by maximum duration.
    last_n (int): Select last N range records.
    top_n (int): Select top N range records by maximum duration.
    random_n (int): Select N range records randomly.
    seed (int): Seed to make output deterministic.
    proj_start (str, int, or frequency_like): See `Ranges.get_projections`.

        Allows an additional option "current_or_{value}", which sets `proj_start` to
        the duration of the current open range, and to the specified value if there is no open range.
    proj_period (str, int, or frequency_like): See `Ranges.get_projections`.

        Allows additional options "current_or_{option}", "mean", "min", "max", "median", or
        a percentage such as "50%" representing a quantile. All of those options are based
        on the duration of all the closed ranges filtered by the arguments above.
    incl_end_idx (bool): See `Ranges.get_projections`.
    extend (bool): See `Ranges.get_projections`.
    ffill (bool): See `Ranges.get_projections`.
    plot_past_period (str, int, or frequency_like): Past period to plot.

        Allows the same options as `proj_period` plus "proj_period" and "current_or_proj_period".
    plot_ohlc (bool or DataFrame): Whether to plot OHLC.
    plot_close (bool or Series): Whether to plot close.
    plot_projections (bool): See `vectorbtpro.generic.accessors.GenericDFAccessor.plot_projections`.
    plot_bands (bool): See `vectorbtpro.generic.accessors.GenericDFAccessor.plot_projections`.
    plot_lower (bool, str, or callable): See `vectorbtpro.generic.accessors.GenericDFAccessor.plot_projections`.
    plot_middle (bool, str, or callable): See `vectorbtpro.generic.accessors.GenericDFAccessor.plot_projections`.
    plot_upper (bool, str, or callable): See `vectorbtpro.generic.accessors.GenericDFAccessor.plot_projections`.
    plot_aux_middle (bool, str, or callable): See `vectorbtpro.generic.accessors.GenericDFAccessor.plot_projections`.
    plot_fill (bool): See `vectorbtpro.generic.accessors.GenericDFAccessor.plot_projections`.
    colorize (bool, str, or callable): See `vectorbtpro.generic.accessors.GenericDFAccessor.plot_projections`.
    ohlc_type: Either 'OHLC', 'Candlestick' or Plotly trace.

        Pass None to use the default.
    ohlc_trace_kwargs (dict): Keyword arguments passed to `ohlc_type`.
    close_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `Ranges.close`.
    projection_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for projections.
    lower_trace_kwargs (dict): Keyword arguments passed to `plotly.plotly.graph_objects.Scatter` for lower band.
    middle_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for middle band.
    upper_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for upper band.
    aux_middle_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for auxiliary middle band.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> price = pd.Series(
    ...     [11, 12, 13, 14, 11, 12, 13, 12, 11, 12],
    ...     index=pd.date_range("2020", periods=10),
    ... )
    >>> vbt.Ranges.from_array(
    ...     price >= 12,
    ...     attach_as_close=False,
    ...     close=price,
    ... ).plot_projections(
    ...     proj_start=0,
    ...     proj_period=4,
    ...     extend=True,
    ...     plot_past_period=None
    ... ).show()
    ```

    ![](/assets/images/api/ranges_plot_projections.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/ranges_plot_projections.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: plot_shapes
```
Plot range shapes.

Args:
    column (str): Name of the column to plot.
    plot_ohlc (bool or DataFrame): Whether to plot OHLC.
    plot_close (bool or Series): Whether to plot close.
    ohlc_type: Either 'OHLC', 'Candlestick' or Plotly trace.

        Pass None to use the default.
    ohlc_trace_kwargs (dict): Keyword arguments passed to `ohlc_type`.
    close_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `Ranges.close`.
    shape_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Figure.add_shape` for shapes.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    xref (str): X coordinate axis.
    yref (str): Y coordinate axis.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    * Plot zones colored by duration:

    ```pycon
    >>> price = pd.Series(
    ...     [1, 2, 1, 2, 3, 2, 1, 2, 3],
    ...     index=pd.date_range("2020", periods=9),
    ... )

    >>> def get_opacity(self_col, i):
    ...     real_duration = self_col.get_real_duration().values
    ...     return real_duration[i] / real_duration.max() * 0.5

    >>> vbt.Ranges.from_array(price >= 2).plot_shapes(
    ...     shape_kwargs=dict(fillcolor="teal", opacity=vbt.RepFunc(get_opacity))
    ... ).show()
    ```

    ![](/assets/images/api/ranges_plot_shapes.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/ranges_plot_shapes.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: plot
```
Plot ranges.

Args:
    column (str): Name of the column to plot.
    top_n (int): Filter top N range records by maximum duration.
    plot_ohlc (bool or DataFrame): Whether to plot OHLC.
    plot_close (bool or Series): Whether to plot close.
    plot_markers (bool): Whether to plot markers.
    plot_zones (bool): Whether to plot zones.
    ohlc_type: Either 'OHLC', 'Candlestick' or Plotly trace.

        Pass None to use the default.
    ohlc_trace_kwargs (dict): Keyword arguments passed to `ohlc_type`.
    close_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `Ranges.close`.
    start_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for start values.
    end_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for end values.
    open_shape_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Figure.add_shape` for open zones.
    closed_shape_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Figure.add_shape` for closed zones.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    xref (str): X coordinate axis.
    yref (str): Y coordinate axis.
    fig (Figure or FigureWidget): Figure to add traces to.
    return_close (bool): Whether to return the close series along with the figure.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> price = pd.Series(
    ...     [1, 2, 1, 2, 3, 2, 1, 2, 3],
    ...     index=pd.date_range("2020", periods=9),
    ... )
    >>> vbt.Ranges.from_array(price >= 2).plot().show()
    ```

    ![](/assets/images/api/ranges_plot.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/ranges_plot.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: Ranges
#### Função: plots_defaults
```
Defaults for `Ranges.plots`.

Merges `vectorbtpro.records.base.Records.plots_defaults` and
`plots` from `vectorbtpro._settings.ranges`.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: PatternRanges
#### Função: resolve_search_config
```
Resolve search config for `PatternRanges.from_pattern_search`.

Converts array-like objects into arrays and enums into integers.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: PatternRanges
#### Função: from_pattern_search
```
Build `PatternRanges` from all occurrences of a pattern in an array.

Searches for parameters of the type `vectorbtpro.utils.params.Param`, and if found, broadcasts
and combines them using `vectorbtpro.utils.params.combine_params`. Then, converts them
into a list of search configurations. If none of such parameters was found among the passed arguments,
builds one search configuration using the passed arguments. If `search_configs` is not None, uses it
instead. In all cases, it uses the defaults defined in the signature of this method to augment
search configurations. For example, passing `min_similarity` of 95% will use it in all search
configurations except where it was explicitly overridden.

Argument `search_configs` must be provided as a sequence of `PSC` instances.
If any element is a list of `PSC` instances itself, it will be used per column in `arr`,
otherwise per entire `arr`. Each configuration will be resolved using `PatternRanges.resolve_search_config`
to prepare arguments for the use in Numba.

After all the search configurations have been resolved, uses `vectorbtpro.utils.execution.execute`
to loop over each configuration and execute it using `vectorbtpro.generic.nb.records.find_pattern_1d_nb`.
The results are then concatenated into a single records array and wrapped with `PatternRanges`.

If `attach_as_close` is True, will attach `arr` as `close`.

`**kwargs` will be passed to `PatternRanges.__init__`.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: PatternRanges
#### Função: with_delta
```
Pass self to `Ranges.from_delta` but with the index set to the last index.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: PatternRanges
#### Função: resolve_row_stack_kwargs
```
Resolve keyword arguments for initializing `PatternRanges` after stacking along columns.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: PatternRanges
#### Função: resolve_column_stack_kwargs
```
Resolve keyword arguments for initializing `PatternRanges` after stacking along columns.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: PatternRanges
#### Função: indexing_func
```
Perform indexing on `PatternRanges`.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: PatternRanges
#### Função: search_configs
```
List of `PSC` instances, one per column.
```

---

# Pasta: generic
### Arquivo: ranges.py
#### Classe: PatternRanges
#### Função: plot
```
Plot pattern ranges.

Based on `Ranges.plot` and `vectorbtpro.generic.accessors.GenericSRAccessor.plot_pattern`.

Args:
    column (str): Name of the column to plot.
    top_n (int): Filter top N range records by maximum duration.
    fit_ranges (bool, int, or sequence of int): Whether or which range records to fit.

        True to fit to all range records, integer or a sequence of such to fit to specific range records.
    plot_patterns (bool or array_like): Whether to plot `PSC.pattern`.
    plot_max_error (array_like): Whether to plot `PSC.max_error`.
    fill_distance (bool): Whether to fill the space between close and pattern.

        Visible for every interpolation mode except discrete.
    pattern_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for pattern.
    lower_max_error_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for lower max error.
    upper_max_error_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for upper max error.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    xref (str): X coordinate axis.
    yref (str): Y coordinate axis.
    fig (Figure or FigureWidget): Figure to add traces to.
    **kwargs: Keyword arguments passed to `Ranges.plot`.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Docstring do Módulo
```
Mixin class for working with simulation ranges.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
```
Mixin class for working with simulation ranges.

Should be subclassed by a subclass of `vectorbtpro.base.wrapping.Wrapping`.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: row_stack_sim_start
```
Row-stack simulation start.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: row_stack_sim_end
```
Row-stack simulation end.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: column_stack_sim_start
```
Column-stack simulation start.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: column_stack_sim_end
```
Column-stack simulation end.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: sim_start_indexing_func
```
Indexing function for simulation start.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: sim_end_indexing_func
```
Indexing function for simulation end.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: resample_sim_start
```
Resample simulation start.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: resample_sim_end
```
Resample simulation end.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: resolve_sim_start_value
```
Resolve a single value of simulation start.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: resolve_sim_end_value
```
Resolve a single value of simulation end.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: resolve_sim_start
```
Resolve simulation start.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: resolve_sim_end
```
Resolve simulation end.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: get_sim_start
```
Get simulation start.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: sim_start
```
`SimRangeMixin.get_sim_start` with default arguments.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: get_sim_end
```
Get simulation end.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: sim_end
```
`SimRangeMixin.get_sim_end` with default arguments.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: get_sim_start_index
```
Get index of simulation start.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: sim_start_index
```
`SimRangeMixin.get_sim_start_index` with default arguments.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: get_sim_end_index
```
Get index of simulation end.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: sim_end_index
```
`SimRangeMixin.get_sim_end_index` with default arguments.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: get_sim_duration
```
Get duration of simulation range.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: sim_duration
```
`SimRangeMixin.get_sim_duration` with default arguments.
```

---

# Pasta: generic
### Arquivo: sim_range.py
#### Classe: SimRangeMixin
#### Função: fit_fig_to_sim_range
```
Fit figure to simulation range.
```

---

# Pasta: generic
### Arquivo: stats_builder.py
#### Docstring do Módulo
```
Mixin for building statistics out of performance metrics.
```

---

# Pasta: generic
### Arquivo: stats_builder.py
#### Classe: MetaStatsBuilderMixin
```
Meta class that exposes a read-only class property `StatsBuilderMixin.metrics`.
```

---

# Pasta: generic
### Arquivo: stats_builder.py
#### Classe: StatsBuilderMixin
```
Mixin that implements `StatsBuilderMixin.stats`.

Required to be a subclass of `vectorbtpro.base.wrapping.Wrapping`.
```

---

# Pasta: generic
### Arquivo: stats_builder.py
#### Classe: MetaStatsBuilderMixin
#### Função: metrics
```
Metrics supported by `StatsBuilderMixin.stats`.
```

---

# Pasta: generic
### Arquivo: stats_builder.py
#### Classe: StatsBuilderMixin
#### Função: stats_defaults
```
Defaults for `StatsBuilderMixin.stats`.

See `vectorbtpro._settings.stats_builder`.
```

---

# Pasta: generic
### Arquivo: stats_builder.py
#### Classe: StatsBuilderMixin
#### Função: metrics
```
Metrics supported by `${cls_name}`.

```python
${metrics}
```

Returns `${cls_name}._metrics`, which gets (hybrid-) copied upon creation of each instance.
Thus, changing this config won't affect the class.

To change metrics, you can either change the config in-place, override this property,
or overwrite the instance variable `${cls_name}._metrics`.
```

---

# Pasta: generic
### Arquivo: stats_builder.py
#### Classe: StatsBuilderMixin
#### Função: stats
```
Compute various metrics on this object.

Args:
    metrics (str, tuple, iterable, or dict): Metrics to calculate.

        Each element can be either:

        * Metric name (see keys in `StatsBuilderMixin.metrics`)
        * Tuple of a metric name and a settings dict as in `StatsBuilderMixin.metrics`
        * Tuple of a metric name and a template of instance `vectorbtpro.utils.template.CustomTemplate`
        * Tuple of a metric name and a list of settings dicts to be expanded into multiple metrics

        The settings dict can contain the following keys:

        * `title`: Title of the metric. Defaults to the name.
        * `tags`: Single or multiple tags to associate this metric with.
            If any of these tags is in `tags`, keeps this metric.
        * `check_{filter}` and `inv_check_{filter}`: Whether to check this metric against a
            filter defined in `filters`. True (or False for inverse) means to keep this metric.
        * `calc_func` (required): Calculation function for custom metrics.
            Must return either a scalar for one column/group, pd.Series for multiple columns/groups,
            or a dict of such for multiple sub-metrics.
        * `resolve_calc_func`: whether to resolve `calc_func`. If the function can be accessed
            by traversing attributes of this object, you can specify the path to this function
            as a string (see `vectorbtpro.utils.attr_.deep_getattr` for the path format).
            If `calc_func` is a function, arguments from merged metric settings are matched with
            arguments in the signature (see below). If `resolve_calc_func` is False, `calc_func`
            must accept (resolved) self and dictionary of merged metric settings.
            Defaults to True.
        * `use_shortcuts`: Whether to use shortcut properties whenever possible when
            resolving `calc_func`. Defaults to True.
        * `post_calc_func`: Function to post-process the result of `calc_func`.
            Must accept (resolved) self, output of `calc_func`, and dictionary of merged metric settings,
            and return whatever is acceptable to be returned by `calc_func`. Defaults to None.
        * `fill_wrap_kwargs`: Whether to fill `wrap_kwargs` with `to_timedelta` and `silence_warnings`.
            Defaults to False.
        * `apply_to_timedelta`: Whether to apply `vectorbtpro.base.wrapping.ArrayWrapper.arr_to_timedelta`
            on the result. To disable this globally, pass `to_timedelta=False` in `settings`.
            Defaults to False.
        * `pass_{arg}`: Whether to pass any argument from the settings (see below). Defaults to True if
            this argument was found in the function's signature. Set to False to not pass.
            If argument to be passed was not found, `pass_{arg}` is removed.
        * `resolve_path_{arg}`: Whether to resolve an argument that is meant to be an attribute of
            this object and is the first part of the path of `calc_func`. Passes only optional arguments.
            Defaults to True. See `vectorbtpro.utils.attr_.AttrResolverMixin.resolve_attr`.
        * `resolve_{arg}`: Whether to resolve an argument that is meant to be an attribute of
            this object and is present in the function's signature. Defaults to False.
            See `vectorbtpro.utils.attr_.AttrResolverMixin.resolve_attr`.
        * `use_shortcuts_{arg}`: Whether to use shortcut properties whenever possible when resolving
            an argument. Defaults to True.
        * `select_col_{arg}`: Whether to select the column from an argument that is meant to be
            an attribute of this object. Defaults to False.
        * `template_context`: Mapping to replace templates in metric settings. Used across all settings.
        * Any other keyword argument that overrides the settings or is passed directly to `calc_func`.

        If `resolve_calc_func` is True, the calculation function may "request" any of the
        following arguments by accepting them or if `pass_{arg}` was found in the settings dict:

        * Each of `vectorbtpro.utils.attr_.AttrResolverMixin.self_aliases`: original object
            (ungrouped, with no column selected)
        * `group_by`: won't be passed if it was used in resolving the first attribute of `calc_func`
            specified as a path, use `pass_group_by=True` to pass anyway
        * `column`
        * `metric_name`
        * `agg_func`
        * `silence_warnings`
        * `to_timedelta`: replaced by True if None and frequency is set
        * Any argument from `settings`
        * Any attribute of this object if it meant to be resolved
            (see `vectorbtpro.utils.attr_.AttrResolverMixin.resolve_attr`)

        Pass `metrics='all'` to calculate all supported metrics.
    tags (str or iterable): Tags to select.

        See `vectorbtpro.utils.tagging.match_tags`.
    column (str): Name of the column/group.

        !!! hint
            There are two ways to select a column: `obj['a'].stats()` and `obj.stats(column='a')`.
            They both accomplish the same thing but in different ways: `obj['a'].stats()` computes
            statistics of the column 'a' only, while `obj.stats(column='a')` computes statistics of
            all columns first and only then selects the column 'a'. The first method is preferred
            when you have a lot of data or caching is disabled. The second method is preferred when
            most attributes have already been cached.
    group_by (any): Group or ungroup columns. See `vectorbtpro.base.grouping.base.Grouper`.
    agg_func (callable): Aggregation function to aggregate statistics across all columns.
        By default, takes the mean of all columns. If None, returns all columns as a DataFrame.

        Must take `pd.Series` and return a const.

        Takes effect if `column` was specified or this object contains only one column of data.

        If `agg_func` has been overridden by a metric:

        * Takes effect if global `agg_func` is not None
        * Raises a warning if it's None but the result of calculation has multiple values
    dropna (bool): Whether to hide metrics that are all NaN.
    silence_warnings (bool): Whether to silence all warnings.
    template_context (mapping): Context used to substitute templates.

        Gets merged over `template_context` from `StatsBuilderMixin.stats_defaults`.

        Applied on `settings` and then on each metric settings.
    filters (dict): Filters to apply.

        Each item consists of the filter name and settings dict.

        The settings dict can contain the following keys:

        * `filter_func`: Filter function that must accept resolved self and
            merged settings for a metric, and return either True or False.
        * `warning_message`: Warning message to be shown when skipping a metric.
            Can be a template that will be substituted using merged metric settings as context.
            Defaults to None.
        * `inv_warning_message`: Same as `warning_message` but for inverse checks.

        Gets merged over `filters` from `StatsBuilderMixin.stats_defaults`.
    settings (dict): Global settings and resolution arguments.

        Extends/overrides `settings` from `StatsBuilderMixin.stats_defaults`.
        Gets extended/overridden by metric settings.
    metric_settings (dict): Keyword arguments for each metric.

        Extends/overrides all global and metric settings.

For template logic, see `vectorbtpro.utils.template`.

For defaults, see `StatsBuilderMixin.stats_defaults`.

!!! hint
    There are two types of arguments: optional (or resolution) and mandatory arguments.
    Optional arguments are only passed if they are found in the function's signature.
    Mandatory arguments are passed regardless of this. Optional arguments can only be defined
    using `settings` (that is, globally), while mandatory arguments can be defined both using
    default metric settings and `{metric_name}_kwargs`. Overriding optional arguments using default
    metric settings or `{metric_name}_kwargs` won't turn them into mandatory. For this, pass `pass_{arg}=True`.

!!! hint
    Make sure to resolve and then to re-use as many object attributes as possible to
    utilize built-in caching (even if global caching is disabled).

Usage:
    See `vectorbtpro.portfolio.base`.
```

---

# Pasta: generic
### Arquivo: stats_builder.py
#### Classe: StatsBuilderMixin
#### Função: build_metrics_doc
```
Build metrics documentation.
```

---

# Pasta: generic
### Arquivo: stats_builder.py
#### Classe: StatsBuilderMixin
#### Função: override_metrics_doc
```
Call this method on each subclass that overrides `StatsBuilderMixin.metrics`.
```

---

# Pasta: generic
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules for working with generic time series.

In contrast to the `vectorbtpro.base` sub-package, focuses on the data itself.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Docstring do Módulo
```
Generic Numba-compiled functions for mapping, applying, and reducing.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: map_1d_nb
```
Map elements element-wise using `map_func_nb`.

`map_func_nb` must accept the element and `*args`. Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: map_nb
```
2-dim version of `map_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: map_1d_meta_nb
```
Meta version of `map_1d_nb`.

`map_func_nb` must accept the row index, the column index, and `*args`.
Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: map_meta_nb
```
2-dim version of `map_1d_meta_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: apply_nb
```
Apply function on each column of an object.

`apply_func_nb` must accept the array and `*args`.
Must return a single value or an array of shape `a.shape[1]`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: apply_meta_nb
```
Meta version of `apply_nb` that prepends the column index to the arguments of `apply_func_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: row_apply_nb
```
`apply_nb` but applied on rows rather than columns.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: row_apply_meta_nb
```
Meta version of `row_apply_nb` that prepends the row index to the arguments of `apply_func_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: rolling_reduce_1d_nb
```
Provide rolling window calculations.

`reduce_func_nb` must accept the array and `*args`. Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: rolling_reduce_nb
```
2-dim version of `rolling_reduce_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: rolling_reduce_two_1d_nb
```
Provide rolling window calculations for two arrays.

`reduce_func_nb` must accept two arrays and `*args`. Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: rolling_reduce_two_nb
```
2-dim version of `rolling_reduce_two_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: rolling_reduce_1d_meta_nb
```
Meta version of `rolling_reduce_1d_nb`.

`reduce_func_nb` must accept the start row index, the end row index, the column, and `*args`.
Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: rolling_reduce_meta_nb
```
2-dim version of `rolling_reduce_1d_meta_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: rolling_freq_reduce_1d_nb
```
Provide rolling, frequency-based window calculations.

`reduce_func_nb` must accept the array and `*args`. Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: rolling_freq_reduce_nb
```
2-dim version of `rolling_reduce_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: rolling_freq_reduce_1d_meta_nb
```
Meta version of `rolling_freq_reduce_1d_nb`.

`reduce_func_nb` must accept the start row index, the end row index, the column, and `*args`.
Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: rolling_freq_reduce_meta_nb
```
2-dim version of `rolling_freq_reduce_1d_meta_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: groupby_reduce_1d_nb
```
Provide group-by reduce calculations.

`reduce_func_nb` must accept the array and `*args`. Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: groupby_reduce_nb
```
2-dim version of `groupby_reduce_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: groupby_reduce_1d_meta_nb
```
Meta version of `groupby_reduce_1d_nb`.

`reduce_func_nb` must accept the array of indices in the group, the group index, the column index,
and `*args`. Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: groupby_reduce_meta_nb
```
2-dim version of `groupby_reduce_1d_meta_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: groupby_transform_nb
```
Provide group-by transform calculations.

`transform_func_nb` must accept the 2-dim array of the group and `*args`. Must return a scalar
or an array that broadcasts against the group array's shape.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: groupby_transform_meta_nb
```
Meta version of `groupby_transform_nb`.

`transform_func_nb` must accept the array of indices in the group, the group index, and `*args`.
Must return a scalar or an array that broadcasts against the group's shape.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: reduce_index_ranges_1d_nb
```
Reduce each index range.

`reduce_func_nb` must accept the array and `*args`. Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: reduce_index_ranges_nb
```
2-dim version of `reduce_index_ranges_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: reduce_index_ranges_1d_meta_nb
```
Meta version of `reduce_index_ranges_1d_nb`.

`reduce_func_nb` must accept the start row index, the end row index, the column,
and `*args`. Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: reduce_index_ranges_meta_nb
```
2-dim version of `reduce_index_ranges_1d_meta_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: apply_and_reduce_1d_nb
```
Apply `apply_func_nb` and reduce into a single value using `reduce_func_nb`.

`apply_func_nb` must accept the array and `*apply_args`.
Must return an array.

`reduce_func_nb` must accept the array of results from `apply_func_nb` and `*reduce_args`.
Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: apply_and_reduce_nb
```
2-dim version of `apply_and_reduce_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: apply_and_reduce_1d_meta_nb
```
Meta version of `apply_and_reduce_1d_nb`.

`apply_func_nb` must accept the column index, the array, and `*apply_args`.
Must return an array.

`reduce_func_nb` must accept the column index, the array of results from `apply_func_nb`, and `*reduce_args`.
Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: apply_and_reduce_meta_nb
```
2-dim version of `apply_and_reduce_1d_meta_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: reduce_nb
```
Reduce each column into a single value using `reduce_func_nb`.

`reduce_func_nb` must accept the array and `*args`. Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: reduce_meta_nb
```
Meta version of `reduce_nb`.

`reduce_func_nb` must accept the column index and `*args`. Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: reduce_to_array_nb
```
Same as `reduce_nb` but `reduce_func_nb` must return an array.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: reduce_to_array_meta_nb
```
Same as `reduce_meta_nb` but `reduce_func_nb` must return an array.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: reduce_grouped_nb
```
Reduce each group of columns into a single value using `reduce_func_nb`.

`reduce_func_nb` must accept the 2-dim array and `*args`. Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: reduce_grouped_meta_nb
```
Meta version of `reduce_grouped_nb`.

`reduce_func_nb` must accept the column indices of the group, the group index, and `*args`.
Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: flatten_forder_nb
```
Flatten the array in F order.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: reduce_flat_grouped_nb
```
Same as `reduce_grouped_nb` but passes flattened array.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: reduce_grouped_to_array_nb
```
Same as `reduce_grouped_nb` but `reduce_func_nb` must return an array.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: reduce_grouped_to_array_meta_nb
```
Same as `reduce_grouped_meta_nb` but `reduce_func_nb` must return an array.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: reduce_flat_grouped_to_array_nb
```
Same as `reduce_grouped_to_array_nb` but passes flattened array.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: squeeze_grouped_nb
```
Squeeze each group of columns into a single column using `squeeze_func_nb`.

`squeeze_func_nb` must accept index the array and `*args`. Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: squeeze_grouped_meta_nb
```
Meta version of `squeeze_grouped_nb`.

`squeeze_func_nb` must accept the row index, the column indices of the group,
the group index, and `*args`. Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: flatten_grouped_nb
```
Flatten each group of columns.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: flatten_uniform_grouped_nb
```
Flatten each group of columns of the same length.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: proximity_reduce_nb
```
Flatten `window` surrounding rows and columns and reduce them into a single value using `reduce_func_nb`.

`reduce_func_nb` must accept the array and `*args`. Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: proximity_reduce_meta_nb
```
Meta version of `proximity_reduce_nb`.

`reduce_func_nb` must accept the start row index, the end row index, the start column index,
the end column index, and `*args`. Must return a single value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: nth_reduce_nb
```
Get n-th element.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: first_reduce_nb
```
Get first non-NA element.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: last_reduce_nb
```
Get last non-NA element.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: first_index_reduce_nb
```
Get index of first non-NA element.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: last_index_reduce_nb
```
Get index of last non-NA element.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: nth_index_reduce_nb
```
Get index of n-th element including NA elements.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: any_reduce_nb
```
Get whether any of the elements are True.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: all_reduce_nb
```
Get whether all of the elements are True.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: min_reduce_nb
```
Get min. Ignores NaN.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: max_reduce_nb
```
Get max. Ignores NaN.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: mean_reduce_nb
```
Get mean. Ignores NaN.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: median_reduce_nb
```
Get median. Ignores NaN.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: std_reduce_nb
```
Get std. Ignores NaN.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: sum_reduce_nb
```
Get sum. Ignores NaN.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: prod_reduce_nb
```
Get product. Ignores NaN.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: nonzero_prod_reduce_nb
```
Get product. Ignores zero and NaN. Default value is zero.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: count_reduce_nb
```
Get count. Ignores NaN.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: argmin_reduce_nb
```
Get position of min.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: argmax_reduce_nb
```
Get position of max.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: describe_reduce_nb
```
Get descriptive statistics. Ignores NaN.

Numba equivalent to `pd.Series(arr).describe(perc)`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: cov_reduce_grouped_meta_nb
```
Get correlation coefficient. Ignores NaN.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: corr_reduce_grouped_meta_nb
```
Get correlation coefficient. Ignores NaN.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: apply_reduce.py
#### Função: wmean_range_reduce_meta_nb
```
Get the weighted average.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Docstring do Módulo
```
Generic Numba-compiled functions for base operations.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: select_indices_1d_nb
```
Set each element to a value by boolean mask.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: select_indices_nb
```
2-dim version of `select_indices_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: shuffle_1d_nb
```
Shuffle each column in the array.

Specify `seed` to make output deterministic.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: shuffle_nb
```
2-dim version of `shuffle_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: set_by_mask_1d_nb
```
Set each element to a value by boolean mask.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: set_by_mask_nb
```
2-dim version of `set_by_mask_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: set_by_mask_mult_1d_nb
```
Set each element in one array to the corresponding element in another by boolean mask.

`values` must be of the same shape as in the array.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: set_by_mask_mult_nb
```
2-dim version of `set_by_mask_mult_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: first_valid_index_1d_nb
```
Get the index of the first valid value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: first_valid_index_nb
```
2-dim version of `first_valid_index_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: last_valid_index_1d_nb
```
Get the index of the last valid value.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: last_valid_index_nb
```
2-dim version of `last_valid_index_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: fillna_1d_nb
```
Replace NaNs with value.

Numba equivalent to `pd.Series(arr).fillna(value)`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: fillna_nb
```
2-dim version of `fillna_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: fbfill_nb
```
Forward and backward fill NaN values.

!!! note
    If there are no NaN (or any) values, will return `arr`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: bshift_1d_nb
```
Shift backward by `n` positions.

Numba equivalent to `pd.Series(arr).shift(-n)`.

!!! warning
    This operation looks ahead.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: bshift_nb
```
2-dim version of `bshift_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: fshift_1d_nb
```
Shift forward by `n` positions.

Numba equivalent to `pd.Series(arr).shift(n)`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: fshift_nb
```
2-dim version of `fshift_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: diff_1d_nb
```
Compute the 1-th discrete difference.

Numba equivalent to `pd.Series(arr).diff()`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: diff_nb
```
2-dim version of `diff_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: pct_change_1d_nb
```
Compute the percentage change.

Numba equivalent to `pd.Series(arr).pct_change()`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: pct_change_nb
```
2-dim version of `pct_change_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: bfill_1d_nb
```
Fill NaNs by propagating first valid observation backward.

Numba equivalent to `pd.Series(arr).fillna(method='bfill')`.

!!! warning
    This operation looks ahead.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: bfill_nb
```
2-dim version of `bfill_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: ffill_1d_nb
```
Fill NaNs by propagating last valid observation forward.

Numba equivalent to `pd.Series(arr).fillna(method='ffill')`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: ffill_nb
```
2-dim version of `ffill_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nanprod_nb
```
Numba equivalent of `np.nanprod` along axis 0.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nancumsum_nb
```
Numba equivalent of `np.nancumsum` along axis 0.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nancumprod_nb
```
Numba equivalent of `np.nancumprod` along axis 0.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nansum_nb
```
Numba equivalent of `np.nansum` along axis 0.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nancnt_1d_nb
```
Compute count while ignoring NaNs and not allocating any arrays.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nancnt_nb
```
2-dim version of `nancnt_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nanmin_nb
```
Numba equivalent of `np.nanmin` along axis 0.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nanmax_nb
```
Numba equivalent of `np.nanmax` along axis 0.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nanmean_nb
```
Numba equivalent of `np.nanmean` along axis 0.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nanmedian_nb
```
Numba equivalent of `np.nanmedian` along axis 0.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nanpercentile_noarr_1d_nb
```
Numba equivalent of `np.nanpercentile` that does not allocate any arrays.

!!! note
    Has worst case time complexity of O(N^2), which makes it much slower than `np.nanpercentile`,
    but still faster if used in rolling calculations, especially for `q` near 0 and 100.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nanpartition_mean_noarr_1d_nb
```
Average of `np.partition` that ignores NaN values and does not allocate any arrays.

!!! note
    Has worst case time complexity of O(N^2), which makes it much slower than `np.partition`,
    but still faster if used in rolling calculations, especially for `q` near 0.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nanvar_1d_nb
```
Numba equivalent of `np.nanvar` that does not allocate any arrays.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nanstd_1d_nb
```
Numba equivalent of `np.nanstd`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nanstd_nb
```
2-dim version of `nanstd_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nancov_1d_nb
```
Numba equivalent of `np.cov` that ignores NaN values.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nancov_nb
```
2-dim version of `nancov_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nancorr_1d_nb
```
Numba equivalent of `np.corrcoef` that ignores NaN values.

Numerically stable.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: nancorr_nb
```
2-dim version of `nancorr_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: rank_1d_nb
```
Compute numerical data ranks.

Numba equivalent to `pd.Series(arr).rank(pct=pct)`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: rank_nb
```
2-dim version of `rank_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: polyfit_1d_nb
```
Compute the least squares polynomial fit.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: fir_filter_1d_nb
```
Filter data along one-dimension with an FIR filter.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: value_counts_nb
```
Compute value counts per column/group.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: value_counts_1d_nb
```
Compute value counts.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: value_counts_per_row_nb
```
Compute value counts per row.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: repartition_nb
```
Repartition a 2-dimensional array into a 1-dimensional by removing empty elements.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: crossed_above_1d_nb
```
Get the crossover of the first array going above the second array.

If `dropna` is True, produces the same results as if all rows with at least one NaN were dropped.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: crossed_above_nb
```
2-dim version of `crossed_above_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: crossed_below_1d_nb
```
Get the crossover of the first array going below the second array.

If `dropna` is True, produces the same results as if all rows with at least one NaN were dropped.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: crossed_below_nb
```
2-dim version of `crossed_below_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: demean_nb
```
Demean each value within its group.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: to_renko_1d_nb
```
Convert to Renko format.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: to_renko_ohlc_1d_nb
```
Convert to Renko OHLC format.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: realign_1d_nb
```
Get the latest in `arr` at each index in `target_index` based on `source_index`.

If `source_rbound` is True, then each element in `source_index` is effectively located at
the right bound, which is the frequency or the next element (excluding) if the frequency is None.
The same for `target_rbound` and `target_index`.

!!! note
    Both index arrays must be increasing. Repeating values are allowed.

    If `arr` contains bar data, both indexes must represent the opening time.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: base.py
#### Função: realign_nb
```
2-dim version of `realign_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: iter_.py
#### Docstring do Módulo
```
Generic Numba-compiled functions for iterative use.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: iter_.py
#### Função: iter_above_nb
```
Check whether `arr1` is above `arr2` at specific row and column.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: iter_.py
#### Função: iter_below_nb
```
Check whether `arr1` is below `arr2` at specific row and column.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: iter_.py
#### Função: iter_crossed_above_nb
```
Check whether `arr1` crossed above `arr2` at specific row and column.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: iter_.py
#### Função: iter_crossed_below_nb
```
Check whether `arr1` crossed below `arr2` at specific row and column.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: patterns.py
#### Docstring do Módulo
```
Generic Numba-compiled functions for working with patterns.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: patterns.py
#### Função: linear_interp_nb
```
Get the value at a specific position in a target size using linear interpolation.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: patterns.py
#### Função: nearest_interp_nb
```
Get the value at a specific position in a target size using nearest-neighbor interpolation.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: patterns.py
#### Função: discrete_interp_nb
```
Get the value at a specific position in a target size using discrete interpolation.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: patterns.py
#### Função: mixed_interp_nb
```
Get the value at a specific position in a target size using mixed interpolation.

Mixed interpolation is based on the discrete interpolation, while filling resulting NaN values
using the linear interpolation. This way, the vertical scale of the pattern array is respected.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: patterns.py
#### Função: interp_nb
```
Get the value at a specific position in a target size using an interpolation mode.

See `vectorbtpro.generic.enums.InterpMode`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: patterns.py
#### Função: interp_resize_1d_nb
```
Resize an array using `interp_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: patterns.py
#### Função: fit_pattern_nb
```
Fit pattern.

Returns the resized and rescaled pattern and max error.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: patterns.py
#### Função: pattern_similarity_nb
```
Get the similarity between an array and a pattern array.

At each position in the array, the value in `arr` is first mapped into the range of `pattern`.
Then, the absolute distance between the actual and expected value is calculated (= error).
This error is then divided by the maximum error at this position to get a relative value between 0 and 1.
Finally, all relative errors are added together and subtracted from 1 to get the similarity measure.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Docstring do Módulo
```
Generic Numba-compiled functions for records.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Função: get_ranges_nb
```
Fill range records between gaps.

Usage:
    * Find ranges in time series:

    ```pycon
    >>> from vectorbtpro import *

    >>> a = np.array([
    ...     [np.nan, np.nan, np.nan, np.nan],
    ...     [     2, np.nan, np.nan, np.nan],
    ...     [     3,      3, np.nan, np.nan],
    ...     [np.nan,      4,      4, np.nan],
    ...     [     5, np.nan,      5,      5],
    ...     [     6,      6, np.nan,      6]
    ... ])
    >>> records = vbt.nb.get_ranges_nb(a, np.nan)

    >>> pd.DataFrame.from_records(records)
       id  col  start_idx  end_idx  status
    0   0    0          1        3       1
    1   1    0          4        5       0
    2   0    1          2        4       1
    3   1    1          5        5       0
    4   0    2          3        5       1
    5   0    3          4        5       0
    ```
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Função: get_ranges_from_delta_nb
```
Build delta ranges.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Função: range_duration_nb
```
Get duration of each range record.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Função: range_coverage_nb
```
Get coverage of range records.

Set `overlapping` to True to get the number of overlapping steps.
Set `normalize` to True to get the number of steps in relation either to the total number of steps
(when `overlapping=False`) or to the number of covered steps (when `overlapping=True`).
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Função: ranges_to_mask_nb
```
Convert ranges to 2-dim mask.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Função: map_ranges_to_projections_nb
```
Map each range into a projection.

Returns two arrays:

1. One-dimensional array where elements are record indices
2. Two-dimensional array where rows are projections
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Função: find_pattern_1d_nb
```
Find all occurrences of a pattern in an array.

Uses `vectorbtpro.generic.nb.patterns.pattern_similarity_nb` to fill records of the type
`vectorbtpro.generic.enums.pattern_range_dt`.

Goes through the array, and for each window selected between `window` and `max_window` (including),
checks whether the window of array values is similar enough to the pattern. If so, writes a new
range to the output array. If `window_select_prob` is set, decides whether to test a window based on
the given probability. The same for `row_select_prob` but on rows.

If `roll_forward`, windows are rolled forward (`start_idx` is guaranteed to be sorted), otherwise
backward (`end_idx` is guaranteed to be sorted).

By default, creates an empty record array of the same size as the number of rows in `arr`.
This can be increased or decreased using `max_records`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Função: find_pattern_nb
```
2-dim version of `find_pattern_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Função: drawdown_1d_nb
```
Compute drawdown.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Função: drawdown_nb
```
2-dim version of `drawdown_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Função: fill_drawdown_record_nb
```
Fill a drawdown record.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Função: get_drawdowns_nb
```
Fill drawdown records by analyzing a time series.

Only `close` must be provided, other time series are optional.

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> close = np.array([
    ...     [1, 5, 1, 3],
    ...     [2, 4, 2, 2],
    ...     [3, 3, 3, 1],
    ...     [4, 2, 2, 2],
    ...     [5, 1, 1, 3]
    ... ])
    >>> records = vbt.nb.get_drawdowns_nb(None, None, None, close)

    >>> pd.DataFrame.from_records(records)
       id  col  start_idx  valley_idx  end_idx  start_val  valley_val  end_val  \
    0   0    1          0           4        4        5.0         1.0      1.0
    1   0    2          2           4        4        3.0         1.0      1.0
    2   0    3          0           2        4        3.0         1.0      3.0

       status
    0       0
    1       0
    2       1
    ```
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Função: dd_drawdown_nb
```
Compute the drawdown of each drawdown record.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Função: dd_decline_duration_nb
```
Compute the duration of the peak-to-valley phase of each drawdown record.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Função: dd_recovery_duration_nb
```
Compute the duration of the valley-to-recovery phase of each drawdown record.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Função: dd_recovery_duration_ratio_nb
```
Compute the ratio of the recovery duration to the decline duration of each drawdown record.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Função: dd_recovery_return_nb
```
Compute the recovery return of each drawdown record.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: records.py
#### Função: bar_price_nb
```
Return the bar price.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Docstring do Módulo
```
Generic Numba-compiled functions for rolling and expanding windows.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_sum_acc_nb
```
Accumulator of `rolling_sum_1d_nb`.

Takes a state of type `vectorbtpro.generic.enums.RollSumAIS` and returns
a state of type `vectorbtpro.generic.enums.RollSumAOS`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_sum_1d_nb
```
Compute rolling sum.

Uses `rolling_sum_acc_nb` at each iteration.

Numba equivalent to `pd.Series(arr).rolling(window, min_periods=minp).sum()`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_sum_nb
```
2-dim version of `rolling_sum_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_prod_acc_nb
```
Accumulator of `rolling_prod_1d_nb`.

Takes a state of type `vectorbtpro.generic.enums.RollProdAIS` and returns
a state of type `vectorbtpro.generic.enums.RollProdAOS`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_prod_1d_nb
```
Compute rolling product.

Uses `rolling_prod_acc_nb` at each iteration.

Numba equivalent to `pd.Series(arr).rolling(window, min_periods=minp).apply(np.prod)`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_prod_nb
```
2-dim version of `rolling_prod_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_mean_acc_nb
```
Accumulator of `rolling_mean_1d_nb`.

Takes a state of type `vectorbtpro.generic.enums.RollMeanAIS` and returns
a state of type `vectorbtpro.generic.enums.RollMeanAOS`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_mean_1d_nb
```
Compute rolling mean.

Uses `rolling_mean_acc_nb` at each iteration.

Numba equivalent to `pd.Series(arr).rolling(window, min_periods=minp).mean()`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_mean_nb
```
2-dim version of `rolling_mean_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_std_acc_nb
```
Accumulator of `rolling_std_1d_nb`.

Takes a state of type `vectorbtpro.generic.enums.RollStdAIS` and returns
a state of type `vectorbtpro.generic.enums.RollStdAOS`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_std_1d_nb
```
Compute rolling standard deviation.

Uses `rolling_std_acc_nb` at each iteration.

Numba equivalent to `pd.Series(arr).rolling(window, min_periods=minp).std(ddof=ddof)`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_std_nb
```
2-dim version of `rolling_std_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_zscore_acc_nb
```
Accumulator of `rolling_zscore_1d_nb`.

Takes a state of type `vectorbtpro.generic.enums.RollZScoreAIS` and returns
a state of type `vectorbtpro.generic.enums.RollZScoreAOS`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_zscore_1d_nb
```
Compute rolling z-score.

Uses `rolling_zscore_acc_nb` at each iteration.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_zscore_nb
```
2-dim version of `rolling_zscore_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: wm_mean_acc_nb
```
Accumulator of `wm_mean_1d_nb`.

Takes a state of type `vectorbtpro.generic.enums.WMMeanAIS` and returns
a state of type `vectorbtpro.generic.enums.WMMeanAOS`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: wm_mean_1d_nb
```
Compute weighted moving average.

Uses `wm_mean_acc_nb` at each iteration.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: wm_mean_nb
```
2-dim version of `wm_mean_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: alpha_from_com_nb
```
Get the smoothing factor `alpha` from a center of mass.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: alpha_from_span_nb
```
Get the smoothing factor `alpha` from a span.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: alpha_from_halflife_nb
```
Get the smoothing factor `alpha` from a half-life.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: alpha_from_wilder_nb
```
Get the smoothing factor `alpha` from a Wilder's period.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: ewm_mean_acc_nb
```
Accumulator of `ewm_mean_1d_nb`.

Takes a state of type `vectorbtpro.generic.enums.EWMMeanAIS` and returns
a state of type `vectorbtpro.generic.enums.EWMMeanAOS`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: ewm_mean_1d_nb
```
Compute exponential weighted moving average.

Uses `ewm_mean_acc_nb` at each iteration.

Numba equivalent to `pd.Series(arr).ewm(span=span, min_periods=minp, adjust=adjust).mean()`.

Adaptation of `pd._libs.window.aggregations.window_aggregations.ewma` with default arguments.

!!! note
    In contrast to the Pandas implementation, `minp` is effective within `span`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: ewm_mean_nb
```
2-dim version of `ewm_mean_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: ewm_std_acc_nb
```
Accumulator of `ewm_std_1d_nb`.

Takes a state of type `vectorbtpro.generic.enums.EWMStdAIS` and returns
a state of type `vectorbtpro.generic.enums.EWMStdAOS`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: ewm_std_1d_nb
```
Compute exponential weighted moving standard deviation.

Uses `ewm_std_acc_nb` at each iteration.

Numba equivalent to `pd.Series(arr).ewm(span=span, min_periods=minp).std()`.

Adaptation of `pd._libs.window.aggregations.window_aggregations.ewmcov` with default arguments.

!!! note
    In contrast to the Pandas implementation, `minp` is effective within `span`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: ewm_std_nb
```
2-dim version of `ewm_std_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: wwm_mean_1d_nb
```
Compute Wilder's exponential weighted moving average.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: wwm_mean_nb
```
2-dim version of `wwm_mean_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: wwm_std_1d_nb
```
Compute Wilder's exponential weighted moving standard deviation.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: wwm_std_nb
```
2-dim version of `wwm_std_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: vidya_acc_nb
```
Accumulator of `vidya_1d_nb`.

Takes a state of type `vectorbtpro.generic.enums.VidyaAIS` and returns
a state of type `vectorbtpro.generic.enums.VidyaAOS`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: vidya_1d_nb
```
Compute VIDYA.

Uses `vidya_acc_nb` at each iteration.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: vidya_nb
```
2-dim version of `vidya_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: ma_1d_nb
```
Compute a moving average based on the mode of the type `vectorbtpro.generic.enums.WType`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: ma_nb
```
2-dim version of `ma_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: msd_1d_nb
```
Compute a moving standard deviation based on the mode of the type `vectorbtpro.generic.enums.WType`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: msd_nb
```
2-dim version of `msd_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_cov_acc_nb
```
Accumulator of `rolling_cov_1d_nb`.

Takes a state of type `vectorbtpro.generic.enums.RollCovAIS` and returns
a state of type `vectorbtpro.generic.enums.RollCovAOS`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_cov_1d_nb
```
Compute rolling covariance.

Numba equivalent to `pd.Series(arr1).rolling(window, min_periods=minp).cov(arr2)`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_cov_nb
```
2-dim version of `rolling_cov_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_corr_acc_nb
```
Accumulator of `rolling_corr_1d_nb`.

Takes a state of type `vectorbtpro.generic.enums.RollCorrAIS` and returns
a state of type `vectorbtpro.generic.enums.RollCorrAOS`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_corr_1d_nb
```
Compute rolling correlation coefficient.

Numba equivalent to `pd.Series(arr1).rolling(window, min_periods=minp).corr(arr2)`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_corr_nb
```
2-dim version of `rolling_corr_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_ols_acc_nb
```
Accumulator of `rolling_ols_1d_nb`.

Takes a state of type `vectorbtpro.generic.enums.RollOLSAIS` and returns
a state of type `vectorbtpro.generic.enums.RollOLSAOS`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_ols_1d_nb
```
Compute rolling linear regression.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_ols_nb
```
2-dim version of `rolling_ols_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_rank_1d_nb
```
Rolling version of `rank_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_rank_nb
```
2-dim version of `rolling_rank_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_min_1d_nb
```
Compute rolling min.

Numba equivalent to `pd.Series(arr).rolling(window, min_periods=minp).min()`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_min_nb
```
2-dim version of `rolling_min_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_max_1d_nb
```
Compute rolling max.

Numba equivalent to `pd.Series(arr).rolling(window, min_periods=minp).max()`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_max_nb
```
2-dim version of `rolling_max_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_argmin_1d_nb
```
Compute rolling min index.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_argmin_nb
```
2-dim version of `rolling_argmin_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_argmax_1d_nb
```
Compute rolling max index.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_argmax_nb
```
2-dim version of `rolling_argmax_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_any_1d_nb
```
Compute rolling any.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_any_nb
```
2-dim version of `rolling_any_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_all_1d_nb
```
Compute rolling all.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_all_nb
```
2-dim version of `rolling_all_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_pattern_similarity_1d_nb
```
Compute rolling pattern similarity.

Uses `vectorbtpro.generic.nb.patterns.pattern_similarity_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: rolling_pattern_similarity_nb
```
2-dim version of `rolling_pattern_similarity_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: expanding_min_1d_nb
```
Compute expanding min.

Numba equivalent to `pd.Series(arr).expanding(min_periods=minp).min()`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: expanding_min_nb
```
2-dim version of `expanding_min_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: expanding_max_1d_nb
```
Compute expanding max.

Numba equivalent to `pd.Series(arr).expanding(min_periods=minp).max()`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: rolling.py
#### Função: expanding_max_nb
```
2-dim version of `expanding_max_1d_nb`.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Docstring do Módulo
```
Generic Numba-compiled functions for simulation ranges.

!!! warning
    Resolution is more flexible and may return None while preparation always returns NumPy arrays.
    Thus, use preparation, not resolution, in Numba-parallel workflows.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Função: resolve_sim_start_nb
```
Resolve simulation start.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Função: resolve_sim_end_nb
```
Resolve simulation end.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Função: resolve_sim_range_nb
```
Resolve simulation start and end.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Função: resolve_grouped_sim_start_nb
```
Resolve grouped simulation start.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Função: resolve_grouped_sim_end_nb
```
Resolve grouped simulation end.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Função: resolve_grouped_sim_range_nb
```
Resolve grouped simulation start and end.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Função: resolve_ungrouped_sim_start_nb
```
Resolve ungrouped simulation start.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Função: resolve_ungrouped_sim_end_nb
```
Resolve ungrouped simulation end.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Função: resolve_ungrouped_sim_range_nb
```
Resolve ungrouped simulation start and end.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Função: prepare_sim_start_nb
```
Prepare simulation start.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Função: prepare_sim_end_nb
```
Prepare simulation end.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Função: prepare_sim_range_nb
```
Prepare simulation start and end.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Função: prepare_grouped_sim_start_nb
```
Prepare grouped simulation start.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Função: prepare_grouped_sim_end_nb
```
Prepare grouped simulation end.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Função: prepare_grouped_sim_range_nb
```
Prepare grouped simulation start and end.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Função: prepare_ungrouped_sim_start_nb
```
Prepare ungrouped simulation start.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Função: prepare_ungrouped_sim_end_nb
```
Prepare ungrouped simulation end.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: sim_range.py
#### Função: prepare_ungrouped_sim_range_nb
```
Prepare ungrouped simulation start and end.
```

---

# Pasta: generic
## Subpasta: nb
### Arquivo: __init__.py
#### Docstring do Módulo
```
Numba-compiled functions for generic data.

Provides an arsenal of Numba-compiled functions that are used by accessors
and in many other parts of a backtesting pipeline, such as technical indicators.
These only accept NumPy arrays and other Numba-compatible types.

!!! note
    vectorbt treats matrices as first-class citizens and expects input arrays to be
    2-dim, unless function has suffix `_1d` or is meant to be input to another function.
    Data is processed along index (axis 0).

    Rolling functions with `minp=None` have `min_periods` set to the window size.

    All functions passed as argument must be Numba-compiled.

    Records must retain the order they were created in.

!!! warning
    Make sure to use `parallel=True` only if your columns are independent.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Docstring do Módulo
```
Base class for splitting.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: FixRange
```
Class that represents a fixed range.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: RelRange
```
Class that represents a relative range.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Takeable
```
Class that represents an object from which a range can be taken.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: ZeroLengthError
```
Thrown whenever a range has a length of zero.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
```
Base class for splitting.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: RelRange
#### Função: to_slice
```
Convert the relative range into a slice.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: from_splits
```
Create a `Splitter` instance from an iterable of splits.

Argument `splits` supports both absolute and relative ranges.
To transform relative ranges into the absolute format, enable `fix_ranges`.
Arguments `split_range_kwargs` are then passed to `Splitter.split_range`.

Enable `wrap_with_fixrange` to wrap any fixed range with `FixRange`. If the range
is an array, it will be wrapped regardless of this argument to avoid building a 3d array.

Pass a template via `split_check_template` to discard splits that do not fulfill certain criteria.
The current split will be available as `split`. Should return a boolean (`False` to discard).

Labels for splits and sets can be provided via `split_labels` and `set_labels` respectively.
Both arguments can be provided as templates. The split array will be available as `splits`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: from_single
```
Create a `Splitter` instance from a single split.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: from_rolling
```
Create a `Splitter` instance from a rolling range of a fixed length.

Uses `Splitter.from_splits` to prepare the splits array and labels, and to build the instance.

Args:
    index (index_like): Index.
    length (int, float, or timedelta_like): See `RelRange.length`.
    offset (int, float, or timedelta_like): See `RelRange.offset`.
    offset_anchor (str): See `RelRange.offset_anchor`.
    offset_anchor_set (int): Offset anchor set.

        Selects the set from the previous range to be used as an offset anchor.
        If None, the whole previous split is considered as a single range.
        By default, it's the first set.
    offset_space (str): See `RelRange.offset_space`.
    backwards (bool or str): Whether to roll backwards.

        If 'sorted', will roll backwards and sort the resulting splits by the start index.
    split (any): Ranges to split the range into.

        If None, will produce the entire range as a single range.
        Otherwise, will use `Splitter.split_range` to split the range into multiple ranges.
    split_range_kwargs (dict): Keyword arguments passed to `Splitter.split_range`.
    range_bounds_kwargs (dict): Keyword arguments passed to `Splitter.get_range_bounds`.
    template_context (dict): Context used to substitute templates in ranges.
    freq (any): Index frequency in case it cannot be parsed from `index`.

        If None, will be parsed using `vectorbtpro.base.accessors.BaseIDXAccessor.get_freq`.
    **kwargs: Keyword arguments passed to the constructor of `Splitter`.

Usage:
    * Divide a range into a set of non-overlapping ranges:

    ```pycon
    >>> from vectorbtpro import *

    >>> index = pd.date_range("2020", "2021", freq="D")

    >>> splitter = vbt.Splitter.from_rolling(index, 30)
    >>> splitter.plot().show()
    ```

    ![](/assets/images/api/from_rolling_1.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/from_rolling_1.dark.svg#only-dark){: .iimg loading=lazy }

    * Divide a range into ranges, each split into 1/2:

    ```pycon
    >>> splitter = vbt.Splitter.from_rolling(
    ...     index,
    ...     60,
    ...     split=1/2,
    ...     set_labels=["train", "test"]
    ... )
    >>> splitter.plot().show()
    ```

    ![](/assets/images/api/from_rolling_2.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/from_rolling_2.dark.svg#only-dark){: .iimg loading=lazy }

    * Make the ranges above non-overlapping by using the right bound of the last
    set as an offset anchor:

    ```pycon
    >>> splitter = vbt.Splitter.from_rolling(
    ...     index,
    ...     60,
    ...     offset_anchor_set=-1,
    ...     split=1/2,
    ...     set_labels=["train", "test"]
    ... )
    >>> splitter.plot().show()
    ```

    ![](/assets/images/api/from_rolling_3.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/from_rolling_3.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: from_n_rolling
```
Create a `Splitter` instance from a number of rolling ranges of the same length.

If `length` is None, splits the index evenly into `n` non-overlapping ranges
using `Splitter.from_rolling`. Otherwise, picks `n` evenly-spaced, potentially overlapping
ranges of a fixed length. For other arguments, see `Splitter.from_rolling`.

If `length` is "optimize", searches for a length to cover the most of the index.
Use `optimize_anchor_set` to provide the index of a set that should become non-overlapping.

Usage:
    * Roll 10 ranges with 100 elements, and split it into 3/4:

    ```pycon
    >>> from vectorbtpro import *

    >>> index = pd.date_range("2020", "2021", freq="D")

    >>> splitter = vbt.Splitter.from_n_rolling(
    ...     index,
    ...     10,
    ...     length=100,
    ...     split=3/4,
    ...     set_labels=["train", "test"]
    ... )
    >>> splitter.plot().show()
    ```

    ![](/assets/images/api/from_n_rolling.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/from_n_rolling.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: from_expanding
```
Create a `Splitter` instance from an expanding range.

Argument `min_length` is the minimum length of the expanding range. Provide it as
a float between 0 and 1 to make it relative to the length of the index. Argument `offset` is
an offset after the right bound of the previous range from which the next range should start.
It can also be a float relative to the index length. For other arguments, see `Splitter.from_rolling`.

Usage:
    * Roll an expanding range with a length of 10 and an offset of 10, and split it into 3/4:

    ```pycon
    >>> from vectorbtpro import *

    >>> index = pd.date_range("2020", "2021", freq="D")

    >>> splitter = vbt.Splitter.from_expanding(
    ...     index,
    ...     10,
    ...     10,
    ...     split=3/4,
    ...     set_labels=["train", "test"]
    ... )
    >>> splitter.plot().show()
    ```

    ![](/assets/images/api/from_expanding.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/from_expanding.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: from_n_expanding
```
Create a `Splitter` instance from a number of expanding ranges.

Picks `n` evenly-spaced, expanding ranges. Argument `min_length` defines the minimum
length for each range. For other arguments, see `Splitter.from_rolling`.

Usage:
    * Roll 10 expanding ranges with a minimum length of 100, while reserving 50 elements for test:

    ```pycon
    >>> from vectorbtpro import *

    >>> index = pd.date_range("2020", "2021", freq="D")

    >>> splitter = vbt.Splitter.from_n_expanding(
    ...     index,
    ...     10,
    ...     min_length=100,
    ...     split=-50,
    ...     set_labels=["train", "test"]
    ... )
    >>> splitter.plot().show()
    ```

    ![](/assets/images/api/from_n_expanding.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/from_n_expanding.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: from_ranges
```
Create a `Splitter` instance from ranges.

Uses `vectorbtpro.base.indexing.get_index_ranges` to generate start and end indices.
Passes only related keyword arguments found in `kwargs`.

Other keyword arguments will be passed to `Splitter.from_splits`. For details on
`split` and `split_range_kwargs`, see `Splitter.from_rolling`.

Usage:
    * Translate each quarter into a range:

    ```pycon
    >>> from vectorbtpro import *

    >>> index = pd.date_range("2020", "2021", freq="D")

    >>> splitter = vbt.Splitter.from_ranges(index, every="QS")
    >>> splitter.plot().show()
    ```

    ![](/assets/images/api/from_ranges_1.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/from_ranges_1.dark.svg#only-dark){: .iimg loading=lazy }

    * In addition to the above, reserve the last month for testing purposes:

    ```pycon
    >>> splitter = vbt.Splitter.from_ranges(
    ...     index,
    ...     every="QS",
    ...     split=(1.0, lambda index: index.month == index.month[-1]),
    ...     split_range_kwargs=dict(backwards=True)
    ... )
    >>> splitter.plot().show()
    ```

    ![](/assets/images/api/from_ranges_2.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/from_ranges_2.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: from_grouper
```
Create a `Splitter` instance from a grouper.

See `vectorbtpro.base.accessors.BaseIDXAccessor.get_grouper`.

Uses `Splitter.from_splits` to prepare the splits array and labels, and to build the instance.

Usage:
    * Map each month into a range:

    ```pycon
    >>> from vectorbtpro import *

    >>> index = pd.date_range("2020", "2021", freq="D")

    >>> def is_month_end(index, split):
    ...     last_range = split[-1]
    ...     return index[last_range][-1].is_month_end

    >>> splitter = vbt.Splitter.from_grouper(
    ...     index,
    ...     "M",
    ...     split_check_template=vbt.RepFunc(is_month_end)
    ... )
    >>> splitter.plot().show()
    ```

    ![](/assets/images/api/from_grouper.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/from_grouper.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: from_n_random
```
Create a `Splitter` instance from a number of random ranges.

Randomly picks the length of a range between `min_length` and `max_length` (including) using
`length_choice_func`, which receives an array of possible values and selects one. It defaults to
`numpy.random.Generator.choice`. Optional function `length_p_func` takes the same as
`length_choice_func` and must return either None or probabilities.

Randomly picks the start position of a range starting at `min_start` and ending at `max_end`
(excluding) minus the chosen length using `start_choice_func`, which receives an array of possible
values and selects one. It defaults to `numpy.random.Generator.choice`. Optional function
`start_p_func` takes the same as `start_choice_func` and must return either None or probabilities.

!!! note
    Each function must take two arguments: the iteration index and the array with possible values.

For other arguments, see `Splitter.from_rolling`.

Usage:
    * Generate 20 random ranges with a length from [40, 100], and split each into 3/4:

    ```pycon
    >>> from vectorbtpro import *

    >>> index = pd.date_range("2020", "2021", freq="D")

    >>> splitter = vbt.Splitter.from_n_random(
    ...     index,
    ...     20,
    ...     min_length=40,
    ...     max_length=100,
    ...     split=3/4,
    ...     set_labels=["train", "test"]
    ... )
    >>> splitter.plot().show()
    ```

    ![](/assets/images/api/from_n_random.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/from_n_random.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: from_sklearn
```
Create a `Splitter` instance from a scikit-learn's splitter.

The splitter must be an instance of `sklearn.model_selection.BaseCrossValidator`.

Uses `Splitter.from_splits` to prepare the splits array and labels, and to build the instance.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: from_purged
```
Create a `Splitter` instance from a purged splitter.

The splitter must be an instance of `vectorbtpro.generic.splitting.purged.BasePurgedCV`.

Uses `Splitter.from_splits` to prepare the splits array and labels, and to build the instance.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: from_purged_walkforward
```
Create a `Splitter` instance from `vectorbtpro.generic.splitting.purged.PurgedWalkForwardCV`.

Keyword arguments are passed to `Splitter.from_purged`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: from_purged_kfold
```
Create a `Splitter` instance from `vectorbtpro.generic.splitting.purged.PurgedKFoldCV`.

Keyword arguments are passed to `Splitter.from_purged`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: from_split_func
```
Create a `Splitter` instance from a custom split function.

In a while-loop, substitutes templates in `split_args` and `split_kwargs` and passes
them to `split_func`, which should return either a split (see `new_split` in `Splitter.split_range`,
also supports a single range if it's not an iterable) or None to abrupt the while-loop.
If `fix_ranges` is True, the returned split is then converted into a fixed split using
`Splitter.split_range` and the bounds of its sets are measured using `Splitter.get_range_bounds`.

Each template substitution has the following information:

* `split_idx`: Current split index, starting at 0
* `splits`: Nested list of splits appended up to this point
* `bounds`: Nested list of bounds appended up to this point
* `prev_start`: Left bound of the previous split
* `prev_end`: Right bound of the previous split
* Arguments and keyword arguments passed to `Splitter.from_split_func`

Usage:
    * Rolling window of 30 elements, 20 for train and 10 for test:

    ```pycon
    >>> from vectorbtpro import *

    >>> index = pd.date_range("2020", "2021", freq="D")

    >>> def split_func(splits, bounds, index):
    ...     if len(splits) == 0:
    ...         new_split = (slice(0, 20), slice(20, 30))
    ...     else:
    ...         # Previous split, first set, right bound
    ...         prev_end = bounds[-1][0][1]
    ...         new_split = (
    ...             slice(prev_end, prev_end + 20),
    ...             slice(prev_end + 20, prev_end + 30)
    ...         )
    ...     if new_split[-1].stop > len(index):
    ...         return None
    ...     return new_split

    >>> splitter = vbt.Splitter.from_split_func(
    ...     index,
    ...     split_func,
    ...     split_args=(
    ...         vbt.Rep("splits"),
    ...         vbt.Rep("bounds"),
    ...         vbt.Rep("index"),
    ...     ),
    ...     set_labels=["train", "test"]
    ... )
    >>> splitter.plot().show()
    ```

    ![](/assets/images/api/from_split_func.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/from_split_func.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: guess_method
```
Guess the factory method based on keyword arguments.

Returns None if cannot guess.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: split_and_take
```
Split an index and take from an object.

Argument `splitter` can be an actual `Splitter` instance, the name of a factory method
(such as "from_n_rolling"), or the factory method itself. If `splitter` is None,
the right method will be guessed based on the supplied arguments using `Splitter.guess_method`.

Keyword arguments `splitter_kwargs` are passed to the factory method. Keyword arguments
`take_kwargs` are passed to `Splitter.take`. If variable keyword arguments are provided, they
will be used as `take_kwargs` if a splitter instance has been built, otherwise, arguments will
be distributed based on the signatures of the factory method and `Splitter.take`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: split_and_apply
```
Split an index and apply a function.

Argument `splitter` can be an actual `Splitter` instance, the name of a factory method
(such as "from_n_rolling"), or the factory method itself. If `splitter` is None,
the right method will be guessed based on the supplied arguments using `Splitter.guess_method`.

Keyword arguments `splitter_kwargs` are passed to the factory method. Keyword arguments
`apply_kwargs` are passed to `Splitter.apply`. If variable keyword arguments are provided, they
will be used as `apply_kwargs` if a splitter instance has been built, otherwise, arguments will
be distributed based on the signatures of the factory method and `Splitter.apply`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: resolve_row_stack_kwargs
```
Resolve keyword arguments for initializing `Splitter` after stacking along rows.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: resolve_column_stack_kwargs
```
Resolve keyword arguments for initializing `Splitter` after stacking along columns.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: row_stack
```
Stack multiple `Splitter` instances along rows.

Uses `vectorbtpro.base.wrapping.ArrayWrapper.row_stack` to stack the wrappers.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: column_stack
```
Stack multiple `Splitter` instances along columns.

Uses `vectorbtpro.base.wrapping.ArrayWrapper.column_stack` to stack the wrappers.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: indexing_func_meta
```
Perform indexing on `Splitter` and return metadata.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: indexing_func
```
Perform indexing on `Splitter`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: index
```
Index.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: splits_arr
```
Two-dimensional, object-dtype DataFrame with splits.

First axis represents splits. Second axis represents sets. Elements represent ranges.
Range must be either a slice, a sequence of indices, a mask, or a callable that returns such.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: splits
```
`Splitter.splits_arr` as a DataFrame.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: split_labels
```
Split labels.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: set_labels
```
Set labels.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: n_splits
```
Number of splits.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: n_sets
```
Number of sets.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_split_grouper
```
Get split grouper.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_set_grouper
```
Get set grouper.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_n_splits
```
Get number of splits while considering the grouper.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_n_sets
```
Get number of sets while considering the grouper.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_split_labels
```
Get split labels while considering the grouper.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_set_labels
```
Get set labels while considering the grouper.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: to_fixed
```
Convert relative ranges into fixed ones and return a new `Splitter` instance.

Keyword arguments `split_range_kwargs` are passed to `Splitter.split_range`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: to_grouped
```
Merge all ranges within the same group and return a new `Splitter` instance.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: is_range_relative
```
Return whether a range is relative.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_ready_range
```
Get a range that can be directly used in array indexing.

Such a range is either an integer or datetime-like slice (right bound is always exclusive!),
a one-dimensional NumPy array with integer indices or datetime-like objects,
or a one-dimensional NumPy mask of the same length as the index.

Argument `range_format` accepts the following options:

* 'any': Return any format
* 'indices': Return indices
* 'mask': Return mask of the same length as index
* 'slice': Return slice
* 'slice_or_indices': If slice fails, return indices
* 'slice_or_mask': If slice fails, return mask
* 'slice_or_any': If slice fails, return any format
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: split_range
```
Split a fixed range into a split of multiple fixed ranges.

Range must be either a template, a callable, a tuple (start and stop), a slice, a sequence
of indices, or a mask. This range will then be re-mapped into the index.

Each sub-range in `new_split` can be either a fixed or relative range, that is, an instance
of `RelRange` or a number that will be used as a length to create an `RelRange`.
Each sub-range will then be re-mapped into the main range. Argument `new_split` can also
be provided as an integer or a float indicating the length; in such a case the second part
(or the first one depending on `backwards`) will stretch. If `new_split` is a string,
the following options are supported:

* 'by_gap': Split `range_` by gap using `vectorbtpro.generic.splitting.nb.split_range_by_gap_nb`.

New ranges are returned relative to the index and in the same order as passed.

For `range_format`, see `Splitter.get_ready_range`. Enable `wrap_with_template` to wrap the
resulting ranges with a template of the type `vectorbtpro.utils.template.Rep`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: merge_split
```
Merge a split of multiple fixed ranges into a fixed range.

Creates one mask and sets True for each range. If all input ranges are masks,
returns that mask. If all input ranges are slices, returns a slice if possible.
Otherwise, returns integer indices.

For `range_format`, see `Splitter.get_ready_range`. Enable `wrap_with_template` to wrap the
resulting range with a template of the type `vectorbtpro.utils.template.Rep`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: select_indices
```
Get indices corresponding to selected splits and sets.

Arguments `split` and `set_` can be either integers and labels. Also, multiple
values are accepted; in such a case, the corresponding ranges are merged.
If split/set labels are of an integer data type, treats the provided values as labels
rather than indices, unless the split/set index is not of an integer data type or the values
are wrapped with `vectorbtpro.utils.selection.PosSel`.

If `split_group_by` and/or `set_group_by` are provided, their groupers get
created using `vectorbtpro.base.accessors.BaseIDXAccessor.get_grouper` and
arguments `split` and `set_` become relative to the groups.

If `split`/`set_` is not provided, selects all indices.

Returns four arrays: split group indices, set group indices, split indices, and set indices.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: select_range
```
Select a range.

Passes `**select_indices_kwargs` to `Splitter.select_indices` to get the indices for selected
splits and sets. If multiple ranges correspond to those indices, merges them using
`Splitter.merge_split`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: remap_range
```
Remap a range to a target index.

If `index` and `target_index` are the same, returns the range. Otherwise,
uses `vectorbtpro.base.resampling.base.Resampler.resample_source_mask` to resample
the range into the target index. In such a case, `freq` and `target_freq` must be provided.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_obj_index
```
Get index from an object.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_ready_obj_range
```
Get a range that is ready to be mapped into an array-like object.

If the object is Pandas-like and `obj_index` is not None, searches for an index in the object
using `Splitter.get_obj_index`. Once found, uses `Splitter.remap_range` to get the range
that maps to the object index. Finally, uses `Splitter.get_ready_range` to convert the range
into the one that can be used directly in indexing.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: take_range
```
Take a ready range from an array-like object.

Set `point_wise` to True to select one range point at a time and return a tuple.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: take_range_from_takeable
```
Take a range from a takeable object.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: take
```
Take all ranges from an array-like object and optionally column-stack them.

Uses `Splitter.select_indices` to get the indices for selected splits and sets.
Arguments `split_group_by` and `set_group_by` can be used to group splits and sets respectively.
Ranges belonging to the same split and set group will be merged.

For each index pair, resolves the source range using `Splitter.select_range` and
`Splitter.get_ready_range`. Then, remaps this range into the object index using
`Splitter.get_ready_obj_range` and takes the slice from the object using `Splitter.take_range`.
If the object is a custom template, substitutes its instead of calling `Splitter.take_range`.
Finally, uses `vectorbtpro.base.merging.column_stack_merge` (`stack_axis=1`) or
`vectorbtpro.base.merging.row_stack_merge` (`stack_axis=0`) with `stack_kwargs` to merge the taken slices.

If `attach_bounds` is enabled, measures the bounds of each range and makes it an additional
level in the final index hierarchy. The argument supports the following options:

* True, 'index', 'source', or 'source_index': Attach source (index) bounds
* 'target' or 'target_index': Attach target (index) bounds
* False: Do not attach

Argument `into` supports the following options:

* None: Series of range slices
* 'stacked': Stack all slices into a single object
* 'stacked_by_split': Stack set slices in each split and return a Series of objects
* 'stacked_by_set': Stack split slices in each set and return a Series of objects
* 'split_major_meta': Generator with ranges processed lazily in split-major order.
    Returns meta with indices and labels, and the generator.
* 'set_major_meta': Generator with ranges processed lazily in set-major order.
    Returns meta with indices and labels, and the generator.

Prepend any stacked option with "from_start_" (also "reset_") or "from_end_" to reset the index
from start and from end respectively.

Usage:
    * Roll a window and stack it along columns by keeping the index:

    ```pycon
    >>> from vectorbtpro import *

    >>> data = vbt.YFData.pull(
    ...     "BTC-USD",
    ...     start="2020-01-01 UTC",
    ...     end="2021-01-01 UTC"
    ... )
    >>> splitter = vbt.Splitter.from_n_rolling(
    ...     data.wrapper.index,
    ...     3,
    ...     length=5
    ... )
    >>> splitter.take(data.close, into="stacked")
    split                                0            1             2
    Date
    2020-01-01 00:00:00+00:00  7200.174316          NaN           NaN
    2020-01-02 00:00:00+00:00  6985.470215          NaN           NaN
    2020-01-03 00:00:00+00:00  7344.884277          NaN           NaN
    2020-01-04 00:00:00+00:00  7410.656738          NaN           NaN
    2020-01-05 00:00:00+00:00  7411.317383          NaN           NaN
    2020-06-29 00:00:00+00:00          NaN  9190.854492           NaN
    2020-06-30 00:00:00+00:00          NaN  9137.993164           NaN
    2020-07-01 00:00:00+00:00          NaN  9228.325195           NaN
    2020-07-02 00:00:00+00:00          NaN  9123.410156           NaN
    2020-07-03 00:00:00+00:00          NaN  9087.303711           NaN
    2020-12-27 00:00:00+00:00          NaN          NaN  26272.294922
    2020-12-28 00:00:00+00:00          NaN          NaN  27084.808594
    2020-12-29 00:00:00+00:00          NaN          NaN  27362.437500
    2020-12-30 00:00:00+00:00          NaN          NaN  28840.953125
    2020-12-31 00:00:00+00:00          NaN          NaN  29001.720703
    ```

    * Disgard the index and attach index bounds to the column hierarchy:

    ```pycon
    >>> splitter.take(
    ...     data.close,
    ...     into="reset_stacked",
    ...     attach_bounds="index"
    ... )
    split                         0                         1  \
    start 2020-01-01 00:00:00+00:00 2020-06-29 00:00:00+00:00
    end   2020-01-06 00:00:00+00:00 2020-07-04 00:00:00+00:00
    0                   7200.174316               9190.854492
    1                   6985.470215               9137.993164
    2                   7344.884277               9228.325195
    3                   7410.656738               9123.410156
    4                   7411.317383               9087.303711

    split                         2
    start 2020-12-27 00:00:00+00:00
    end   2021-01-01 00:00:00+00:00
    0                  26272.294922
    1                  27084.808594
    2                  27362.437500
    3                  28840.953125
    4                  29001.720703
    ```
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: parse_and_inject_takeables
```
Parse `Takeable` instances from function annotations and inject them into flattened annotated arguments.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: apply
```
Apply a function on each range.

Uses `Splitter.select_indices` to get the indices for selected splits and sets.
Arguments `split_group_by` and `set_group_by` can be used to group splits and sets respectively.
Ranges belonging to the same split and set group will be merged.

For each index pair, in a lazily manner, resolves the source range using `Splitter.select_range`
and `Splitter.get_ready_range`. Then, takes each argument from `args` and `kwargs`
wrapped with `Takeable`, remaps the range into each object's index using `Splitter.get_ready_obj_range`,
and takes the slice from that object using `Splitter.take_range`. The original object will
be substituted by this slice. At the end, substitutes any templates in the prepared
`args` and `kwargs` and saves the function and arguments for execution.

For substitution, the following information is available:

* `split/set_group_indices`: Indices corresponding to the selected row/column groups
* `split/set_indices`: Indices corresponding to the selected rows/columns
* `n_splits/sets`: Number of the selected rows/columns
* `split/set_labels`: Labels corresponding to the selected row/column groups
* `split/set_idx`: Index of the selected row/column
* `split/set_label`: Label of the selected row/column
* `range_`: Selected range ready for indexing (see `Splitter.get_ready_range`)
* `range_meta`: Various information on the selected range
* `obj_range_meta`: Various information on the range taken from each takeable argument.
    Positional arguments are denoted by position, keyword arguments are denoted by keys.
* `args`: Positional arguments with ranges already selected
* `kwargs`: Keyword arguments with ranges already selected
* `bounds`: A tuple of either integer or index bounds. Can be source or target depending on `attach_bounds`.
* `template_context`: Passed template context

Since each range is processed lazily (that is, upon request), there are multiple iteration
modes controlled by the argument `iteration`:

* 'split_major': Flatten all ranges in split-major order and iterate over them
* 'set_major': Flatten all ranges in set-major order and iterate over them
* 'split_wise': Iterate over splits, while ranges in each split are processed sequentially
* 'set_wise': Iterate over sets, while ranges in each set are processed sequentially

The execution is done using `vectorbtpro.utils.execution.execute` with `execute_kwargs`.
Once all results have been obtained, attempts to merge them using `merge_func` with `merge_kwargs`
(all templates in it will be substituted as well), which can also be a string or a tuple of
strings resolved using `vectorbtpro.base.merging.resolve_merge_func`. If `wrap_results` is enabled,
packs the results into a Pandas object. If `apply_func` returns something complex, the resulting
Pandas object will be of object data type. If `apply_func` returns a tuple (detected by the first
returned result), a Pandas object is built for each element of that tuple.

If `merge_all` is True, will merge all results in a flattened manner irrespective of the
iteration mode. Otherwise, will merge by split/set.

If `vectorbtpro.utils.execution.NoResult` is returned, will skip the current iteration and
remove it from the final index.

Usage:
    * Get the return of each data range:

    ```pycon
    >>> from vectorbtpro import *

    >>> data = vbt.YFData.pull(
    ...     "BTC-USD",
    ...     start="2020-01-01 UTC",
    ...     end="2021-01-01 UTC"
    ... )
    >>> splitter = vbt.Splitter.from_n_rolling(data.wrapper.index, 5)

    >>> def apply_func(data):
    ...     return data.close.iloc[-1] - data.close.iloc[0]

    >>> splitter.apply(apply_func, vbt.Takeable(data))
    split
    0    -1636.467285
    1     3706.568359
    2     2944.720703
    3     -118.113281
    4    17098.916016
    dtype: float64
    ```

    * The same but by indexing manually:

    ```pycon
    >>> def apply_func(range_, data):
    ...     data = data.iloc[range_]
    ...     return data.close.iloc[-1] - data.close.iloc[0]

    >>> splitter.apply(apply_func, vbt.Rep("range_"), data)
    split
    0    -1636.467285
    1     3706.568359
    2     2944.720703
    3     -118.113281
    4    17098.916016
    dtype: float64
    ```

    * Divide into two windows, each consisting of 50% train and 50% test, compute SMA for
    each range, and row-stack the outputs of each set upon merging:

    ```pycon
    >>> splitter = vbt.Splitter.from_n_rolling(data.wrapper.index, 2, split=0.5)

    >>> def apply_func(data):
    ...     return data.run("SMA", 10).real

    >>> splitter.apply(
    ...     apply_func,
    ...     vbt.Takeable(data),
    ...     merge_func="row_stack"
    ... ).unstack("set").vbt.drop_levels("split", axis=0).vbt.plot().show()
    ```

    ![](/assets/images/api/Splitter_apply.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/Splitter_apply.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: shuffle_splits
```
Shuffle splits.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: break_up_splits
```
Split each split into multiple splits.

If there are multiple sets, make sure to merge them into one beforehand.

Arguments `new_split` and `**split_range_kwargs` are passed to `Splitter.split_range`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: split_set
```
Split a set (column) into multiple sets (columns).

Arguments `new_split` and `**split_range_kwargs` are passed to `Splitter.split_range`.

Column must be provided if there are two or more sets.

Use `new_set_labels` to specify the labels of the new sets; it must have the same length
as there are new ranges in the new split. To provide final labels, define `columns` in
`wrapper_kwargs`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: merge_sets
```
Merge multiple sets (columns) into a set (column).

Arguments `**merge_split_kwargs` are passed to `Splitter.merge_split`.

If columns are not provided, merges all columns. If provided and `insert_at_last` is True,
a new column is inserted at the position of the last column.

Use `new_set_label` to specify the label of the new set. To provide final labels,
define `columns` in `wrapper_kwargs`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: map_bounds_to_index
```
Map bounds to index.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_range_bounds
```
Get the left (inclusive) and right (exclusive) bound of a range.

!!! note
    Even when mapped to the index, the right bound is always exclusive.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_bounds_arr
```
Three-dimensional integer array with bounds.

First axis represents splits. Second axis represents sets. Third axis represents bounds.

Each range is getting selected using `Splitter.select_range` and then measured using
`Splitter.get_range_bounds`. Keyword arguments `**kwargs` are passed to
`Splitter.get_range_bounds`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: bounds_arr
```
`Splitter.get_bounds_arr` with default arguments.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_bounds
```
Boolean Series/DataFrame where index are bounds and columns are splits stacked together.

Keyword arguments `**kwargs` are passed to `Splitter.get_bounds_arr`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: bounds
```
`Splitter.get_bounds` with default arguments.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: index_bounds
```
`Splitter.get_bounds` with `index_bounds=True`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_duration
```
Get duration.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: duration
```
`Splitter.get_duration` with default arguments.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: index_duration
```
`Splitter.get_duration` with `index_bounds=True`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_range_mask
```
Get the mask of a range.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_iter_split_mask_arrs
```
Generator of two-dimensional boolean arrays, one per split.

First axis represents sets. Second axis represents index.

Keyword arguments `**kwargs` are passed to `Splitter.get_range_mask`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: iter_split_mask_arrs
```
`Splitter.get_iter_split_mask_arrs` with default arguments.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_iter_set_mask_arrs
```
Generator of two-dimensional boolean arrays, one per set.

First axis represents splits. Second axis represents index.

Keyword arguments `**kwargs` are passed to `Splitter.get_range_mask`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: iter_set_mask_arrs
```
`Splitter.get_iter_set_mask_arrs` with default arguments.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_iter_split_masks
```
Generator of boolean DataFrames, one per split.

Keyword arguments `**kwargs` are passed to `Splitter.get_iter_split_mask_arrs`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: iter_split_masks
```
`Splitter.get_iter_split_masks` with default arguments.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_iter_set_masks
```
Generator of boolean DataFrames, one per set.

Keyword arguments `**kwargs` are passed to `Splitter.get_iter_set_mask_arrs`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: iter_set_masks
```
`Splitter.get_iter_set_masks` with default arguments.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_mask_arr
```
Three-dimensional boolean array with splits.

First axis represents splits. Second axis represents sets. Third axis represents index.

Keyword arguments `**kwargs` are passed to `Splitter.get_iter_split_mask_arrs`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: mask_arr
```
`Splitter.get_mask_arr` with default arguments.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_mask
```
Boolean Series/DataFrame where index is `Splitter.index` and columns are splits stacked together.

Keyword arguments `**kwargs` are passed to `Splitter.get_mask_arr`.

!!! warning
    Boolean arrays for a big number of splits may take a considerable amount of memory.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: mask
```
`Splitter.get_mask` with default arguments.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_split_coverage
```
Get the coverage of each split mask.

If `overlapping` is True, returns the number of overlapping True values between sets in each split.
If `normalize` is True, returns the number of True values in each split relative to the
length of the index. If `normalize` and `relative` are True, returns the number of True values
in each split relative to the total number of True values across all splits.

Keyword arguments `**kwargs` are passed to `Splitter.get_mask_arr`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: split_coverage
```
`Splitter.get_split_coverage` with default arguments.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_set_coverage
```
Get the coverage of each set mask.

If `overlapping` is True, returns the number of overlapping True values between splits in each set.
If `normalize` is True, returns the number of True values in each set relative to the
length of the index. If `normalize` and `relative` are True, returns the number of True values
in each set relative to the total number of True values across all sets.

Keyword arguments `**kwargs` are passed to `Splitter.get_mask_arr`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: set_coverage
```
`Splitter.get_set_coverage` with default arguments.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_range_coverage
```
Get the coverage of each range mask.

If `normalize` is True, returns the number of True values in each range relative to the
length of the index. If `normalize` and `relative` are True, returns the number of True values
in each range relative to the total number of True values in its split.

Keyword arguments `**kwargs` are passed to `Splitter.get_mask_arr`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: range_coverage
```
`Splitter.get_range_coverage` with default arguments.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_coverage
```
Get the coverage of the entire mask.

If `overlapping` is True, returns the number of overlapping True values.
If `normalize` is True, returns the number of True values relative to the length of the index.
If `overlapping` and `normalize` are True, returns the number of overlapping True values relative
to the total number of True values.

Keyword arguments `**kwargs` are passed to `Splitter.get_mask_arr`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: coverage
```
`Splitter.get_coverage` with default arguments.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: get_overlap_matrix
```
Get the overlap between each pair of ranges.

The argument `by` can be one of 'split', 'set', and 'range'.

If `normalize` is True, returns the number of True values in each overlap relative
to the total number of True values in both ranges.

Keyword arguments `**kwargs` are passed to `Splitter.get_mask_arr`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: split_overlap_matrix
```
`Splitter.get_overlap_matrix` with `by="split"`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: set_overlap_matrix
```
`Splitter.get_overlap_matrix` with `by="set"`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: range_overlap_matrix
```
`Splitter.get_overlap_matrix` with `by="range"`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: stats_defaults
```
Defaults for `Splitter.stats`.

Merges `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats_defaults` and
`stats` from `vectorbtpro._settings.splitter`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: plot
```
Plot splits as rows and sets as colors.

Args:
    split_group_by (any): Split groups. See `vectorbtpro.base.accessors.BaseIDXAccessor.get_grouper`.
    set_group_by (any): Set groups. See `vectorbtpro.base.accessors.BaseIDXAccessor.get_grouper`.
    mask_kwargs (dict): Keyword arguments passed to `Splitter.get_iter_set_masks`.
    trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Heatmap`.

        Can be a sequence, one per set.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    * Plot a scikit-learn splitter:

    ```pycon
    >>> from vectorbtpro import *
    >>> from sklearn.model_selection import TimeSeriesSplit

    >>> index = pd.date_range("2020", "2021", freq="D")
    >>> splitter = vbt.Splitter.from_sklearn(index, TimeSeriesSplit())
    >>> splitter.plot().show()
    ```

    ![](/assets/images/api/Splitter.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/Splitter.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: plot_coverage
```
Plot index as rows and sets as lines.

Args:
    stacked (bool): Whether to plot as an area plot.
    split_group_by (any): Split groups. See `vectorbtpro.base.accessors.BaseIDXAccessor.get_grouper`.
    set_group_by (any): Set groups. See `vectorbtpro.base.accessors.BaseIDXAccessor.get_grouper`.
    mask_kwargs (dict): Keyword arguments passed to `Splitter.get_iter_set_masks`.
    trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter`.

        Can be a sequence, one per set.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    * Area plot:

    ```pycon
    >>> from vectorbtpro import *
    >>> from sklearn.model_selection import TimeSeriesSplit

    >>> index = pd.date_range("2020", "2021", freq="D")
    >>> splitter = vbt.Splitter.from_sklearn(index, TimeSeriesSplit())
    >>> splitter.plot_coverage().show()
    ```

    ![](/assets/images/api/Splitter_coverage_area.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/Splitter_coverage_area.dark.svg#only-dark){: .iimg loading=lazy }

    * Line plot:

    ```pycon
    >>> splitter.plot_coverage(stacked=False).show()
    ```

    ![](/assets/images/api/Splitter_coverage_line.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/Splitter_coverage_line.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: base.py
#### Classe: Splitter
#### Função: plots_defaults
```
Defaults for `Splitter.plots`.

Merges `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots_defaults` and
`plots` from `vectorbtpro._settings.splitter`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: decorators.py
#### Docstring do Módulo
```
Decorators for splitting.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: decorators.py
#### Função: split
```
Decorator that splits the inputs of a function.

Does the following:

1. Resolves the splitter of the type `vectorbtpro.generic.splitting.base.Splitter` using
the argument `splitter`. It can be either an already provided splitter instance, the
name of a factory method (such as "from_n_rolling"), or the factory method itself.
If `splitter` is None, the right method will be guessed based on the supplied arguments
using `vectorbtpro.generic.splitting.base.Splitter.guess_method`. To construct a splitter,
it will pass `index` and `**splitter_kwargs`. Index is getting resolved either using an already
provided `index`, by parsing the argument under a name/position provided in `index_from`,
or by parsing the first argument from `takeable_args` (in this order).
2. Wraps arguments in `takeable_args` with `vectorbtpro.generic.splitting.base.Takeable`
3. Runs `vectorbtpro.generic.splitting.base.Splitter.apply` with arguments passed
to the function as `args` and `kwargs`, but also `apply_kwargs` (the ones passed to
the decorator)

Keyword arguments `splitter_kwargs` are passed to the factory method. Keyword arguments
`apply_kwargs` are passed to `vectorbtpro.generic.splitting.base.Splitter.apply`. If variable
keyword arguments are provided, they will be used as `splitter_kwargs` if `apply_kwargs` is already set,
and vice versa. If `splitter_kwargs` and `apply_kwargs` aren't set, they will be used as `splitter_kwargs`
if a splitter instance hasn't been built yet, otherwise as `apply_kwargs`. If both arguments are set,
will raise an error.

Usage:
    * Split a Series and return its sum:

    ```pycon
    >>> from vectorbtpro import *

    >>> @vbt.split(
    ...     splitter="from_n_rolling",
    ...     splitter_kwargs=dict(n=2),
    ...     takeable_args=["sr"]
    ... )
    ... def f(sr):
    ...     return sr.sum()

    >>> index = pd.date_range("2020-01-01", "2020-01-06")
    >>> sr = pd.Series(np.arange(len(index)), index=index)
    >>> f(sr)
    split
    0     3
    1    12
    dtype: int64
    ```

    * Perform a split manually:

    ```pycon
    >>> @vbt.split(
    ...     splitter="from_n_rolling",
    ...     splitter_kwargs=dict(n=2),
    ...     takeable_args=["index"]
    ... )
    ... def f(index, sr):
    ...     return sr[index].sum()

    >>> f(index, sr)
    split
    0     3
    1    12
    dtype: int64
    ```

    * Construct splitter and mark arguments as "takeable" manually:

    ```pycon
    >>> splitter = vbt.Splitter.from_n_rolling(index, n=2)
    >>> @vbt.split(splitter=splitter)
    ... def f(sr):
    ...     return sr.sum()

    >>> f(vbt.Takeable(sr))
    split
    0     3
    1    12
    dtype: int64
    ```

    * Split multiple timeframes using a custom index:

    ```pycon
    >>> @vbt.split(
    ...     splitter="from_n_rolling",
    ...     splitter_kwargs=dict(n=2),
    ...     index=index,
    ...     takeable_args=["h12_sr", "d2_sr"]
    ... )
    ... def f(h12_sr, d2_sr):
    ...     return h12_sr.sum() + d2_sr.sum()

    >>> h12_index = pd.date_range("2020-01-01", "2020-01-06", freq="12H")
    >>> d2_index = pd.date_range("2020-01-01", "2020-01-06", freq="2D")
    >>> h12_sr = pd.Series(np.arange(len(h12_index)), index=h12_index)
    >>> d2_sr = pd.Series(np.arange(len(d2_index)), index=d2_index)
    >>> f(h12_sr, d2_sr)
    split
    0    15
    1    42
    dtype: int64
    ```
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: decorators.py
#### Função: cv_split
```
Decorator that combines `split` and `vectorbtpro.utils.params.parameterized` for cross-validation.

Creates a new apply function that is going to be decorated with `split` and thus applied
at each single range using `vectorbtpro.generic.splitting.base.Splitter.apply`. Inside
this apply function, there is a test whether the current range belongs to the first (training) set.
If yes, parameterizes the underlying function and runs it on the entire grid of parameters.
The returned results are then stored in a global list. These results are then read by the other
(testing) sets in the same split. If `selection` is a template, it can evaluate the grid results
(available as `grid_results`) and return the best parameter combination. This parameter combination
is then executed by each set (including training).

Argument `selection` also accepts "min" for `np.argmin` and "max" for `np.argmax`.

Keyword arguments `parameterized_kwargs` will be passed to `vectorbtpro.utils.params.parameterized`
and will have their templates substituted with a context that will also include the split-related context
(including `split_idx`, `set_idx`, etc., see `vectorbtpro.generic.splitting.base.Splitter.apply`).

If `return_grid` is True or 'first', returns both the grid and the selection. If `return_grid`
is 'all', executes the grid on each set and returns along with the selection.
Otherwise, returns only the selection.

If `vectorbtpro.utils.execution.NoResultsException` is raised or `skip_errored` is True and
any exception is raised, will skip the current iteration and remove it from the final index.

Usage:
    * Permutate a series and pick the first value. Make the seed parameterizable.
    Cross-validate based on the highest picked value:

    ```pycon
    >>> from vectorbtpro import *

    >>> @vbt.cv_split(
    ...     splitter="from_n_rolling",
    ...     splitter_kwargs=dict(n=3, split=0.5),
    ...     takeable_args=["sr"],
    ...     merge_func="concat",
    ... )
    ... def f(sr, seed):
    ...     np.random.seed(seed)
    ...     return np.random.permutation(sr)[0]

    >>> index = pd.date_range("2020-01-01", "2020-02-01")
    >>> np.random.seed(0)
    >>> sr = pd.Series(np.random.permutation(np.arange(len(index))), index=index)
    >>> f(sr, vbt.Param([41, 42, 43]))
    split  set    seed
    0      set_0  41      22
           set_1  41      28
    1      set_0  43       8
           set_1  43      31
    2      set_0  43      19
           set_1  43       0
    dtype: int64
    ```

    * Extend the example above to also return the grid results of each set:

    ```pycon
    >>> f(sr, vbt.Param([41, 42, 43]), _return_grid="all")
    (split  set    seed
     0      set_0  41      22
                   42      22
                   43       2
            set_1  41      28
                   42      28
                   43      20
     1      set_0  41       5
                   42       5
                   43       8
            set_1  41      23
                   42      23
                   43      31
     2      set_0  41      18
                   42      18
                   43      19
            set_1  41      27
                   42      27
                   43       0
     dtype: int64,
     split  set    seed
     0      set_0  41      22
            set_1  41      28
     1      set_0  43       8
            set_1  43      31
     2      set_0  43      19
            set_1  43       0
     dtype: int64)
    ```
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: nb.py
#### Docstring do Módulo
```
Numba-compiled functions for splitting.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: nb.py
#### Função: split_overlap_matrix_nb
```
Compute the overlap matrix for splits.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: nb.py
#### Função: norm_split_overlap_matrix_nb
```
Compute the normalized overlap matrix for splits.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: nb.py
#### Função: set_overlap_matrix_nb
```
Compute the overlap matrix for sets.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: nb.py
#### Função: norm_set_overlap_matrix_nb
```
Compute the normalized overlap matrix for sets.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: nb.py
#### Função: range_overlap_matrix_nb
```
Compute the overlap matrix for ranges.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: nb.py
#### Função: norm_range_overlap_matrix_nb
```
Compute the normalized overlap matrix for ranges.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: nb.py
#### Função: split_range_by_gap_nb
```
Split a range with gaps into start and end indices.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Docstring do Módulo
```
Classes for purged cross-validation in time series.

As described in Advances in Financial Machine Learning, Marcos Lopez de Prado, 2018.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: BasePurgedCV
```
Abstract class for purged time series cross-validation.

Time series cross-validation requires each sample has a prediction time, at which the features are used to
predict the response, and an evaluation time, at which the response is known and the error can be
computed. Importantly, it means that unlike in standard sklearn cross-validation, the samples X, response y,
`pred_times` and `eval_times` must all be pandas DataFrames/Series having the same index. It is also
assumed that the samples are time-ordered with respect to the prediction time.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: PurgedWalkForwardCV
```
Purged walk-forward cross-validation.

The samples are decomposed into `n_folds` folds containing equal numbers of samples, without
shuffling. In each cross validation round, `n_test_folds` contiguous folds are used as the
test set, while the train set consists in between `min_train_folds` and `max_train_folds`
immediately preceding folds.

Each sample should be tagged with a prediction time and an evaluation time.
The split is such that the intervals [`pred_times`, `eval_times`] associated to samples in
the train and test set do not overlap. (The overlapping samples are dropped.)

With `split_by_time=True` in the `PurgedWalkForwardCV.split` method, it is also possible to
split the samples in folds spanning equal time intervals (using the prediction time as a time tag),
instead of folds containing equal numbers of samples.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: PurgedKFoldCV
```
Purged and embargoed combinatorial cross-validation.

The samples are decomposed into `n_folds` folds containing equal numbers of samples, without shuffling.
In each cross validation round, `n_test_folds` folds are used as the test set, while the other folds
are used as the train set. There are as many rounds as `n_test_folds` folds among the `n_folds` folds.

Each sample should be tagged with a prediction time and an evaluation time. The split is such
that the intervals [`pred_times`, `eval_times`] associated to samples in the train and test set
do not overlap. (The overlapping samples are dropped.) In addition, an "embargo" period is defined,
giving the minimal time between an evaluation time in the test set and a prediction time in the
training set. This is to avoid, in the presence of temporal correlation, a contamination of the
test set by the train set.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: BasePurgedCV
#### Função: n_folds
```
Number of folds.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: BasePurgedCV
#### Função: purge_td
```
Purge period.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: BasePurgedCV
#### Função: pred_times
```
Times at which predictions are made.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: BasePurgedCV
#### Função: eval_times
```
Times at which the response becomes available and the error can be computed.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: BasePurgedCV
#### Função: indices
```
Indices.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: BasePurgedCV
#### Função: purge
```
Purge part of the train set.

Given a left boundary index `test_fold_start` of the test set and a right boundary index
`test_fold_end`, this method removes from the train set all the samples whose evaluation time
is posterior to the prediction time of the first test sample after the boundary.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: BasePurgedCV
#### Função: split
```
Yield the indices of the train and test sets.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: PurgedWalkForwardCV
#### Função: n_test_folds
```
Number of folds used in the test set.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: PurgedWalkForwardCV
#### Função: min_train_folds
```
Minimal number of folds to be used in the train set.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: PurgedWalkForwardCV
#### Função: max_train_folds
```
Maximal number of folds to be used in the train set.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: PurgedWalkForwardCV
#### Função: split_by_time
```
Whether the folds span identical time intervals. Otherwise, the folds contain
an (approximately) equal number of samples.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: PurgedWalkForwardCV
#### Função: fold_bounds
```
Fold boundaries.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: PurgedWalkForwardCV
#### Função: compute_fold_bounds
```
Compute a list containing the fold (left) boundaries.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: PurgedWalkForwardCV
#### Função: compute_train_set
```
Compute the position indices of the samples in the train set.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: PurgedWalkForwardCV
#### Função: compute_test_set
```
Compute the position indices of the samples in the test set.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: PurgedKFoldCV
#### Função: n_test_folds
```
Number of folds used in the test set.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: PurgedKFoldCV
#### Função: embargo_td
```
Embargo period.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: PurgedKFoldCV
#### Função: embargo
```
Apply the embargo procedure to part of the train set.

This amounts to dropping the train set samples whose prediction time occurs within
`PurgedKFoldCV.embargo_td` of the test set sample evaluation times. This method applies
the embargo only to the part of the training set immediately following the end of the test
set determined by `test_fold_end`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: PurgedKFoldCV
#### Função: compute_train_set
```
Compute the position indices of the samples in the train set.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: purged.py
#### Classe: PurgedKFoldCV
#### Função: compute_test_set
```
Compute the position indices of the samples in the test set.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: sklearn_.py
#### Docstring do Módulo
```
Scikit-learn compatible class for splitting.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: sklearn_.py
#### Classe: SplitterCV
```
Scikit-learn compatible cross-validator based on `vectorbtpro.generic.splitting.base.Splitter`.

Usage:
    * Replicate `TimeSeriesSplit` from scikit-learn:

    ```pycon
    >>> from vectorbtpro import *

    >>> X = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])
    >>> y = np.array([1, 2, 3, 4])

    >>> cv = vbt.SplitterCV(
    ...     "from_expanding",
    ...     min_length=2,
    ...     offset=1,
    ...     split=-1
    ... )
    >>> for i, (train_indices, test_indices) in enumerate(cv.split(X)):
    ...     print("Split %d:" % i)
    ...     X_train, X_test = X[train_indices], X[test_indices]
    ...     print("  X:", X_train.tolist(), X_test.tolist())
    ...     y_train, y_test = y[train_indices], y[test_indices]
    ...     print("  y:", y_train.tolist(), y_test.tolist())
    Split 0:
      X: [[1, 2]] [[3, 4]]
      y: [1] [2]
    Split 1:
      X: [[1, 2], [3, 4]] [[5, 6]]
      y: [1, 2] [3]
    Split 2:
      X: [[1, 2], [3, 4], [5, 6]] [[7, 8]]
      y: [1, 2, 3] [4]
    ```
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: sklearn_.py
#### Classe: SplitterCV
#### Função: splitter
```
Splitter.

Either as a `vectorbtpro.generic.splitting.base.Splitter` instance, a factory method name,
or the factory method itself.

If None, will be determined automatically based on `SplitterCV.splitter_kwargs`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: sklearn_.py
#### Classe: SplitterCV
#### Função: splitter_cls
```
Splitter class.

Defaults to `vectorbtpro.generic.splitting.base.Splitter`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: sklearn_.py
#### Classe: SplitterCV
#### Função: splitter_kwargs
```
Keyword arguments passed to the factory method.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: sklearn_.py
#### Classe: SplitterCV
#### Função: split_group_by
```
Split groups. See `vectorbtpro.base.accessors.BaseIDXAccessor.get_grouper`.

Not passed to the factory method.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: sklearn_.py
#### Classe: SplitterCV
#### Função: set_group_by
```
Set groups. See `vectorbtpro.base.accessors.BaseIDXAccessor.get_grouper`.

Not passed to the factory method.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: sklearn_.py
#### Classe: SplitterCV
#### Função: template_context
```
Mapping used to substitute templates in ranges.

Passed to the factory method.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: sklearn_.py
#### Classe: SplitterCV
#### Função: get_splitter
```
Get splitter of type `vectorbtpro.generic.splitting.base.Splitter`.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: sklearn_.py
#### Classe: SplitterCV
#### Função: _iter_masks
```
Generates boolean masks corresponding to train and test sets.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: sklearn_.py
#### Classe: SplitterCV
#### Função: _iter_train_masks
```
Generates boolean masks corresponding to train sets.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: sklearn_.py
#### Classe: SplitterCV
#### Função: _iter_test_masks
```
Generates boolean masks corresponding to test sets.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: sklearn_.py
#### Classe: SplitterCV
#### Função: _iter_indices
```
Generates integer indices corresponding to train and test sets.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: sklearn_.py
#### Classe: SplitterCV
#### Função: _iter_train_indices
```
Generates integer indices corresponding to train sets.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: sklearn_.py
#### Classe: SplitterCV
#### Função: _iter_test_indices
```
Generates integer indices corresponding to test sets.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: sklearn_.py
#### Classe: SplitterCV
#### Função: get_n_splits
```
Returns the number of splitting iterations in the cross-validator.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: sklearn_.py
#### Classe: SplitterCV
#### Função: split
```
Generate indices to split data into training and test set.
```

---

# Pasta: generic
## Subpasta: splitting
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules for splitting.
```

---

# Pasta: indicators
### Arquivo: configs.py
#### Docstring do Módulo
```
Configs for custom indicators.
```

---

# Pasta: indicators
### Arquivo: enums.py
#### Docstring do Módulo
```
Named tuples and enumerated types for indicators.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Docstring do Módulo
```
Functions and config for evaluating indicator expressions.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: delay
```
Value of `x` `d` days ago.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: delta
```
Today’s value of `x` minus the value of `x` `d` days ago.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: cs_rescale
```
Rescale `x` such that `sum(abs(x)) = 1`.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: cs_rank
```
Rank cross-sectionally.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: cs_demean
```
Demean `x` against groups `g` cross-sectionally.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: ts_min
```
Return the rolling min.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: ts_max
```
Return the rolling max.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: ts_argmin
```
Return the rolling argmin.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: ts_argmax
```
Return the rolling argmax.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: ts_rank
```
Return the rolling rank.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: ts_sum
```
Return the rolling sum.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: ts_product
```
Return the rolling product.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: ts_mean
```
Return the rolling mean.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: ts_wmean
```
Weighted moving average over the past `d` days with linearly decaying weight.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: ts_std
```
Return the rolling standard deviation.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: ts_corr
```
Time-serial correlation of `x` and `y` for the past `d` days.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: ts_cov
```
Time-serial covariance of `x` and `y` for the past `d` days.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: adv
```
Average daily dollar volume for the past `d` days.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: returns
```
Daily close-to-close returns.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: vwap
```
VWAP.
```

---

# Pasta: indicators
### Arquivo: expr.py
#### Função: cap
```
Market capitalization.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Docstring do Módulo
```
Factory for building indicators.

Run for the examples below:

```pycon
>>> from vectorbtpro import *

>>> price = pd.DataFrame({
...     'a': [1, 2, 3, 4, 5],
...     'b': [5, 4, 3, 2, 1]
... }, index=pd.date_range("2020", periods=5)).astype(float)
>>> price
            a    b
2020-01-01  1.0  5.0
2020-01-02  2.0  4.0
2020-01-03  3.0  3.0
2020-01-04  4.0  2.0
2020-01-05  5.0  1.0
```
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Função: prepare_params
```
Prepare parameters.

Resolves references and performs broadcasting to the input shape.

Returns prepared parameters as well as whether the user provided a single parameter combination.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Função: build_columns
```
For each parameter in `params`, create a new column level with parameter values
and stack it on top of `input_columns`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Função: combine_objs
```
Combines/compares `obj` to `other`, for example, to generate signals.

Both will broadcast together.
Pass `other` as a tuple or a list to compare with multiple arguments.
In this case, a new column level will be created with the name `level_name`.

See `vectorbtpro.base.accessors.BaseAccessor.combine`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Função: combine_indicator_with_other
```
Combine `IndicatorBase` with other compatible object.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
```
Indicator base class.

Properties should be set before instantiation.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Função: indicator
```
Shortcut for `vectorbtpro.indicators.factory.IndicatorFactory.get_indicator`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Função: talib
```
Shortcut for `vectorbtpro.indicators.factory.IndicatorFactory.from_talib`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Função: pandas_ta
```
Shortcut for `vectorbtpro.indicators.factory.IndicatorFactory.from_pandas_ta`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Função: ta
```
Shortcut for `vectorbtpro.indicators.factory.IndicatorFactory.from_ta`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Função: wqa101
```
Shortcut for `vectorbtpro.indicators.factory.IndicatorFactory.from_wqa101`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Função: technical
```
Shortcut for `vectorbtpro.indicators.factory.IndicatorFactory.from_technical`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Função: techcon
```
Shortcut for `vectorbtpro.indicators.factory.IndicatorFactory.from_techcon`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Função: smc
```
Shortcut for `vectorbtpro.indicators.factory.IndicatorFactory.from_smc`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: __getattr__
```
Redirect queries targeted at a generic output name by "output" or the short name of the indicator.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: main_output
```
Get main output.

It's either the only output or an output that matches the short name of the indicator.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: run_pipeline
```
A pipeline for running an indicator, used by `IndicatorFactory`.

Args:
    num_ret_outputs (int): The number of output arrays returned by `custom_func`.
    custom_func (callable): A custom calculation function.

        See `IndicatorFactory.with_custom_func`.
    *args: Arguments passed to the `custom_func`.
    require_input_shape (bool): Whether to input shape is required.

        Will set `pass_input_shape` to True and raise an error if `input_shape` is None.
    input_shape (tuple): Shape to broadcast each input to.

        Can be passed to `custom_func`. See `pass_input_shape`.
    input_index (index_like): Sets index of each input.

        Can be used to label index if no inputs passed.
    input_columns (index_like): Sets columns of each input.

        Can be used to label columns if no inputs passed.
    inputs (mapping or sequence of array_like): A mapping or sequence of input arrays.

        Use mapping to also supply names. If sequence, will convert to a mapping using `input_{i}` key.
    in_outputs (mapping or sequence of array_like): A mapping or sequence of in-place output arrays.

        Use mapping to also supply names. If sequence, will convert to a mapping using `in_output_{i}` key.
    in_output_settings (dict or sequence of dict): Settings corresponding to each in-place output.

        If mapping, should contain keys from `in_outputs`.

        Following keys are accepted:

        * `dtype`: Create this array using this data type and `np.empty`. Default is None.
    broadcast_named_args (dict): Dictionary with named arguments to broadcast together with inputs.

        You can then pass argument names wrapped with `vectorbtpro.utils.template.Rep`
        and this method will substitute them by their corresponding broadcasted objects.
    broadcast_kwargs (dict): Keyword arguments passed to `vectorbtpro.base.reshaping.broadcast`
        to broadcast inputs.
    template_context (dict): Context used to substitute templates in `args` and `kwargs`.
    params (mapping or sequence of any): A mapping or sequence of parameters.

        Use mapping to also supply names. If sequence, will convert to a mapping using `param_{i}` key.

        Each element is either an array-like object or a single value of any type.
    param_product (bool): Whether to build a Cartesian product out of all parameters.
    random_subset (int): Number of parameter combinations to pick randomly.
    param_settings (dict or sequence of dict): Settings corresponding to each parameter.

        If mapping, should contain keys from `params`.

        Following keys are accepted:

        * `dtype`: If data type is an enumerated type or other mapping, and a string as parameter
            value was passed, will convert it first.
        * `dtype_kwargs`: Keyword arguments passed to the function processing the data type.
            If data type is enumerated, it will be `vectorbtpro.utils.enum_.map_enum_fields`.
        * `is_tuple`: If tuple was passed, it will be considered as a single value.
            To treat it as multiple values, pack it into a list.
        * `is_array_like`: If array-like object was passed, it will be considered as a single value.
            To treat it as multiple values, pack it into a list.
        * `template`: Template to substitute each parameter value with, before broadcasting to input.
        * `min_one_dim`: Whether to convert any scalar into a one-dimensional array.
            Works only if `bc_to_input` is False.
        * `bc_to_input`: Whether to broadcast parameter to input size. You can also broadcast
            parameter to an axis by passing an integer.
        * `broadcast_kwargs`: Keyword arguments passed to `vectorbtpro.base.reshaping.broadcast`.
        * `per_column`: Whether each parameter value can be split by columns such that it can
            be better reflected in a multi-index. Does not affect broadcasting.
        * `post_index_func`: Function to convert the final index level of the parameter. Defaults to None.
    run_unique (bool): Whether to run only on unique parameter combinations.

        Disable if two identical parameter combinations can lead to different results
        (e.g., due to randomness) or if inputs are large and `custom_func` is fast.

        !!! note
            Cache, raw output, and output objects outside of `num_ret_outputs` will be returned
            for unique parameter combinations only.
    silence_warnings (bool): Whether to hide warnings such as coming from `run_unique`.
    per_column (bool): Whether the values of each parameter should be split by columns.

        Defaults to False. Will pass `per_column` if it's not None.

        Each list of parameter values will broadcast to the number of columns and
        each parameter value will be applied per column rather than per whole input.
        Input shape must be known beforehand.

        Each from inputs, in-outputs, and parameters will be passed to `custom_func`
        with the full shape. Expects the outputs be of the same shape as inputs.
    keep_pd (bool): Whether to keep inputs as pandas objects, otherwise convert to NumPy arrays.
    to_2d (bool): Whether to reshape inputs to 2-dim arrays, otherwise keep as-is.
    pass_packed (bool): Whether to pass inputs and parameters to `custom_func` as lists.

        If `custom_func` is Numba-compiled, passes tuples.
    pass_input_shape (bool): Whether to pass `input_shape` to `custom_func` as keyword argument.

        Defaults to True if `require_input_shape` is True, otherwise to False.
    pass_wrapper (bool): Whether to pass the input wrapper to `custom_func` as keyword argument.
    pass_param_index (bool): Whether to pass parameter index.
    pass_final_index (bool): Whether to pass final index.
    pass_single_comb (bool): Whether to pass whether there is only one parameter combination.
    level_names (list of str): A list of column level names corresponding to each parameter.

        Must have the same length as `params`.
    hide_levels (list of int or str): A list of level names or indices of parameter levels to hide.
    build_col_kwargs (dict): Keyword arguments passed to `build_columns`.
    return_raw (bool or str): Whether to return raw outputs and hashed parameter tuples without
        further post-processing.

        Pass "outputs" to only return outputs.
    use_raw (bool): Takes the raw results and uses them instead of running `custom_func`.
    wrapper_kwargs (dict): Keyword arguments passed to `vectorbtpro.base.wrapping.ArrayWrapper`.
    seed (int): Seed to make output deterministic.
    **kwargs: Keyword arguments passed to the `custom_func`.

        Some common arguments include `return_cache` to return cache and `use_cache` to use cache.
        If `use_cache` is False, disables caching completely. Those are only applicable to `custom_func`
        that supports it (`custom_func` created using `IndicatorFactory.with_apply_func` are supported by default).

Returns:
    Array wrapper, list of inputs (`np.ndarray`), input mapper (`np.ndarray`), list of outputs
    (`np.ndarray`), list of parameter arrays (`np.ndarray`), list of parameter mappers (`np.ndarray`),
    list of outputs that are outside of `num_ret_outputs`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: _run
```
Private run method.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: run
```
Public run method.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: _run_combs
```
Private run combinations method.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: run_combs
```
Public run combinations method.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: row_stack
```
Stack multiple `IndicatorBase` instances along rows.

Uses `vectorbtpro.base.wrapping.ArrayWrapper.row_stack` to stack the wrappers.

All objects to be merged must have the same columns x parameters.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: column_stack
```
Stack multiple `IndicatorBase` instances along columns x parameters.

Uses `vectorbtpro.base.wrapping.ArrayWrapper.column_stack` to stack the wrappers.

All objects to be merged must have the same index.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: _tuple_mapper
```
Mapper of multiple parameters.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: _param_mapper
```
Mapper of all parameters.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: _visible_param_mapper
```
Mapper of visible parameters.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: indexing_func
```
Perform indexing on `IndicatorBase`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: short_name
```
Name of the indicator.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: input_names
```
Names of the input arrays.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: param_names
```
Names of the parameters.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: in_output_names
```
Names of the in-place output arrays.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: output_names
```
Names of the regular output arrays.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: lazy_output_names
```
Names of the lazy output arrays.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: output_flags
```
Dictionary of output flags.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: level_names
```
Column level names corresponding to each parameter.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: param_defaults
```
Parameter defaults extracted from the signature of `IndicatorBase.run`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: unpack
```
Return outputs, either one output or a tuple if there are multiple.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: to_dict
```
Return outputs as a dict.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: to_frame
```
Return outputs as a DataFrame.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: dropna
```
Drop missing values.

Keyword arguments are passed to `pd.Series.dropna` or `pd.DataFrame.dropna`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: rename
```
Replace the short name of the indicator.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorBase
#### Função: items
```
Iterate over columns (or groups if grouped and `Wrapping.group_select` is True).

Allows the following additional options for `group_by`: "all_params", "params"
(only those that aren't hidden), and parameter names.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: __init__
```
A factory for creating new indicators.

Initialize `IndicatorFactory` to create a skeleton and then use a class method
such as `IndicatorFactory.with_custom_func` to bind a calculation function to the skeleton.

Args:
    class_name (str): Name for the created indicator class.
    class_docstring (str): Docstring for the created indicator class.
    module_name (str): Name of the module the class originates from.
    short_name (str): Short name of the indicator.

        Defaults to lower-case `class_name`.
    prepend_name (bool): Whether to prepend `short_name` to each parameter level.
    input_names (list of str): List with input names.
    param_names (list of str): List with parameter names.
    in_output_names (list of str): List with in-output names.

        An in-place output is an output that is not returned but modified in-place.
        Some advantages of such outputs include:

        1) they don't need to be returned,
        2) they can be passed between functions as easily as inputs,
        3) they can be provided with already allocated data to safe memory,
        4) if data or default value are not provided, they are created empty to not occupy memory.
    output_names (list of str): List with output names.
    output_flags (dict): Dictionary of in-place and regular output flags.
    lazy_outputs (dict): Dictionary with user-defined functions that will be
        bound to the indicator class and wrapped with `property` if not already wrapped.
    attr_settings (dict): Dictionary with attribute settings.

        Attributes can be `input_names`, `in_output_names`, `output_names`, and `lazy_outputs`.

        Following keys are accepted:

        * `dtype`: Data type used to determine which methods to generate around this attribute.
            Set to None to disable. Default is `np.float_`. Can be set to instance of
            `collections.namedtuple` acting as enumerated type, or any other mapping;
            It will then create a property with suffix `readable` that contains data in a string format.
        * `enum_unkval`: Value to be considered as unknown. Applies to enumerated data types only.
        * `make_cacheable`: Whether to make the property cacheable. Applies to inputs only.
    metrics (dict): Metrics supported by `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats`.

        If dict, will be converted to `vectorbtpro.utils.config.Config`.
    stats_defaults (callable or dict): Defaults for `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats`.

        If dict, will be converted into a property.
    subplots (dict): Subplots supported by `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots`.

        If dict, will be converted to `vectorbtpro.utils.config.Config`.
    plots_defaults (callable or dict): Defaults for `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots`.

        If dict, will be converted into a property.
    **kwargs: Custom keyword arguments passed to the config.

!!! note
    The `__init__` method is not used for running the indicator, for this use `run`.
    The reason for this is indexing, which requires a clean `__init__` method for creating
    a new indicator object with newly indexed attributes.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: class_name
```
Name for the created indicator class.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: class_docstring
```
Docstring for the created indicator class.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: module_name
```
Name of the module the class originates from.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: short_name
```
Short name of the indicator.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: prepend_name
```
Whether to prepend `IndicatorFactory.short_name` to each parameter level.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: input_names
```
List with input names.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: param_names
```
List with parameter names.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: in_output_names
```
List with in-output names.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: output_names
```
List with output names.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: output_flags
```
Dictionary of in-place and regular output flags.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: lazy_outputs
```
Dictionary with user-defined functions that will become properties.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: attr_settings
```
Dictionary with attribute settings.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: metrics
```
Metrics supported by `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: stats_defaults
```
Defaults for `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: subplots
```
Subplots supported by `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: plots_defaults
```
Defaults for `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: Indicator
```
Built indicator class.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: with_custom_func
```
Build indicator class around a custom calculation function.

In contrast to `IndicatorFactory.with_apply_func`, this method offers full flexibility.
It's up to the user to handle caching and concatenate columns for each parameter (for example,
by using `vectorbtpro.base.combining.apply_and_concat`). Also, you must ensure that
each output array has an appropriate number of columns, which is the number of columns in
input arrays multiplied by the number of parameter combinations.

Args:
    custom_func (callable): A function that takes broadcast arrays corresponding
        to `input_names`, broadcast in-place output arrays corresponding to `in_output_names`,
        broadcast parameter arrays corresponding to `param_names`, and other arguments and
        keyword arguments, and returns outputs corresponding to `output_names` and other objects
        that are then returned with the indicator instance.

        Can be Numba-compiled.

        !!! note
            Shape of each output must be the same and match the shape of each input stacked
            n times (= the number of parameter values) along the column axis.
    require_input_shape (bool): Whether to input shape is required.
    param_settings (dict): A dictionary of parameter settings keyed by name.
        See `IndicatorBase.run_pipeline` for keys.

        Can be overwritten by any run method.
    in_output_settings (dict): A dictionary of in-place output settings keyed by name.
        See `IndicatorBase.run_pipeline` for keys.

        Can be overwritten by any run method.
    hide_params (bool or list of str): Parameter names to hide column levels for,
        or whether to hide all parameters.

        Can be overwritten by any run method.
    hide_default (bool): Whether to hide column levels of parameters with default value.

        Can be overwritten by any run method.
    var_args (bool): Whether run methods should accept variable arguments (`*args`).

        Set to True if `custom_func` accepts positional agruments that are not listed in the config.
    keyword_only_args (bool): Whether run methods should accept keyword-only arguments (`*`).

        Set to True to force the user to use keyword arguments (e.g., to avoid misplacing arguments).
    **pipeline_kwargs: Keyword arguments passed to `IndicatorBase.run_pipeline`.

        Can be overwritten by any run method.

        Can contain default values and also references to other arguments wrapped
        with `vectorbtpro.base.reshaping.Ref`.

Returns:
    `Indicator`, and optionally other objects that are returned by `custom_func`
    and exceed `output_names`.

Usage:
    * The following example produces the same indicator as the `IndicatorFactory.with_apply_func` example.

    ```pycon
    >>> @njit
    >>> def apply_func_nb(i, ts1, ts2, p1, p2, arg1, arg2):
    ...     return ts1 * p1[i] + arg1, ts2 * p2[i] + arg2

    >>> @njit
    ... def custom_func(ts1, ts2, p1, p2, arg1, arg2):
    ...     return vbt.base.combining.apply_and_concat_multiple_nb(
    ...         len(p1), apply_func_nb, ts1, ts2, p1, p2, arg1, arg2)

    >>> MyInd = vbt.IF(
    ...     input_names=['ts1', 'ts2'],
    ...     param_names=['p1', 'p2'],
    ...     output_names=['o1', 'o2']
    ... ).with_custom_func(custom_func, var_args=True, arg2=200)

    >>> myInd = MyInd.run(price, price * 2, [1, 2], [3, 4], 100)
    >>> myInd.o1
    custom_p1              1             2
    custom_p2              3             4
                    a      b      a      b
    2020-01-01  101.0  105.0  102.0  110.0
    2020-01-02  102.0  104.0  104.0  108.0
    2020-01-03  103.0  103.0  106.0  106.0
    2020-01-04  104.0  102.0  108.0  104.0
    2020-01-05  105.0  101.0  110.0  102.0
    >>> myInd.o2
    custom_p1              1             2
    custom_p2              3             4
                    a      b      a      b
    2020-01-01  206.0  230.0  208.0  240.0
    2020-01-02  212.0  224.0  216.0  232.0
    2020-01-03  218.0  218.0  224.0  224.0
    2020-01-04  224.0  212.0  232.0  216.0
    2020-01-05  230.0  206.0  240.0  208.0
    ```

    The difference between `apply_func_nb` here and in `IndicatorFactory.with_apply_func` is that
    here it takes the index of the current parameter combination that can be used for parameter selection.

    * You can also remove the entire `apply_func_nb` and define your logic in `custom_func`
    (which shouldn't necessarily be Numba-compiled):

    ```pycon
    >>> @njit
    ... def custom_func(ts1, ts2, p1, p2, arg1, arg2):
    ...     input_shape = ts1.shape
    ...     n_params = len(p1)
    ...     out1 = np.empty((input_shape[0], input_shape[1] * n_params), dtype=np.float_)
    ...     out2 = np.empty((input_shape[0], input_shape[1] * n_params), dtype=np.float_)
    ...     for k in range(n_params):
    ...         for col in range(input_shape[1]):
    ...             for i in range(input_shape[0]):
    ...                 out1[i, input_shape[1] * k + col] = ts1[i, col] * p1[k] + arg1
    ...                 out2[i, input_shape[1] * k + col] = ts2[i, col] * p2[k] + arg2
    ...     return out1, out2
    ```
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: with_apply_func
```
Build indicator class around a custom apply function.

In contrast to `IndicatorFactory.with_custom_func`, this method handles a lot of things for you,
such as caching, parameter selection, and concatenation. Your part is writing a function `apply_func`
that accepts a selection of parameters (single values as opposed to multiple values in
`IndicatorFactory.with_custom_func`) and does the calculation. It then automatically concatenates
the resulting arrays into a single array per output.

While this approach is simpler, it's also less flexible, since we can only work with
one parameter selection at a time and can't view all parameters.

The execution and concatenation is performed using `vectorbtpro.base.combining.apply_and_concat_each`.

!!! note
    If `apply_func` is a Numba-compiled function:

    * All inputs are automatically converted to NumPy arrays
    * Each argument in `*args` must be of a Numba-compatible type
    * You cannot pass keyword arguments
    * Your outputs must be arrays of the same shape, data type and data order

!!! note
    Reserved arguments such as  `per_column` (in this order) get passed as positional
    arguments if `jitted_loop` is True, otherwise as keyword arguments.

Args:
    apply_func (callable): A function that takes inputs, selection of parameters, and
        other arguments, and does calculations to produce outputs.

        Arguments are passed to `apply_func` in the following order:

        * `i` (index of the parameter combination) if `select_params` is set to False
        * `input_shape` if `pass_input_shape` is set to True and `input_shape` not in `kwargs_as_args`
        * Input arrays corresponding to `input_names`. Passed as a tuple if `pass_packed`, otherwise unpacked.
            If `select_params` is True, each argument is a list composed of multiple arrays -
            one per parameter combination. When `per_column` is True, each of those arrays
            corresponds to a column. Otherwise, they all refer to the same array. If `takes_1d`,
            each array gets additionally split into multiple column arrays. Still passed
            as a single array to the caching function.
        * In-output arrays corresponding to `in_output_names`. Passed as a tuple if `pass_packed`, otherwise unpacked.
            If `select_params` is True, each argument is a list composed of multiple arrays -
            one per parameter combination. When `per_column` is True, each of those arrays
            corresponds to a column. If `takes_1d`, each array gets additionally split into
            multiple column arrays. Still passed as a single array to the caching function.
        * Parameters corresponding to `param_names`. Passed as a tuple if `pass_packed`, otherwise unpacked.
            If `select_params` is True, each argument is a list composed of multiple values -
            one per parameter combination.  When `per_column` is True, each of those values
            corresponds to a column. If `takes_1d`, each value gets additionally repeated by
            the number of columns in the input arrays.
        * Variable arguments if `var_args` is set to True
        * `per_column` if `pass_per_column` is set to True and `per_column` not in
            `kwargs_as_args` and `jitted_loop` is set to True
        * Arguments listed in `kwargs_as_args` passed as positional. Can include `takes_1d` and `per_column`.
        * Other keyword arguments if `jitted_loop` is False. Also includes `takes_1d` and `per_column`
            if they must be passed and not in `kwargs_as_args`.

        Can be Numba-compiled (but doesn't have to).

        !!! note
            Shape of each output must be the same and match the shape of each input.
    cache_func (callable): A caching function to preprocess data beforehand.

        Takes the same arguments as `apply_func`. Must return a single object or a tuple of objects.
        All returned objects will be passed unpacked as last arguments to `apply_func`.

        Can be Numba-compiled (but doesn't have to).
    takes_1d (bool): Whether to split 2-dim arrays into multiple 1-dim arrays along the column axis.

        Gets applied on inputs and in-outputs, while parameters get repeated by the number of columns.
    select_params (bool): Whether to automatically select in-outputs and parameters.

        If False, prepends the current iteration index to the arguments.
    pass_packed (bool): Whether to pass packed tuples for inputs, in-place outputs, and parameters.
    cache_pass_packed (bool): Overrides `pass_packed` for the caching function.
    pass_per_column (bool): Whether to pass `per_column`.
    cache_pass_per_column (bool): Overrides `pass_per_column` for the caching function.
    forward_skipna (bool): Whether to forward `skipna` to the apply function.
    kwargs_as_args (iterable of str): Keyword arguments from `kwargs` dict to pass as
        positional arguments to the apply function.

        Should be used together with `jitted_loop` set to True since Numba doesn't support
        variable keyword arguments.

        Defaults to []. Order matters.
    jit_kwargs (dict): Keyword arguments passed to `@njit` decorator of the parameter selection function.

        By default, has `nogil` set to True.
    **kwargs: Keyword arguments passed to `IndicatorFactory.with_custom_func`, all the way down
        to `vectorbtpro.base.combining.apply_and_concat_each`.

Returns:
    Indicator

Usage:
    * The following example produces the same indicator as the `IndicatorFactory.with_custom_func` example.

    ```pycon
    >>> @njit
    ... def apply_func_nb(ts1, ts2, p1, p2, arg1, arg2):
    ...     return ts1 * p1 + arg1, ts2 * p2 + arg2

    >>> MyInd = vbt.IF(
    ...     input_names=['ts1', 'ts2'],
    ...     param_names=['p1', 'p2'],
    ...     output_names=['out1', 'out2']
    ... ).with_apply_func(
    ...     apply_func_nb, var_args=True,
    ...     kwargs_as_args=['arg2'], arg2=200)

    >>> myInd = MyInd.run(price, price * 2, [1, 2], [3, 4], 100)
    >>> myInd.out1
    custom_p1              1             2
    custom_p2              3             4
                    a      b      a      b
    2020-01-01  101.0  105.0  102.0  110.0
    2020-01-02  102.0  104.0  104.0  108.0
    2020-01-03  103.0  103.0  106.0  106.0
    2020-01-04  104.0  102.0  108.0  104.0
    2020-01-05  105.0  101.0  110.0  102.0
    >>> myInd.out2
    custom_p1              1             2
    custom_p2              3             4
                    a      b      a      b
    2020-01-01  206.0  230.0  208.0  240.0
    2020-01-02  212.0  224.0  216.0  232.0
    2020-01-03  218.0  218.0  224.0  224.0
    2020-01-04  224.0  212.0  232.0  216.0
    2020-01-05  230.0  206.0  240.0  208.0
    ```

    * To change the execution engine or specify other engine-related arguments, use `execute_kwargs`:

    ```pycon
    >>> import time

    >>> def apply_func(ts, p):
    ...     time.sleep(1)
    ...     return ts * p

    >>> MyInd = vbt.IF(
    ...     input_names=['ts'],
    ...     param_names=['p'],
    ...     output_names=['out']
    ... ).with_apply_func(apply_func)

    >>> %timeit MyInd.run(price, [1, 2, 3])
    3.02 s ± 3.47 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

    >>> %timeit MyInd.run(price, [1, 2, 3], execute_kwargs=dict(engine='dask'))
    1.02 s ± 2.67 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
    ```
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: custom_indicators
```
Custom indicators keyed by custom locations.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: list_custom_locations
```
List custom locations.

Appear in the order they were registered.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: list_builtin_locations
```
List built-in locations.

Appear in the order as defined by the author.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: list_locations
```
List all supported locations.

First come custom locations, then built-in locations.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: match_location
```
Match location.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: split_indicator_name
```
Split an indicator name into location and actual name.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: register_custom_indicator
```
Register a custom indicator under a custom location.

Argument `if_exists` can be "raise", "skip", or "override".
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: deregister_custom_indicator
```
Deregister a custom indicator by its name and location.

If `location` is None, deregisters all indicators with the same name across all custom locations.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: get_custom_indicator
```
Get a custom indicator.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: list_custom_indicators
```
List custom indicators.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: list_vbt_indicators
```
List all vectorbt indicators.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: list_indicators
```
List indicators, optionally matching a pattern.

Pattern can also be a location, in such a case all indicators from that location will be returned.
For supported locations, see `IndicatorFactory.list_locations`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: get_indicator
```
Get the indicator class by its name.

The name can contain a location suffix followed by a colon. For example, "talib:sma"
or "talib_sma" will return the TA-Lib's SMA. Without a location, the indicator will be
searched throughout all indicators, including the vectorbt's ones.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: list_talib_indicators
```
List all parseable indicators in `talib`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: from_talib
```
Build an indicator class around a `talib` function.

Requires [TA-Lib](https://github.com/mrjbq7/ta-lib) installed.

For input, parameter and output names, see [docs](https://github.com/mrjbq7/ta-lib/blob/master/docs/index.md).

Args:
    func_name (str): Function name.
    factory_kwargs (dict): Keyword arguments passed to `IndicatorFactory`.
    **kwargs: Keyword arguments passed to `IndicatorFactory.with_apply_func`.

Returns:
    Indicator

Usage:
    ```pycon
    >>> SMA = vbt.IF.from_talib('SMA')

    >>> sma = SMA.run(price, timeperiod=[2, 3])
    >>> sma.real
    sma_timeperiod         2         3
                      a    b    a    b
    2020-01-01      NaN  NaN  NaN  NaN
    2020-01-02      1.5  4.5  NaN  NaN
    2020-01-03      2.5  3.5  2.0  4.0
    2020-01-04      3.5  2.5  3.0  3.0
    2020-01-05      4.5  1.5  4.0  2.0
    ```

    * To get help on running the indicator, use `vectorbtpro.utils.formatting.phelp`:

    ```pycon
    >>> vbt.phelp(SMA.run)
    SMA.run(
        close,
        timeperiod=Default(value=30),
        timeframe=Default(value=None),
        short_name='sma',
        hide_params=None,
        hide_default=True,
        **kwargs
    ):
        Run `SMA` indicator.

        * Inputs: `close`
        * Parameters: `timeperiod`, `timeframe`
        * Outputs: `real`

        Pass a list of parameter names as `hide_params` to hide their column levels, or True to hide all.
        Set `hide_default` to False to show the column levels of the parameters with a default value.

        Other keyword arguments are passed to `SMA.run_pipeline`.
    ```

    * To plot an indicator:

    ```pycon
    >>> sma.plot(column=(2, 'a')).show()
    ```

    ![](/assets/images/api/talib_plot.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/talib_plot.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: parse_pandas_ta_config
```
Parse the config of a `pandas_ta` indicator.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: list_pandas_ta_indicators
```
List all parseable indicators in `pandas_ta`.

!!! note
    Returns only the indicators that have been successfully parsed.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: from_pandas_ta
```
Build an indicator class around a `pandas_ta` function.

Requires [pandas-ta](https://github.com/twopirllc/pandas-ta) installed.

Args:
    func_name (str): Function name.
    parse_kwargs (dict): Keyword arguments passed to `IndicatorFactory.parse_pandas_ta_config`.
    factory_kwargs (dict): Keyword arguments passed to `IndicatorFactory`.
    **kwargs: Keyword arguments passed to `IndicatorFactory.with_apply_func`.

Returns:
    Indicator

Usage:
    ```pycon
    >>> SMA = vbt.IF.from_pandas_ta('SMA')

    >>> sma = SMA.run(price, length=[2, 3])
    >>> sma.sma
    sma_length         2         3
                  a    b    a    b
    2020-01-01  NaN  NaN  NaN  NaN
    2020-01-02  1.5  4.5  NaN  NaN
    2020-01-03  2.5  3.5  2.0  4.0
    2020-01-04  3.5  2.5  3.0  3.0
    2020-01-05  4.5  1.5  4.0  2.0
    ```

    * To get help on running the indicator, use `vectorbtpro.utils.formatting.phelp`:

    ```pycon
    >>> vbt.phelp(SMA.run)
    SMA.run(
        close,
        length=Default(value=None),
        talib=Default(value=None),
        offset=Default(value=None),
        short_name='sma',
        hide_params=None,
        hide_default=True,
        **kwargs
    ):
        Run `SMA` indicator.

        * Inputs: `close`
        * Parameters: `length`, `talib`, `offset`
        * Outputs: `sma`

        Pass a list of parameter names as `hide_params` to hide their column levels, or True to hide all.
        Set `hide_default` to False to show the column levels of the parameters with a default value.

        Other keyword arguments are passed to `SMA.run_pipeline`.
    ```

    * To get the indicator docstring, use the `help` command or print the `__doc__` attribute:

    ```pycon
    >>> print(SMA.__doc__)
    Simple Moving Average (SMA)

    The Simple Moving Average is the classic moving average that is the equally
    weighted average over n periods.

    Sources:
        https://www.tradingtechnologies.com/help/x-study/technical-indicator-definitions/simple-moving-average-sma/

    Calculation:
        Default Inputs:
            length=10
        SMA = SUM(close, length) / length

    Args:
        close (pd.Series): Series of 'close's
        length (int): It's period. Default: 10
        offset (int): How many periods to offset the result. Default: 0

    Kwargs:
        adjust (bool): Default: True
        presma (bool, optional): If True, uses SMA for initial value.
        fillna (value, optional): pd.DataFrame.fillna(value)
        fill_method (value, optional): Type of fill method

    Returns:
        pd.Series: New feature generated.
    ```
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: list_ta_indicators
```
List all parseable indicators in `ta`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: find_ta_indicator
```
Get `ta` indicator class by its name.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: parse_ta_config
```
Parse the config of a `ta` indicator.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: from_ta
```
Build an indicator class around a `ta` class.

Requires [ta](https://github.com/bukosabino/ta) installed.

Args:
    cls_name (str): Class name.
    factory_kwargs (dict): Keyword arguments passed to `IndicatorFactory`.
    **kwargs: Keyword arguments passed to `IndicatorFactory.with_apply_func`.

Returns:
    Indicator

Usage:
    ```pycon
    >>> SMAIndicator = vbt.IF.from_ta('SMAIndicator')

    >>> sma = SMAIndicator.run(price, window=[2, 3])
    >>> sma.sma_indicator
    smaindicator_window    2         3
                           a    b    a    b
    2020-01-01           NaN  NaN  NaN  NaN
    2020-01-02           1.5  4.5  NaN  NaN
    2020-01-03           2.5  3.5  2.0  4.0
    2020-01-04           3.5  2.5  3.0  3.0
    2020-01-05           4.5  1.5  4.0  2.0
    ```

    * To get help on running the indicator, use `vectorbtpro.utils.formatting.phelp`:

    ```pycon
    >>> vbt.phelp(SMAIndicator.run)
    SMAIndicator.run(
        close,
        window,
        fillna=Default(value=False),
        short_name='smaindicator',
        hide_params=None,
        hide_default=True,
        **kwargs
    ):
        Run `SMAIndicator` indicator.

        * Inputs: `close`
        * Parameters: `window`, `fillna`
        * Outputs: `sma_indicator`

        Pass a list of parameter names as `hide_params` to hide their column levels, or True to hide all.
        Set `hide_default` to False to show the column levels of the parameters with a default value.

        Other keyword arguments are passed to `SMAIndicator.run_pipeline`.
    ```

    * To get the indicator docstring, use the `help` command or print the `__doc__` attribute:

    ```pycon
    >>> print(SMAIndicator.__doc__)
    SMA - Simple Moving Average

        Args:
            close(pandas.Series): dataset 'Close' column.
            window(int): n period.
            fillna(bool): if True, fill nan values.
    ```
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: parse_technical_config
```
Parse the config of a `technical` indicator.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: list_technical_indicators
```
List all parseable indicators in `technical`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: find_technical_indicator
```
Get `technical` indicator function by its name.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: from_technical
```
Build an indicator class around a `technical` function.

Requires [technical](https://github.com/freqtrade/technical) installed.

Args:
    func_name (str): Function name.
    parse_kwargs (dict): Keyword arguments passed to `IndicatorFactory.parse_technical_config`.
    factory_kwargs (dict): Keyword arguments passed to `IndicatorFactory`.
    **kwargs: Keyword arguments passed to `IndicatorFactory.with_apply_func`.

Returns:
    Indicator

Usage:
    ```pycon
    >>> ROLLING_MEAN = vbt.IF.from_technical("ROLLING_MEAN")

    >>> rolling_mean = ROLLING_MEAN.run(price, window=[3, 4])
    >>> rolling_mean.rolling_mean
    rolling_mean_window         3         4
                           a    b    a    b
    2020-01-01           NaN  NaN  NaN  NaN
    2020-01-02           NaN  NaN  NaN  NaN
    2020-01-03           2.0  4.0  NaN  NaN
    2020-01-04           3.0  3.0  2.5  3.5
    2020-01-05           4.0  2.0  3.5  2.5
    ```

    * To get help on running the indicator, use `vectorbtpro.utils.formatting.phelp`:

    ```pycon
    >>> vbt.phelp(ROLLING_MEAN.run)
    ROLLING_MEAN.run(
        close,
        window=Default(value=200),
        min_periods=Default(value=None),
        short_name='rolling_mean',
        hide_params=None,
        hide_default=True,
        **kwargs
    ):
        Run `ROLLING_MEAN` indicator.

        * Inputs: `close`
        * Parameters: `window`, `min_periods`
        * Outputs: `rolling_mean`

        Pass a list of parameter names as `hide_params` to hide their column levels, or True to hide all.
        Set `hide_default` to False to show the column levels of the parameters with a default value.

        Other keyword arguments are passed to `ROLLING_MEAN.run_pipeline`.
    ```
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: from_custom_techcon
```
Create an indicator based on a technical consensus class subclassing
`technical.consensus.consensus.Consensus`.

Requires Technical library: https://github.com/freqtrade/technical
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: from_techcon
```
Create an indicator from a preset technical consensus.

Supported are case-insensitive values `MACON` (or `MovingAverageConsensus`),
`OSCCON` (or `OscillatorConsensus`), and `SUMCON` (or `SummaryConsensus`).
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: list_techcon_indicators
```
List all consensus indicators in `technical`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: find_smc_indicator
```
Get `smartmoneyconcepts` indicator class by its name.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: parse_smc_config
```
Parse the config of a `smartmoneyconcepts` indicator.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: list_smc_indicators
```
List all parseable indicators in `smartmoneyconcepts`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: from_smc
```
Build an indicator class around a `smartmoneyconcepts` function.

Requires [smart-money-concepts](https://github.com/joshyattridge/smart-money-concepts) installed.

Args:
    func_name (str): Function name.
    collapse (bool): Whether to collapse all nested indicators into a single one.
    parse_kwargs (dict): Keyword arguments passed to `IndicatorFactory.parse_smc_config`.
    factory_kwargs (dict): Keyword arguments passed to `IndicatorFactory`.
    **kwargs: Keyword arguments passed to `IndicatorFactory.with_apply_func`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: from_expr
```
Build an indicator class from an indicator expression.

Args:
    expr (str): Expression.

        Expression must be a string with a valid Python code.
        Supported are both single-line and multi-line expressions.
    parse_annotations (bool): Whether to parse annotations starting with `@`.
    factory_kwargs (dict): Keyword arguments passed to `IndicatorFactory`.

        Only applied when calling the class method.
    magnet_inputs (iterable of str): Names recognized as input names.

        Defaults to `open`, `high`, `low`, `close`, and `volume`.
    magnet_in_outputs (iterable of str): Names recognized as in-output names.

        Defaults to an empty list.
    magnet_params (iterable of str): Names recognized as params names.

        Defaults to an empty list.
    func_mapping (mapping): Mapping merged over `vectorbtpro.indicators.expr.expr_func_config`.

        Each key must be a function name and each value must be a dict with
        `func` and optionally `magnet_inputs`, `magnet_in_outputs`, and `magnet_params`.
    res_func_mapping (mapping): Mapping merged over `vectorbtpro.indicators.expr.expr_res_func_config`.

        Each key must be a function name and each value must be a dict with
        `func` and optionally `magnet_inputs`, `magnet_in_outputs`, and `magnet_params`.
    use_pd_eval (bool): Whether to use `pd.eval`.

        Defaults to False.

        Otherwise, uses `vectorbtpro.utils.eval_.multiline_eval`.

        !!! hint
            By default, operates on NumPy objects using NumExpr.
            If you want to operate on Pandas objects, set `keep_pd` to True.
    pd_eval_kwargs (dict): Keyword arguments passed to `pd.eval`.
    return_clean_expr (bool): Whether to return a cleaned expression.
    **kwargs: Keyword arguments passed to `IndicatorFactory.with_apply_func`.

Returns:
    Indicator

Searches each variable name parsed from `expr` in

* `vectorbtpro.indicators.expr.expr_res_func_config` (calls right away)
* `vectorbtpro.indicators.expr.expr_func_config`
* inputs, in-outputs, and params
* keyword arguments
* attributes of `np`
* attributes of `vectorbtpro.generic.nb` (with and without `_nb` suffix)
* attributes of `vbt`

`vectorbtpro.indicators.expr.expr_func_config` and `vectorbtpro.indicators.expr.expr_res_func_config`
can be overridden with `func_mapping` and `res_func_mapping` respectively.

!!! note
    Each variable name is case-sensitive.

When using the class method, all names are parsed from the expression itself.
If any of `open`, `high`, `low`, `close`, and `volume` appear in the expression or
in `magnet_inputs` in either `vectorbtpro.indicators.expr.expr_func_config` or
`vectorbtpro.indicators.expr.expr_res_func_config`, they are automatically added to `input_names`.
Set `magnet_inputs` to an empty list to disable this logic.

If the expression begins with a valid variable name and a colon (`:`), the variable name
will be used as the name of the generated class. Provide another variable in the square brackets
after this one and before the colon to specify the indicator's short name.

If `parse_annotations` is True, variables that start with `@` have a special meaning:

* `@in_*`: input variable
* `@inout_*`: in-output variable
* `@p_*`: parameter variable
* `@out_*`: output variable
* `@out_*:`: indicates that the next part until a comma is an output
* `@talib_*`: name of a TA-Lib function. Uses the indicator's `apply_func`.
* `@res_*`: name of the indicator to resolve automatically. Input names can overlap with
    those of other indicators, while all other information gets a prefix with the indicator's short name.
* `@settings(*)`: settings to be merged with the current `IndicatorFactory.from_expr` settings.
    Everything within the parentheses gets evaluated using the Pythons `eval` command
    and must be a dictionary. Overrides defaults but gets overridden by any argument
    passed to this method. Arguments `expr` and `parse_annotations` cannot be overridden.

!!! note
    The parsed names come in the same order they appear in the expression, not in the execution order,
    apart from the magnet input names, which are added in the same order they appear in the list.

The number of outputs is derived based on the number of commas outside of any bracket pair.
If there is only one output, the output name is `out`. If more - `out1`, `out2`, etc.

Any information can be overridden using `factory_kwargs`.

Usage:
    ```pycon
    >>> WMA = vbt.IF(
    ...     class_name='WMA',
    ...     input_names=['close'],
    ...     param_names=['window'],
    ...     output_names=['wma']
    ... ).from_expr("wm_mean_nb(close, window)")

    >>> wma = WMA.run(price, window=[2, 3])
    >>> wma.wma
    wma_window                   2                   3
                       a         b         a         b
    2020-01-01       NaN       NaN       NaN       NaN
    2020-01-02  1.666667  4.333333       NaN       NaN
    2020-01-03  2.666667  3.333333  2.333333  3.666667
    2020-01-04  3.666667  2.333333  3.333333  2.666667
    2020-01-05  4.666667  1.333333  4.333333  1.666667
    ```

    * The same can be achieved by calling the class method and providing prefixes
    to the variable names to indicate their type:

    ```pycon
    >>> expr = "WMA: @out_wma:wm_mean_nb((@in_high + @in_low) / 2, @p_window)"
    >>> WMA = vbt.IF.from_expr(expr)
    >>> wma = WMA.run(price + 1, price, window=[2, 3])
    >>> wma.wma
    wma_window                   2                   3
                       a         b         a         b
    2020-01-01       NaN       NaN       NaN       NaN
    2020-01-02  2.166667  4.833333       NaN       NaN
    2020-01-03  3.166667  3.833333  2.833333  4.166667
    2020-01-04  4.166667  2.833333  3.833333  3.166667
    2020-01-05  5.166667  1.833333  4.833333  2.166667
    ```

    * Magnet names are recognized automatically:

    ```pycon
    >>> expr = "WMA: @out_wma:wm_mean_nb((high + low) / 2, @p_window)"
    ```

    * Most settings of this method can be overriden from within the expression:

    ```pycon
    >>> expr = """
    ... @settings({factory_kwargs={'class_name': 'WMA', 'param_names': ['window']}})
    ... @out_wma:wm_mean_nb((high + low) / 2, window)
    ... """
    ```
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: from_wqa101
```
Build an indicator class from one of the WorldQuant's 101 alpha expressions.

See `vectorbtpro.indicators.expr.wqa101_expr_config`.

!!! note
    Some expressions that utilize cross-sectional operations require columns to be
    a multi-index with a level `sector`, `subindustry`, or `industry`.

Usage:
    ```pycon
    >>> data = vbt.YFData.pull(['BTC-USD', 'ETH-USD'])

    >>> WQA1 = vbt.IF.from_wqa101(1)
    >>> wqa1 = WQA1.run(data.get('Close'))
    >>> wqa1.out
    symbol                     BTC-USD  ETH-USD
    Date
    2014-09-17 00:00:00+00:00     0.25     0.25
    2014-09-18 00:00:00+00:00     0.25     0.25
    2014-09-19 00:00:00+00:00     0.25     0.25
    2014-09-20 00:00:00+00:00     0.25     0.25
    2014-09-21 00:00:00+00:00     0.25     0.25
    ...                            ...      ...
    2022-01-21 00:00:00+00:00     0.00     0.50
    2022-01-22 00:00:00+00:00     0.00     0.50
    2022-01-23 00:00:00+00:00     0.25     0.25
    2022-01-24 00:00:00+00:00     0.50     0.00
    2022-01-25 00:00:00+00:00     0.50     0.00

    [2688 rows x 2 columns]
    ```

    * To get help on running the indicator, use `vectorbtpro.utils.formatting.phelp`:

    ```pycon
    >>> vbt.phelp(WQA1.run)
    WQA1.run(
        close,
        short_name='wqa1',
        hide_params=None,
        hide_default=True,
        **kwargs
    ):
        Run `WQA1` indicator.

        * Inputs: `close`
        * Outputs: `out`

        Pass a list of parameter names as `hide_params` to hide their column levels, or True to hide all.
        Set `hide_default` to False to show the column levels of the parameters with a default value.

        Other keyword arguments are passed to `WQA1.run_pipeline`.
    ```
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Classe: IndicatorFactory
#### Função: list_wqa101_indicators
```
List all WorldQuant's 101 alpha indicators.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Função: custom_func
```
Custom function that forwards inputs and parameters to `apply_func`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Função: apply_func
```
Apply function for `technical.consensus.movingaverage.MovingAverageConsensus`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Função: plot
```
Plot `MA.ma` against `MA.close`.

Args:
    column (str): Name of the column to plot.
    buy_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `buy`.
    sell_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `sell`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.
```

---

# Pasta: indicators
### Arquivo: factory.py
#### Função: input_prop
```
Input array.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Docstring do Módulo
```
Numba-compiled functions for custom indicators.

Provides an arsenal of Numba-compiled functions that are used by indicator
classes. These only accept NumPy arrays and other Numba-compatible types.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: ma_1d_nb
```
Moving average.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: ma_nb
```
2-dim version of `ma_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: msd_1d_nb
```
Moving standard deviation.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: msd_nb
```
2-dim version of `msd_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: bbands_1d_nb
```
Bollinger Bands.

Returns the upper band, the middle band, and the lower band.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: bbands_nb
```
2-dim version of `bbands_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: bbands_percent_b_1d_nb
```
Bollinger Bands %B.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: bbands_percent_b_nb
```
2-dim version of `bbands_percent_b_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: bbands_bandwidth_1d_nb
```
Bollinger Bands Bandwidth.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: bbands_bandwidth_nb
```
2-dim version of `bbands_bandwidth_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: avg_gain_1d_nb
```
Average gain.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: avg_gain_nb
```
2-dim version of `avg_gain_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: avg_loss_1d_nb
```
Average loss.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: avg_loss_nb
```
2-dim version of `avg_loss_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: rsi_1d_nb
```
RSI.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: rsi_nb
```
2-dim version of `rsi_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: stoch_k_1d_nb
```
Stochastic Oscillator %K.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: stoch_k_nb
```
2-dim version of `stoch_k_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: stoch_1d_nb
```
Stochastic Oscillator.

Returns the fast %K, the slow %K, and the slow %D.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: stoch_nb
```
2-dim version of `stoch_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: macd_1d_nb
```
MACD.

Returns the MACD and the signal.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: macd_nb
```
2-dim version of `macd_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: macd_hist_1d_nb
```
MACD histogram.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: macd_hist_nb
```
2-dim version of `macd_hist_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: iter_tr_nb
```
True Range (TR) at one iteration.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: tr_1d_nb
```
True Range (TR).
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: tr_nb
```
2-dim version of `tr_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: atr_1d_nb
```
Average True Range (ATR).

Returns TR and ATR.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: atr_nb
```
2-dim version of `atr_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: adx_1d_nb
```
Average Directional Movement Index (ADX).

Returns +DI, -DI, DX, and ADX.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: adx_nb
```
2-dim version of `adx_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: obv_1d_nb
```
On-Balance Volume (OBV).
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: obv_nb
```
2-dim version of `obv_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: ols_1d_nb
```
Rolling Ordinary Least Squares (OLS).
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: ols_nb
```
2-dim version of `ols_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: ols_pred_1d_nb
```
OLS prediction.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: ols_pred_nb
```
2-dim version of `ols_pred_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: ols_error_1d_nb
```
OLS error.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: ols_error_nb
```
2-dim version of `ols_error_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: ols_angle_1d_nb
```
OLS angle.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: ols_angle_nb
```
2-dim version of `ols_angle_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: typical_price_1d_nb
```
Typical price.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: typical_price_nb
```
2-dim version of `typical_price_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: vwap_1d_nb
```
Volume-Weighted Average Price (VWAP).
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: vwap_nb
```
2-dim version of `vwap_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: pivot_info_1d_nb
```
Pivot information.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: pivot_info_nb
```
2-dim version of `pivot_info_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: pivot_value_1d_nb
```
Pivot value.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: pivot_value_nb
```
2-dim version of `pivot_value_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: pivots_1d_nb
```
Pivots.

!!! warning
    To be used in plotting. Do not use it as an indicator!
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: pivots_nb
```
2-dim version of `pivots_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: modes_1d_nb
```
Modes.

!!! warning
    To be used in plotting. Do not use it as an indicator!
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: modes_nb
```
2-dim version of `modes_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: iter_med_price_nb
```
Median price at one iteration.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: iter_basic_bands_nb
```
Upper and lower bands at one iteration.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: final_basic_bands_nb
```
Final bands at one iteration.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: supertrend_acc_nb
```
Accumulator of `supertrend_nb`.

Takes a state of type `vectorbtpro.indicators.enums.SuperTrendAIS` and returns
a state of type `vectorbtpro.indicators.enums.SuperTrendAOS`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: supertrend_1d_nb
```
Supertrend.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: supertrend_nb
```
2-dim version of `supertrend_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: signal_detection_1d_nb
```
Signal detection.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: signal_detection_nb
```
2-dim version of `signal_detection_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: get_standard_hurst_nb
```
Estimate the Hurst exponent using standard method.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: get_rs_nb
```
Get rescaled range (R/S) for Hurst exponent estimation.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: get_log_rs_hurst_nb
```
Estimate the Hurst exponent using R/S method.

Windows are log-distributed.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: get_rs_hurst_nb
```
Estimate the Hurst exponent using R/S method.

Windows are linearly distributed.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: get_dma_hurst_nb
```
Estimate the Hurst exponent using DMA method.

Windows are linearly distributed.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: get_dsod_hurst_nb
```
Estimate the Hurst exponent using discrete second order derivative.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: get_hurst_nb
```
Estimate the Hurst exponent using various methods.

Uses the following methods:

* `HurstMethod.Standard`: `vectorbtpro.generic.nb.base.get_standard_hurst_nb`
* `HurstMethod.LogRS`: `vectorbtpro.generic.nb.base.get_log_rs_hurst_nb`
* `HurstMethod.RS`: `vectorbtpro.generic.nb.base.get_rs_hurst_nb`
* `HurstMethod.DMA`: `vectorbtpro.generic.nb.base.get_dma_hurst_nb`
* `HurstMethod.DSOD`: `vectorbtpro.generic.nb.base.get_dsod_hurst_nb`
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: rolling_hurst_1d_nb
```
Rolling version of `get_hurst_nb`.
```

---

# Pasta: indicators
### Arquivo: nb.py
#### Função: rolling_hurst_nb
```
2-dim version of `rolling_hurst_1d_nb`.
```

---

# Pasta: indicators
### Arquivo: talib_.py
#### Docstring do Módulo
```
Helper functions for TA-Lib.
```

---

# Pasta: indicators
### Arquivo: talib_.py
#### Função: talib_func
```
Get the TA-Lib indicator function.
```

---

# Pasta: indicators
### Arquivo: talib_.py
#### Função: talib_plot_func
```
Get the TA-Lib indicator plotting function.
```

---

# Pasta: indicators
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules for building and running indicators.

Technical indicators are used to see past trends and anticipate future moves.
See [Using Technical Indicators to Develop Trading Strategies](https://www.investopedia.com/articles/trading/11/indicators-and-strategies-explained.asp).
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: adx.py
#### Docstring do Módulo
```
Module with `ADX`.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: adx.py
#### Classe: _ADX
```
Average Directional Movement Index (ADX).

The indicator is used by some traders to determine the strength of a trend.

See [Average Directional Index (ADX)](https://www.investopedia.com/terms/a/adx.asp).
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: adx.py
#### Classe: _ADX
#### Função: plot
```
Plot `ADX.plus_di`, `ADX.minus_di`, and `ADX.adx`.

Args:
    column (str): Name of the column to plot.
    plus_di_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `ADX.plus_di`.
    minus_di_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `ADX.minus_di`.
    adx_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `ADX.adx`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.ADX.run(ohlcv['High'], ohlcv['Low'], ohlcv['Close']).plot().show()
    ```

    ![](/assets/images/api/ADX.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/ADX.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: atr.py
#### Docstring do Módulo
```
Module with `ATR`.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: atr.py
#### Classe: _ATR
```
Average True Range (ATR).

The indicator provide an indication of the degree of price volatility.
Strong moves, in either direction, are often accompanied by large ranges,
or large True Ranges.

See [Average True Range - ATR](https://www.investopedia.com/terms/a/atr.asp).
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: atr.py
#### Classe: _ATR
#### Função: plot
```
Plot `ATR.tr` and `ATR.atr`.

Args:
    column (str): Name of the column to plot.
    tr_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `ATR.tr`.
    atr_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `ATR.atr`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.ATR.run(ohlcv['High'], ohlcv['Low'], ohlcv['Close']).plot().show()
    ```

    ![](/assets/images/api/ATR.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/ATR.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: bbands.py
#### Docstring do Módulo
```
Module with `BBANDS`.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: bbands.py
#### Classe: _BBANDS
```
Bollinger Bands (BBANDS).

A Bollinger Band® is a technical analysis tool defined by a set of lines plotted two standard
deviations (positively and negatively) away from a simple moving average (SMA) of the security's
price, but can be adjusted to user preferences.

See [Bollinger Band®](https://www.investopedia.com/terms/b/bollingerbands.asp).
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: bbands.py
#### Classe: _BBANDS
#### Função: plot
```
Plot `BBANDS.upper`, `BBANDS.middle`, and `BBANDS.lower` against `BBANDS.close`.

Args:
    column (str): Name of the column to plot.
    plot_close (bool): Whether to plot `BBANDS.close`.
    close_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `BBANDS.close`.
    upper_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `BBANDS.upper`.
    middle_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `BBANDS.middle`.
    lower_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `BBANDS.lower`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.BBANDS.run(ohlcv['Close']).plot().show()
    ```

    ![](/assets/images/api/BBANDS.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/BBANDS.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: hurst.py
#### Docstring do Módulo
```
Module with `HURST`.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: hurst.py
#### Classe: _HURST
```
Moving Hurst exponent (HURST).
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: hurst.py
#### Classe: _HURST
#### Função: plot
```
Plot `HURST.hurst`.

Args:
    column (str): Name of the column to plot.
    hurst_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `HURST.hurst`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> ohlcv = vbt.YFData.pull(
    ...     "BTC-USD",
    ...     start="2020-01-01",
    ...     end="2024-01-01"
    ... ).get()
    >>> vbt.HURST.run(ohlcv["Close"]).plot().show()
    ```

    ![](/assets/images/api/HURST.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/HURST.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: ma.py
#### Docstring do Módulo
```
Module with `MA`.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: ma.py
#### Classe: _MA
```
Moving Average (MA).

A moving average is a widely used indicator in technical analysis that helps smooth out
price action by filtering out the “noise” from random short-term price fluctuations.

See [Moving Average (MA)](https://www.investopedia.com/terms/m/movingaverage.asp).
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: ma.py
#### Classe: _MA
#### Função: plot
```
Plot `MA.ma` against `MA.close`.

Args:
    column (str): Name of the column to plot.
    plot_close (bool): Whether to plot `MA.close`.
    close_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `MA.close`.
    ma_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `MA.ma`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.MA.run(ohlcv['Close']).plot().show()
    ```

    ![](/assets/images/api/MA.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/MA.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: macd.py
#### Docstring do Módulo
```
Module with `MACD`.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: macd.py
#### Classe: _MACD
```
Moving Average Convergence Divergence (MACD).

Is a trend-following momentum indicator that shows the relationship between
two moving averages of prices.

See [Moving Average Convergence Divergence – MACD](https://www.investopedia.com/terms/m/macd.asp).
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: macd.py
#### Classe: _MACD
#### Função: plot
```
Plot `MACD.macd`, `MACD.signal` and `MACD.hist`.

Args:
    column (str): Name of the column to plot.
    macd_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `MACD.macd`.
    signal_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `MACD.signal`.
    hist_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Bar` for `MACD.hist`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.MACD.run(ohlcv['Close']).plot().show()
    ```

    ![](/assets/images/api/MACD.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/MACD.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: msd.py
#### Docstring do Módulo
```
Module with `MSD`.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: msd.py
#### Classe: _MSD
```
Moving Standard Deviation (MSD).

Standard deviation is an indicator that measures the size of an assets recent price moves
in order to predict how volatile the price may be in the future.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: msd.py
#### Classe: _MSD
#### Função: plot
```
Plot `MSD.msd`.

Args:
    column (str): Name of the column to plot.
    msd_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `MSD.msd`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.MSD.run(ohlcv['Close']).plot().show()
    ```

    ![](/assets/images/api/MSD.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/MSD.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: obv.py
#### Docstring do Módulo
```
Module with `OBV`.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: obv.py
#### Classe: _OBV
```
On-balance volume (OBV).

It relates price and volume in the stock market. OBV is based on a cumulative total volume.

See [On-Balance Volume (OBV)](https://www.investopedia.com/terms/o/onbalancevolume.asp).
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: obv.py
#### Classe: _OBV
#### Função: plot
```
Plot `OBV.obv`.

Args:
    column (str): Name of the column to plot.
    obv_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `OBV.obv`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```py
    >>> vbt.OBV.run(ohlcv['Close'], ohlcv['Volume']).plot().show()
    ```

    ![](/assets/images/api/OBV.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/OBV.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: ols.py
#### Docstring do Módulo
```
Module with `OLS`.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: ols.py
#### Classe: _OLS
```
Rolling Ordinary Least Squares (OLS).

The indicator can be used to detect changes in the behavior of the stocks against the market or each other.

See [The Linear Regression of Time and Price](https://www.investopedia.com/articles/trading/09/linear-regression-time-price.asp).
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: ols.py
#### Classe: _OLS
#### Função: plot
```
Plot `OLS.pred` against `OLS.y`.

Args:
    column (str): Name of the column to plot.
    plot_y (bool): Whether to plot `OLS.y`.
    y_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `OLS.y`.
    pred_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `OLS.pred`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.OLS.run(np.arange(len(ohlcv)), ohlcv['Close']).plot().show()
    ```

    ![](/assets/images/api/OLS.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/OLS.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: ols.py
#### Classe: _OLS
#### Função: plot_zscore
```
Plot `OLS.zscore` with confidence intervals.

Args:
    column (str): Name of the column to plot.
    alpha (float): The alpha level for the confidence interval.

        The default alpha = .05 returns a 95% confidence interval.
    zscore_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `OLS.zscore`.
    add_shape_kwargs (dict): Keyword arguments passed to `fig.add_shape`
        when adding the range between both confidence intervals.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.OLS.run(np.arange(len(ohlcv)), ohlcv['Close']).plot_zscore().show()
    ```

    ![](/assets/images/api/OLS_zscore.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/OLS_zscore.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: patsim.py
#### Docstring do Módulo
```
Module with `PATSIM`.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: patsim.py
#### Classe: _PATSIM
```
Rolling pattern similarity.

Based on `vectorbtpro.generic.nb.rolling.rolling_pattern_similarity_nb`.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: patsim.py
#### Classe: _PATSIM
#### Função: plot
```
Plot `PATSIM.similarity` against `PATSIM.close`.

Args:
    column (str): Name of the column to plot.
    similarity_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `PATSIM.similarity`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.PATSIM.run(ohlcv['Close'], np.array([1, 2, 3, 2, 1]), 30).plot().show()
    ```

    ![](/assets/images/api/PATSIM.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/PATSIM.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: patsim.py
#### Classe: _PATSIM
#### Função: overlay_with_heatmap
```
Overlay `PATSIM.similarity` as a heatmap on top of `PATSIM.close`.

Args:
    column (str): Name of the column to plot.
    close_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `PATSIM.close`.
    similarity_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Heatmap` for `PATSIM.similarity`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.PATSIM.run(ohlcv['Close'], np.array([1, 2, 3, 2, 1]), 30).overlay_with_heatmap().show()
    ```

    ![](/assets/images/api/PATSIM_heatmap.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/PATSIM_heatmap.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: pivotinfo.py
#### Docstring do Módulo
```
Module with `PIVOTINFO`.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: pivotinfo.py
#### Classe: _PIVOTINFO
```
Indicator that returns various information on pivots identified based on thresholds.

* `conf_pivot`: the type of the latest confirmed pivot (running)
* `conf_idx`: the index of the latest confirmed pivot (running)
* `conf_value`: the high/low value under the latest confirmed pivot (running)
* `last_pivot`: the type of the latest pivot (running)
* `last_idx`: the index of the latest pivot (running)
* `last_value`: the high/low value under the latest pivot (running)
* `pivots`: confirmed pivots stored under their indices (looking ahead - use only for plotting!)
* `modes`: modes between confirmed pivot points (looking ahead - use only for plotting!)
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: pivotinfo.py
#### Classe: _PIVOTINFO
#### Função: plot
```
Plot `PIVOTINFO.conf_value` and `PIVOTINFO.last_value`.

Args:
    column (str): Name of the column to plot.
    conf_value_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `PIVOTINFO.conf_value` line.
    last_value_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `PIVOTINFO.last_value` line.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> fig = ohlcv.vbt.ohlcv.plot()
    >>> vbt.PIVOTINFO.run(ohlcv['High'], ohlcv['Low'], 0.1, 0.1).plot(fig=fig).show()
    ```

    ![](/assets/images/api/PIVOTINFO.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/PIVOTINFO.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: pivotinfo.py
#### Classe: _PIVOTINFO
#### Função: plot_zigzag
```
Plot zig-zag line.

Args:
    column (str): Name of the column to plot.
    zigzag_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for zig-zag line.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> fig = ohlcv.vbt.ohlcv.plot()
    >>> vbt.PIVOTINFO.run(ohlcv['High'], ohlcv['Low'], 0.1, 0.1).plot_zigzag(fig=fig).show()
    ```

    ![](/assets/images/api/PIVOTINFO_zigzag.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/PIVOTINFO_zigzag.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: rsi.py
#### Docstring do Módulo
```
Module with `RSI`.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: rsi.py
#### Classe: _RSI
```
Relative Strength Index (RSI).

Compares the magnitude of recent gains and losses over a specified time
period to measure speed and change of price movements of a security. It is
primarily used to attempt to identify overbought or oversold conditions in
the trading of an asset.

See [Relative Strength Index (RSI)](https://www.investopedia.com/terms/r/rsi.asp).
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: rsi.py
#### Classe: _RSI
#### Função: plot
```
Plot `RSI.rsi`.

Args:
    column (str): Name of the column to plot.
    limits (tuple of float): Tuple of the lower and upper limit.
    rsi_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `RSI.rsi`.
    add_shape_kwargs (dict): Keyword arguments passed to `fig.add_shape` when adding the range between both limits.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.RSI.run(ohlcv['Close']).plot().show()
    ```

    ![](/assets/images/api/RSI.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/RSI.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: sigdet.py
#### Docstring do Módulo
```
Module with `SIGDET`.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: sigdet.py
#### Classe: _SIGDET
```
Robust peak detection algorithm (using z-scores).

See https://stackoverflow.com/a/22640362
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: sigdet.py
#### Classe: _SIGDET
#### Função: plot
```
Plot `SIGDET.signal` against `SIGDET.close`.

Args:
    column (str): Name of the column to plot.
    signal_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `SIGDET.signal`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.SIGDET.run(ohlcv['Close']).plot().show()
    ```

    ![](/assets/images/api/SIGDET.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/SIGDET.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: sigdet.py
#### Classe: _SIGDET
#### Função: plot_bands
```
Plot `SIGDET.upper_band` and `SIGDET.lower_band` against `SIGDET.close`.

Args:
    column (str): Name of the column to plot.
    plot_close (bool): Whether to plot `SIGDET.close`.
    close_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `SIGDET.close`.
    upper_band_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `SIGDET.upper_band`.
    lower_band_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `SIGDET.lower_band`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.SIGDET.run(ohlcv['Close']).plot_bands().show()
    ```

    ![](/assets/images/api/SIGDET_plot_bands.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/SIGDET_plot_bands.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: stoch.py
#### Docstring do Módulo
```
Module with `STOCH`.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: stoch.py
#### Classe: _STOCH
```
Stochastic Oscillator (STOCH).

A stochastic oscillator is a momentum indicator comparing a particular closing price
of a security to a range of its prices over a certain period of time. It is used to
generate overbought and oversold trading signals, utilizing a 0-100 bounded range of values.

See [Stochastic Oscillator](https://www.investopedia.com/terms/s/stochasticoscillator.asp).
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: stoch.py
#### Classe: _STOCH
#### Função: plot
```
Plot `STOCH.slow_k` and `STOCH.slow_d`.

Args:
    column (str): Name of the column to plot.
    limits (tuple of float): Tuple of the lower and upper limit.
    fast_k_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `STOCH.fast_k`.
    slow_k_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `STOCH.slow_k`.
    slow_d_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `STOCH.slow_d`.
    add_shape_kwargs (dict): Keyword arguments passed to `fig.add_shape` when adding the range between both limits.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.STOCH.run(ohlcv['High'], ohlcv['Low'], ohlcv['Close']).plot().show()
    ```

    ![](/assets/images/api/STOCH.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/STOCH.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: supertrend.py
#### Docstring do Módulo
```
Module with `SUPERTREND`.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: supertrend.py
#### Classe: _SUPERTREND
```
Supertrend indicator.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: supertrend.py
#### Classe: _SUPERTREND
#### Função: plot
```
Plot `SUPERTREND.long` and `SUPERTREND.short` against `SUPERTREND.close`.

Args:
    column (str): Name of the column to plot.
    plot_close (bool): Whether to plot `SUPERTREND.close`.
    close_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `SUPERTREND.close`.
    superl_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `SUPERTREND.long`.
    supers_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `SUPERTREND.short`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.SUPERTREND.run(ohlcv['High'], ohlcv['Low'], ohlcv['Close']).plot().show()
    ```

    ![](/assets/images/api/SUPERTREND.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/SUPERTREND.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: vwap.py
#### Docstring do Módulo
```
Module with `VWAP`.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: vwap.py
#### Função: substitute_anchor
```
Substitute reset frequency by group lens.
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: vwap.py
#### Classe: _VWAP
```
Volume-Weighted Average Price (VWAP).

VWAP is a technical analysis indicator used on intraday charts that resets at the start
of every new trading session.

See [Volume-Weighted Average Price (VWAP)](https://www.investopedia.com/terms/v/vwap.asp).
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: vwap.py
#### Classe: _VWAP
#### Função: plot
```
Plot `VWAP.vwap` against `VWAP.close`.

Args:
    column (str): Name of the column to plot.
    plot_close (bool): Whether to plot `VWAP.close`.
    close_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `VWAP.close`.
    vwap_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `VWAP.vwap`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.VWAP.run(
    ...    ohlcv['High'],
    ...    ohlcv['Low'],
    ...    ohlcv['Close'],
    ...    ohlcv['Volume'],
    ...    anchor="W"
    ... ).plot().show()
    ```

    ![](/assets/images/api/VWAP.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/VWAP.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: indicators
## Subpasta: custom
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules with custom indicators built with the indicator factory.

You can access all the indicators by `vbt.*`.

Run for the examples:

```pycon
>>> ohlcv = vbt.YFData.pull(
...     "BTC-USD",
...     start="2019-03-01",
...     end="2019-09-01"
... ).get()
```
```

---

# Pasta: labels
### Arquivo: enums.py
#### Docstring do Módulo
```
Named tuples and enumerated types for label generation.

Defines enums and other schemas for `vectorbtpro.labels`.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Docstring do Módulo
```
Numba-compiled functions for label generation.

!!! note
    Set `wait` to 1 to exclude the current value from calculation of future values.

!!! warning
    Do not attempt to use these functions for building predictor variables as they may introduce
    the look-ahead bias to your model - only use for building target variables.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: future_mean_1d_nb
```
Rolling average over future values.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: future_mean_nb
```
2-dim version of `future_mean_1d_nb`.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: future_std_1d_nb
```
Rolling standard deviation over future values.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: future_std_nb
```
2-dim version of `future_std_1d_nb`.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: future_min_1d_nb
```
Rolling minimum over future values.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: future_min_nb
```
2-dim version of `future_min_1d_nb`.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: future_max_1d_nb
```
Rolling maximum over future values.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: future_max_nb
```
2-dim version of `future_max_1d_nb`.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: fixed_labels_1d_nb
```
Percentage change of the current value relative to a future value.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: fixed_labels_nb
```
2-dim version of `fixed_labels_1d_nb`.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: mean_labels_1d_nb
```
Percentage change of the current value relative to the average of a future period.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: mean_labels_nb
```
2-dim version of `mean_labels_1d_nb`.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: iter_symmetric_up_th_nb
```
Positive upper threshold that is symmetric to a negative one at one iteration.

For example, 50% down requires 100% to go up to the initial level.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: iter_symmetric_down_th_nb
```
Negative upper threshold that is symmetric to a positive one at one iteration.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: pivots_1d_nb
```
Pivots denoted by 1 (peak), 0 (no pivot) or -1 (valley).

Two adjacent peak and valley points should exceed the given threshold parameters.

If any threshold is given element-wise, it will be applied per new/updated pivot.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: pivots_nb
```
2-dim version of `pivots_1d_nb`.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: bin_trend_labels_1d_nb
```
Values classified into 0 (downtrend) and 1 (uptrend).
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: bin_trend_labels_nb
```
2-dim version of `bin_trend_labels_1d_nb`.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: binc_trend_labels_1d_nb
```
Median values normalized between 0 (downtrend) and 1 (uptrend).
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: binc_trend_labels_nb
```
2-dim version of `binc_trend_labels_1d_nb`.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: bincs_trend_labels_1d_nb
```
Median values normalized between 0 (downtrend) and 1 (uptrend) but capped once
the threshold defined at the beginning of the trend is exceeded.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: bincs_trend_labels_nb
```
2-dim version of `bincs_trend_labels_1d_nb`.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: pct_trend_labels_1d_nb
```
Percentage change of median values relative to the next pivot.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: pct_trend_labels_nb
```
2-dim version of `pct_trend_labels_1d_nb`.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: trend_labels_1d_nb
```
Trend labels based on `TrendLabelMode`.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: trend_labels_nb
```
2-dim version of `trend_labels_1d_nb`.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: breakout_labels_1d_nb
```
For each value, return 1 if any value in the next period is greater than the
positive threshold (in %), -1 if less than the negative threshold, and 0 otherwise.

First hit wins. Continue search if both thresholds were hit at the same time.
```

---

# Pasta: labels
### Arquivo: nb.py
#### Função: breakout_labels_nb
```
2-dim version of `breakout_labels_1d_nb`.
```

---

# Pasta: labels
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules for building and running look-ahead indicators and label generators.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: bolb.py
#### Docstring do Módulo
```
Module with `BOLB`.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: bolb.py
#### Classe: _BOLB
```
Label generator based on `vectorbtpro.labels.nb.breakout_labels_nb`.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: bolb.py
#### Classe: _BOLB
#### Função: plot
```
Plot the median of `BOLB.high` and `BOLB.low`, and overlay it with the heatmap of `BOLB.labels`.

`**kwargs` are passed to `vectorbtpro.generic.accessors.GenericAccessor.overlay_with_heatmap`.

Usage:
    ```pycon
    >>> vbt.BOLB.run(ohlcv['High'], ohlcv['Low'], up_th=0.2, down_th=0.2).plot().show()
    ```

    ![](/assets/images/api/BOLB.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/BOLB.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: fixlb.py
#### Docstring do Módulo
```
Module with `FIXLB`.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: fixlb.py
#### Classe: _FIXLB
```
Label generator based on `vectorbtpro.labels.nb.fixed_labels_nb`.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: fixlb.py
#### Classe: _FIXLB
#### Função: plot
```
Plot `FIXLB.close` and overlay it with the heatmap of `FIXLB.labels`.

`**kwargs` are passed to `vectorbtpro.generic.accessors.GenericAccessor.overlay_with_heatmap`.

Usage:
    ```pycon
    >>> vbt.FIXLB.run(ohlcv['Close']).plot().show()
    ```

    ![](/assets/images/api/FIXLB.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/FIXLB.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: fmax.py
#### Docstring do Módulo
```
Module with `FMAX`.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: fmax.py
#### Classe: _FMAX
```
Look-ahead indicator based on `vectorbtpro.labels.nb.future_max_nb`.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: fmax.py
#### Classe: _FMAX
#### Função: plot
```
Plot `FMAX.fmax` against `FMAX.close`.

Args:
    column (str): Name of the column to plot.
    plot_close (bool): Whether to plot `FMAX.close`.
    close_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `FMAX.close`.
    fmax_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `FMAX.fmax`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.FMAX.run(ohlcv['Close']).plot().show()
    ```

    ![](/assets/images/api/FMAX.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/FMAX.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: fmean.py
#### Docstring do Módulo
```
Module with `FMEAN`.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: fmean.py
#### Classe: _FMEAN
```
Look-ahead indicator based on `vectorbtpro.labels.nb.future_mean_nb`.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: fmean.py
#### Classe: _FMEAN
#### Função: plot
```
Plot `FMEAN.fmean` against `FMEAN.close`.

Args:
    column (str): Name of the column to plot.
    plot_close (bool): Whether to plot `FMEAN.close`.
    close_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `FMEAN.close`.
    fmean_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `FMEAN.fmean`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.FMEAN.run(ohlcv['Close']).plot().show()
    ```

    ![](/assets/images/api/FMEAN.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/FMEAN.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: fmin.py
#### Docstring do Módulo
```
Module with `FMIN`.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: fmin.py
#### Classe: _FMIN
```
Look-ahead indicator based on `vectorbtpro.labels.nb.future_min_nb`.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: fmin.py
#### Classe: _FMIN
#### Função: plot
```
Plot `FMIN.fmin` against `FMIN.close`.

Args:
    column (str): Name of the column to plot.
    plot_close (bool): Whether to plot `FMIN.close`.
    close_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `FMIN.close`.
    fmin_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `FMIN.fmin`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.FMIN.run(ohlcv['Close']).plot().show()
    ```

    ![](/assets/images/api/FMIN.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/FMIN.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: fstd.py
#### Docstring do Módulo
```
Module with `FSTD`.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: fstd.py
#### Classe: _FSTD
```
Look-ahead indicator based on `vectorbtpro.labels.nb.future_std_nb`.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: fstd.py
#### Classe: _FSTD
#### Função: plot
```
Plot `FSTD.fstd`.

Args:
    column (str): Name of the column to plot.
    fstd_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `FSTD.fstd`.
    add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments passed to `fig.update_layout`.

Usage:
    ```pycon
    >>> vbt.FSTD.run(ohlcv['Close']).plot().show()
    ```

    ![](/assets/images/api/FSTD.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/FSTD.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: meanlb.py
#### Docstring do Módulo
```
Module with `MEANLB`.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: meanlb.py
#### Classe: _MEANLB
```
Label generator based on `vectorbtpro.labels.nb.mean_labels_nb`.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: meanlb.py
#### Classe: _MEANLB
#### Função: plot
```
Plot `close` and overlay it with the heatmap of `MEANLB.labels`.

`**kwargs` are passed to `vectorbtpro.generic.accessors.GenericAccessor.overlay_with_heatmap`.

Usage:
    ```pycon
    >>> vbt.MEANLB.run(ohlcv['Close']).plot().show()
    ```

    ![](/assets/images/api/MEANLB.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/MEANLB.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: pivotlb.py
#### Docstring do Módulo
```
Module with `PIVOTLB`.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: pivotlb.py
#### Classe: _PIVOTLB
```
Label generator based on `vectorbtpro.labels.nb.pivots_nb`.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: pivotlb.py
#### Classe: _PIVOTLB
#### Função: plot
```
Plot the median of `PIVOTLB.high` and `PIVOTLB.low`, and overlay it with the heatmap of `PIVOTLB.labels`.

`**kwargs` are passed to `vectorbtpro.generic.accessors.GenericAccessor.overlay_with_heatmap`.

Usage:
    ```pycon
    >>> vbt.PIVOTLB.run(ohlcv['High'], ohlcv['Low'], up_th=0.2, down_th=0.2).plot().show()
    ```

    ![](/assets/images/api/PIVOTLB.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/PIVOTLB.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: trendlb.py
#### Docstring do Módulo
```
Module with `TRENDLB`.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: trendlb.py
#### Classe: _TRENDLB
```
Label generator based on `vectorbtpro.labels.nb.trend_labels_nb`.
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: trendlb.py
#### Classe: _TRENDLB
#### Função: plot
```
Plot the median of `TRENDLB.high` and `TRENDLB.low`, and overlay it with the heatmap of `TRENDLB.labels`.

`**kwargs` are passed to `vectorbtpro.generic.accessors.GenericAccessor.overlay_with_heatmap`.

Usage:
    ```pycon
    >>> vbt.TRENDLB.run(ohlcv['High'], ohlcv['Low'], up_th=0.2, down_th=0.2).plot().show()
    ```

    ![](/assets/images/api/TRENDLB.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/TRENDLB.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: labels
## Subpasta: generators
### Arquivo: __init__.py
#### Docstring do Módulo
```
Basic look-ahead indicators and label generators.

You can access all the indicators by `vbt.*`.

Run for the examples:

```pycon
>>> ohlcv = vbt.YFData.pull(
...     "BTC-USD",
...     start="2019-03-01",
...     end="2019-09-01"
... ).get()
```
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Docstring do Módulo
```
Messaging using Python Telegram Bot.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Função: send_action
```
Sends `action` while processing func command.

Suitable only for bound callbacks taking arguments `self`, `update`, `context` and optionally other.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Função: self_decorator
```
Pass bot object to func command.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: LogHandler
```
Handler to log user updates.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
```
Telegram bot.

See [Extensions – Your first Bot](https://github.com/python-telegram-bot/python-telegram-bot/wiki/Extensions-%E2%80%93-Your-first-Bot).

`**kwargs` are passed to `telegram.ext.updater.Updater` and override
settings under `telegram` in `vectorbtpro._settings.messaging`.

Usage:
    Let's extend `TelegramBot` to track cryptocurrency prices:

    ```python
    import ccxt
    import logging
    from vectorbtpro import *

    from telegram.ext import CommandHandler
    from telegram import __version__ as TG_VER

    try:
        from telegram import __version_info__
    except ImportError:
        __version_info__ = (0, 0, 0, 0, 0)

    if __version_info__ >= (20, 0, 0, "alpha", 1):
        raise RuntimeError(f"This example is not compatible with your current PTB version {TG_VER}")

    # Enable logging
    logging.basicConfig(
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", level=logging.INFO
    )
    logger = logging.getLogger(__name__)


    class MyTelegramBot(vbt.TelegramBot):
        @property
        def custom_handlers(self):
            return (CommandHandler('get', self.get),)

        @property
        def help_message(self):
            return "Type /get [symbol] [exchange id (optional)] to get the latest price."

        def get(self, update, context):
            chat_id = update.effective_chat.id

            if len(context.args) == 1:
                symbol = context.args[0]
                exchange = 'binance'
            elif len(context.args) == 2:
                symbol = context.args[0]
                exchange = context.args[1]
            else:
                self.send_message(chat_id, "This command requires symbol and optionally exchange id.")
                return
            try:
                ticker = getattr(ccxt, exchange)().fetchTicker(symbol)
            except Exception as e:
                self.send_message(chat_id, str(e))
                return
            self.send_message(chat_id, str(ticker['last']))


    if __name__ == "__main__":
        bot = MyTelegramBot(token='1628351231:AAEgvZyRRfOV4_6ZArMS_lXzZd6XkG932zg')
        bot.start()
    ```
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: LogHandler
```
Handler to log user updates.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
```
Telegram bot.

See [Extensions – Your first Bot](https://github.com/python-telegram-bot/python-telegram-bot/wiki/Extensions-%E2%80%93-Your-first-Bot).

Keyword arguments are passed to `TelegramBot.build_application`.

!!! note
    If you get "RuntimeError: Cannot close a running event loop" when running in Jupyter,
    you might need to patch `asyncio` using https://github.com/erdewit/nest_asyncio, like this
    (right before `bot.start()` in each new runtime)

    ```pycon
    >>> !pip install nest_asyncio
    >>> import nest_asyncio
    >>> nest_asyncio.apply()
    ```

Usage:
    Let's extend `TelegramBot` to track cryptocurrency prices, as a Python script:

    ```python
    import ccxt
    import logging
    from vectorbtpro import *

    from telegram.ext import CommandHandler
    from telegram import __version__ as TG_VER

    try:
        from telegram import __version_info__
    except ImportError:
        __version_info__ = (0, 0, 0, 0, 0)

    if __version_info__ < (20, 0, 0, "alpha", 1):
        raise RuntimeError(f"This example is not compatible with your current PTB version {TG_VER}")

    # Enable logging
    logging.basicConfig(
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", level=logging.INFO
    )
    logger = logging.getLogger(__name__)


    class MyTelegramBot(vbt.TelegramBot):
        @property
        def custom_handlers(self):
            return (CommandHandler('get', self.get),)

        @property
        def help_message(self):
            return "Type /get [symbol] [exchange id (optional)] to get the latest price."

        async def get(self, update, context):
            chat_id = update.effective_chat.id

            if len(context.args) == 1:
                symbol = context.args[0]
                exchange = 'binance'
            elif len(context.args) == 2:
                symbol = context.args[0]
                exchange = context.args[1]
            else:
                await self.send_message(chat_id, "This command requires symbol and optionally exchange id.")
                return
            try:
                ticker = getattr(ccxt, exchange)().fetchTicker(symbol)
            except Exception as e:
                await self.send_message(chat_id, str(e))
                return
            await self.send_message(chat_id, str(ticker['last']))


    if __name__ == "__main__":
        bot = MyTelegramBot(token='YOUR_TOKEN')
        bot.start()
    ```
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: updater
```
Updater.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: dispatcher
```
Dispatcher.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: log_handler
```
Log handler.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: custom_handlers
```
Custom handlers to add.
Override to add custom handlers. Order counts.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: chat_ids
```
Chat ids that ever interacted with this bot.
A chat id is added upon receiving the "/start" command.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: start
```
Start the bot.
`**kwargs` are passed to `telegram.ext.updater.Updater.start_polling`
and override settings under `telegram` in `vectorbtpro._settings.messaging`.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: started_callback
```
Callback once the bot has been started.
Override to execute custom commands upon starting the bot.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: send
```
Send message of any kind to `chat_id`.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: send_to_all
```
Send message of any kind to all in `TelegramBot.chat_ids`.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: send_message
```
Send text message to `chat_id`.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: send_message_to_all
```
Send text message to all in `TelegramBot.chat_ids`.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: send_giphy
```
Send GIPHY from text to `chat_id`.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: send_giphy_to_all
```
Send GIPHY from text to all in `TelegramBot.chat_ids`.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: start_message
```
Message to be sent upon "/start" command.
Override to define your own message.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: start_callback
```
Start command callback.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: help_message
```
Message to be sent upon "/help" command.
Override to define your own message.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: help_callback
```
Help command callback.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: chat_migration_callback
```
Chat migration callback.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: unknown_callback
```
Unknown command callback.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: error_callback
```
Error callback.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: stop
```
Stop the bot.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: running
```
Whether the bot is running.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: build_application
```
Build application.

`**kwargs` override settings under `telegram` in `vectorbtpro._settings.messaging`
and those keys that can be found as attributes of `telegram.ext._applicationbuilder.ApplicationBuilder`
will be called on the builder. If any value is a dict, it will be unpacked to be able to
provide multiple keyword arguments to any method (apart from `defaults`, which is just one argument).
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: custom_handlers
```
Custom handlers to add.

Override to add custom handlers. Order counts.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: register_handlers
```
Register handlers.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: giphy_kwargs
```
Keyword arguments for GIPHY.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: application
```
Application.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: chat_ids
```
Chat ids that ever interacted with this bot.

A chat id is added upon receiving the "/start" command.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: log_handler
```
Log handler.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: start_message
```
Message to be sent upon "/start" command.

Override to define your own message.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: help_message
```
Message to be sent upon "/help" command.

Override to define your own message.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: loop
```
Event loop.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: start
```
Start the bot.

`**kwargs` override settings under `telegram` in `vectorbtpro._settings.messaging` and only those
keys that can be found in `telegram.ext._updater.Updater.start_polling` are passed.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: stop
```
Stop the bot.
```

---

# Pasta: messaging
### Arquivo: telegram.py
#### Classe: TelegramBot
#### Função: running
```
Whether the bot is running.
```

---

# Pasta: messaging
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules for messaging.
```

---

# Pasta: ohlcv
### Arquivo: accessors.py
#### Docstring do Módulo
```
Custom Pandas accessors for OHLC(V) data.

Methods can be accessed as follows:

* `OHLCVDFAccessor` -> `pd.DataFrame.vbt.ohlcv.*`

The accessors inherit `vectorbtpro.generic.accessors`.

!!! note
    Accessors do not utilize caching.

## Column names

By default, vectorbt searches for columns with names 'open', 'high', 'low', 'close', and 'volume'
(case doesn't matter). You can change the naming either using `feature_map` in
`vectorbtpro._settings.ohlcv`, or by providing `feature_map` directly to the accessor.

```pycon
>>> from vectorbtpro import *

>>> df = pd.DataFrame({
...     'my_open1': [2, 3, 4, 3.5, 2.5],
...     'my_high2': [3, 4, 4.5, 4, 3],
...     'my_low3': [1.5, 2.5, 3.5, 2.5, 1.5],
...     'my_close4': [2.5, 3.5, 4, 3, 2],
...     'my_volume5': [10, 11, 10, 9, 10]
... })
>>> df.vbt.ohlcv.get_feature('open')
None

>>> my_feature_map = {
...     "my_open1": "Open",
...     "my_high2": "High",
...     "my_low3": "Low",
...     "my_close4": "Close",
...     "my_volume5": "Volume",
... }
>>> ohlcv_acc = df.vbt.ohlcv(freq='d', feature_map=my_feature_map)
>>> ohlcv_acc.get_feature('open')
0    2.0
1    3.0
2    4.0
3    3.5
4    2.5
Name: my_open1, dtype: float64
```

## Stats

!!! hint
    See `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats` and `OHLCVDFAccessor.metrics`.

```pycon
>>> ohlcv_acc.stats()
Start                           0
End                             4
Period            5 days 00:00:00
First Price                   2.0
Lowest Price                  1.5
Highest Price                 4.5
Last Price                    2.0
First Volume                   10
Lowest Volume                   9
Highest Volume                 11
Last Volume                    10
Name: agg_stats, dtype: object
```

## Plots

!!! hint
    See `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots` and `OHLCVDFAccessor.subplots`.

`OHLCVDFAccessor` class has a single subplot based on `OHLCVDFAccessor.plot` (without volume):

```pycon
>>> ohlcv_acc.plots(settings=dict(ohlc_type='candlestick')).show()
```

![](/assets/images/api/ohlcv_plots.light.svg#only-light){: .iimg loading=lazy }
![](/assets/images/api/ohlcv_plots.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: ohlcv
### Arquivo: accessors.py
#### Classe: OHLCVDFAccessor
```
Accessor on top of OHLCV data. For DataFrames only.

Accessible via `pd.DataFrame.vbt.ohlcv`.
```

---

# Pasta: ohlcv
### Arquivo: accessors.py
#### Classe: OHLCVDFAccessor
#### Função: df_accessor_cls
```
Accessor class for `pd.DataFrame`.
```

---

# Pasta: ohlcv
### Arquivo: accessors.py
#### Classe: OHLCVDFAccessor
#### Função: feature_map
```
Column names.
```

---

# Pasta: ohlcv
### Arquivo: accessors.py
#### Classe: OHLCVDFAccessor
#### Função: to_data
```
Convert to a `vectorbtpro.data.base.Data` instance.
```

---

# Pasta: ohlcv
### Arquivo: accessors.py
#### Classe: OHLCVDFAccessor
#### Função: resample
```
Perform resampling on `OHLCVDFAccessor`.
```

---

# Pasta: ohlcv
### Arquivo: accessors.py
#### Classe: OHLCVDFAccessor
#### Função: stats_defaults
```
Defaults for `OHLCVDFAccessor.stats`.

Merges `vectorbtpro.generic.accessors.GenericAccessor.stats_defaults` and
`stats` from `vectorbtpro._settings.ohlcv`.
```

---

# Pasta: ohlcv
### Arquivo: accessors.py
#### Classe: OHLCVDFAccessor
#### Função: plot_ohlc
```
Plot OHLC data.

Args:
    ohlc_type: Either 'OHLC', 'Candlestick' or Plotly trace.

        Pass None to use the default.
    trace_kwargs (dict): Keyword arguments passed to `ohlc_type`.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.
```

---

# Pasta: ohlcv
### Arquivo: accessors.py
#### Classe: OHLCVDFAccessor
#### Função: plot_volume
```
Plot volume data.

Args:
    trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Bar`.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.
```

---

# Pasta: ohlcv
### Arquivo: accessors.py
#### Classe: OHLCVDFAccessor
#### Função: plot
```
Plot OHLC(V) data.

Args:
    ohlc_type: Either 'OHLC', 'Candlestick' or Plotly trace.

        Pass None to use the default.
    plot_volume (bool): Whether to plot volume beneath.
    ohlc_trace_kwargs (dict): Keyword arguments passed to `ohlc_type`.
    volume_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Bar`.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace` for OHLC.
    volume_add_trace_kwargs (dict): Keyword arguments passed to `add_trace` for volume.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> vbt.YFData.pull("BTC-USD").get().vbt.ohlcv.plot().show()
    ```

    [=100% "100%"]{: .candystripe .candystripe-animate }

    ![](/assets/images/api/ohlcv_plot.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/ohlcv_plot.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: ohlcv
### Arquivo: accessors.py
#### Classe: OHLCVDFAccessor
#### Função: plots_defaults
```
Defaults for `OHLCVDFAccessor.plots`.

Merges `vectorbtpro.generic.accessors.GenericAccessor.plots_defaults` and
`plots` from `vectorbtpro._settings.ohlcv`.
```

---

# Pasta: ohlcv
### Arquivo: nb.py
#### Docstring do Módulo
```
Numba-compiled functions for OHLCV.

!!! note
    vectorbt treats matrices as first-class citizens and expects input arrays to be
    2-dim, unless function has suffix `_1d` or is meant to be input to another function.
    Data is processed along index (axis 0).
```

---

# Pasta: ohlcv
### Arquivo: nb.py
#### Função: ohlc_every_1d_nb
```
Aggregate every `n` price points into an OHLC point.
```

---

# Pasta: ohlcv
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules for working with OHLC(V) data.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Docstring do Módulo
```
Base class for simulating a portfolio and measuring its performance.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Função: fix_wrapper_for_records
```
Allow flags for records that were restricted for portfolio.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Função: records_indexing_func
```
Apply indexing function on records.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Função: records_resample_func
```
Apply resampling function on records.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Função: returns_resample_func
```
Apply resampling function on returns.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: MetaInOutputs
```
Meta class that exposes a read-only class property `MetaFields.in_output_config`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: PortfolioWithInOutputs
```
Class exposes a read-only class property `RecordsWithFields.field_config`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
```
Class for simulating a portfolio and measuring its performance.

Args:
    wrapper (ArrayWrapper): Array wrapper.

        See `vectorbtpro.base.wrapping.ArrayWrapper`.
    close (array_like): Last asset price at each time step.
    order_records (array_like): A structured NumPy array of order records.
    open (array_like): Open price of each bar.
    high (array_like): High price of each bar.
    low (array_like): Low price of each bar.
    log_records (array_like): A structured NumPy array of log records.
    cash_sharing (bool): Whether to share cash within the same group.
    init_cash (InitCashMode or array_like of float): Initial capital.

        Can be provided in a format suitable for flexible indexing.
    init_position (array_like of float): Initial position.

        Can be provided in a format suitable for flexible indexing.
    init_price (array_like of float): Initial position price.

        Can be provided in a format suitable for flexible indexing.
    cash_deposits (array_like of float): Cash deposited/withdrawn at each timestamp.

        Can be provided in a format suitable for flexible indexing.
    cash_earnings (array_like of float): Earnings added at each timestamp.

        Can be provided in a format suitable for flexible indexing.
    sim_start (int, datetime_like, or array_like): Simulation start per column. Defaults to None.
    sim_end (int, datetime_like, or array_like): Simulation end per column. Defaults to None.
    call_seq (array_like of int): Sequence of calls per row and group. Defaults to None.
    in_outputs (namedtuple): Named tuple with in-output objects.

        To substitute `Portfolio` attributes, provide already broadcasted and grouped objects.
        Also see `Portfolio.in_outputs_indexing_func` on how in-output objects are indexed.
    use_in_outputs (bool): Whether to return in-output objects when calling properties.
    bm_close (array_like): Last benchmark asset price at each time step.
    fillna_close (bool): Whether to forward and backward fill NaN values in `close`.

        Applied after the simulation to avoid NaNs in asset value.

        See `Portfolio.get_filled_close`.
    weights (array_like): Asset weights.

        Applied to the initial position, initial cash, cash deposits, cash earnings, and orders.
    trades_type (str or int): Default `vectorbtpro.portfolio.trades.Trades` to use across `Portfolio`.

        See `vectorbtpro.portfolio.enums.TradesType`.
    orders_cls (type): Class for wrapping order records.
    logs_cls (type): Class for wrapping log records.
    trades_cls (type): Class for wrapping trade records.
    entry_trades_cls (type): Class for wrapping entry trade records.
    exit_trades_cls (type): Class for wrapping exit trade records.
    positions_cls (type): Class for wrapping position records.
    drawdowns_cls (type): Class for wrapping drawdown records.

For defaults, see `vectorbtpro._settings.portfolio`.

!!! note
    Use class methods with `from_` prefix to build a portfolio.
    The `__init__` method is reserved for indexing purposes.

!!! note
    This class is meant to be immutable. To change any attribute, use `Portfolio.replace`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: MetaInOutputs
#### Função: in_output_config
```
In-output config.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: PortfolioWithInOutputs
#### Função: in_output_config
```
In-output config of `${cls_name}`.

```python
${in_output_config}
```
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: row_stack_objs
```
Stack (two-dimensional) objects along rows.

`row_stack_func` must take the portfolio class, and all the arguments passed to this method.
If you don't need any of the arguments, make `row_stack_func` accept them as `**kwargs`.

If all the objects are None, boolean, or empty, returns the first one.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: row_stack_in_outputs
```
Stack `Portfolio.in_outputs` along rows.

All in-output tuples must be either None or have the same fields.

If the field can be found in the attributes of this `Portfolio` instance, reads the
attribute's options to get requirements for the type and layout of the in-output object.

For each field in `Portfolio.in_outputs`, resolves the field's options by parsing its name with
`Portfolio.parse_field_options` and also looks for options in `Portfolio.in_output_config`.
Performs stacking on the in-output objects of the same field using `Portfolio.row_stack_objs`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: row_stack
```
Stack multiple `Portfolio` instances along rows.

Uses `vectorbtpro.base.wrapping.ArrayWrapper.row_stack` to stack the wrappers.

Cash sharing must be the same among all objects.

Close, benchmark close, cash deposits, cash earnings, call sequence, and other two-dimensional arrays
are stacked using `vectorbtpro.base.wrapping.ArrayWrapper.row_stack_arrs`. In-outputs
are stacked using `Portfolio.row_stack_in_outputs`. Records are stacked using
`vectorbtpro.records.base.Records.row_stack_records_arrs`.

If the initial cash of each object is one of the options in `vectorbtpro.portfolio.enums.InitCashMode`,
it will be retained for the resulting object. Once any of the objects has the initial cash listed
as an absolute amount or an array, the initial cash of the first object will be copied over to
the final object, while the initial cash of all other objects will be resolved and used
as cash deposits, unless they all are zero. Set `combine_init_cash` to True to simply sum all
initial cash arrays.

If only the first object has an initial position greater than zero, it will be copied over to
the final object. Otherwise, an error will be thrown, unless `combine_init_position` is enabled
to sum all initial position arrays. The same goes for the initial price, which becomes
a candidate for stacking only if any of the arrays are not NaN.

!!! note
    When possible, avoid using initial position and price in objects to be stacked:
    there is currently no way of injecting them in the correct order, while simply taking
    the sum or weighted average may distort the reality since they weren't available
    prior to the actual simulation.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: column_stack_objs
```
Stack (one and two-dimensional) objects along column.

`column_stack_func` must take the portfolio class, and all the arguments passed to this method.
If you don't need any of the arguments, make `column_stack_func` accept them as `**kwargs`.

If all the objects are None, boolean, or empty, returns the first one.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: column_stack_in_outputs
```
Stack `Portfolio.in_outputs` along columns.

All in-output tuples must be either None or have the same fields.

If the field can be found in the attributes of this `Portfolio` instance, reads the
attribute's options to get requirements for the type and layout of the in-output object.

For each field in `Portfolio.in_outputs`, resolves the field's options by parsing its name with
`Portfolio.parse_field_options` and also looks for options in `Portfolio.in_output_config`.
Performs stacking on the in-output objects of the same field using `Portfolio.column_stack_objs`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: column_stack
```
Stack multiple `Portfolio` instances along columns.

Uses `vectorbtpro.base.wrapping.ArrayWrapper.column_stack` to stack the wrappers.

Cash sharing must be the same among all objects.

Two-dimensional arrays are stacked using
`vectorbtpro.base.wrapping.ArrayWrapper.column_stack_arrs`
while one-dimensional arrays are stacked using
`vectorbtpro.base.wrapping.ArrayWrapper.concat_arrs`.
In-outputs are stacked using `Portfolio.column_stack_in_outputs`. Records are stacked using
`vectorbtpro.records.base.Records.column_stack_records_arrs`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: in_output_config
```
In-output config of `${cls_name}`.

```python
${in_output_config}
```

Returns `${cls_name}._in_output_config`, which gets (hybrid-) copied upon creation of each instance.
Thus, changing this config won't affect the class.

To change in_outputs, you can either change the config in-place, override this property,
or overwrite the instance variable `${cls_name}._in_output_config`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: parse_field_options
```
Parse options based on the name of a field.

Returns a dictionary with the parsed grouping, object type, and cleaned field name.

Grouping is parsed by looking for the following suffixes:

* '_cs': per group if grouped with cash sharing, otherwise per column
* '_pcg': per group if grouped, otherwise per column
* '_pg': per group
* '_pc': per column
* '_records': records

Object type is parsed by looking for the following suffixes:

* '_2d': element per timestamp and column or group (time series)
* '_1d': element per column or group (reduced time series)

Those substrings are then removed to produce a clean field name.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: matches_field_options
```
Return whether options of a field match the requirements.

Requirements include the type of the object (array, reduced array, records),
the grouping of the object (1/2 dimensions, group/column-wise layout). The current
grouping and cash sharing of this portfolio object are also taken into account.

When an option is not in `options`, it's automatically marked as matching.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: wrap_obj
```
Wrap an object.

`wrap_func` must take the portfolio, `obj`, all the arguments passed to this method, and `**kwargs`.
If you don't need any of the arguments, make `indexing_func` accept them as `**kwargs`.

If the object is None or boolean, returns as-is.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_in_output
```
Find and wrap an in-output object matching the field.

If the field can be found in the attributes of this `Portfolio` instance, reads the
attribute's options to get requirements for the type and layout of the in-output object.

For each field in `Portfolio.in_outputs`, resolves the field's options by parsing its name with
`Portfolio.parse_field_options` and also looks for options in `Portfolio.in_output_config`.
If `field` is not in `Portfolio.in_outputs`, searches for the field in aliases and options.
In such case, to narrow down the number of candidates, options are additionally matched against
the requirements using `Portfolio.matches_field_options`. Finally, the matched in-output object is
wrapped using `Portfolio.wrap_obj`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: index_obj
```
Perform indexing on an object.

`indexing_func` must take the portfolio, all the arguments passed to this method, and `**kwargs`.
If you don't need any of the arguments, make `indexing_func` accept them as `**kwargs`.

If the object is None, boolean, or empty, returns as-is.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: in_outputs_indexing_func
```
Perform indexing on `Portfolio.in_outputs`.

If the field can be found in the attributes of this `Portfolio` instance, reads the
attribute's options to get requirements for the type and layout of the in-output object.

For each field in `Portfolio.in_outputs`, resolves the field's options by parsing its name with
`Portfolio.parse_field_options` and also looks for options in `Portfolio.in_output_config`.
Performs indexing on the in-output object using `Portfolio.index_obj`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: indexing_func
```
Perform indexing on `Portfolio`.

In-outputs are indexed using `Portfolio.in_outputs_indexing_func`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: resample_obj
```
Resample an object.

`resample_func` must take the portfolio, `obj`, `resampler`, all the arguments passed to this method,
and `**kwargs`. If you don't need any of the arguments, make `resample_func` accept them as `**kwargs`.
If `resample_func` is a string, will use it as `reduce_func_nb` in
`vectorbtpro.generic.accessors.GenericAccessor.resample_apply`. Default is 'last'.

If the object is None, boolean, or empty, returns as-is.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: resample_in_outputs
```
Resample `Portfolio.in_outputs`.

If the field can be found in the attributes of this `Portfolio` instance, reads the
attribute's options to get requirements for the type and layout of the in-output object.

For each field in `Portfolio.in_outputs`, resolves the field's options by parsing its name with
`Portfolio.parse_field_options` and also looks for options in `Portfolio.in_output_config`.
Performs indexing on the in-output object using `Portfolio.resample_obj`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: resample
```
Resample the `Portfolio` instance.

!!! warning
    Downsampling is associated with information loss:

    * Cash deposits and earnings are assumed to be added/removed at the beginning of each time step.
        Imagine depositing $100 and using them up in the same bar, and then depositing another $100
        and using them up. Downsampling both bars into a single bar will aggregate cash deposits
        and earnings, and put both of them at the beginning of the new bar, even though the second
        deposit was added later in time.
    * Market/benchmark returns are computed by applying the initial value on the close price
        of the first bar and by tracking the price change to simulate holding. Moving the close
        price of the first bar further into the future will affect this computation and almost
        certainly produce a different market value and returns. To mitigate this, make sure
        to downsample to an index with the first bar containing only the first bar from the
        origin timeframe.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: from_orders
```
Simulate portfolio from orders - size, price, fees, and other information.

See `vectorbtpro.portfolio.nb.from_orders.from_orders_nb`.

Prepared by `vectorbtpro.portfolio.preparing.FOPreparer`.

Args:
    close (array_like, OHLCDataMixin, FOPreparer, or PFPrepResult): Latest asset price at each time step.
        Will broadcast.

        Used for calculating unrealized PnL and portfolio value.

        If an instance of `vectorbtpro.data.base.OHLCDataMixin`, will extract the open, high, low, and close price.
        If an instance of `vectorbtpro.portfolio.preparing.FOPreparer`, will use it as a preparer.
        If an instance of `vectorbtpro.portfolio.preparing.PFPrepResult`, will use it as a preparer result.
    size (float or array_like): Size to order.
        See `vectorbtpro.portfolio.enums.Order.size`. Will broadcast.
    size_type (SizeType or array_like): See `vectorbtpro.portfolio.enums.SizeType` and
        `vectorbtpro.portfolio.enums.Order.size_type`. Will broadcast.
    direction (Direction or array_like): See `vectorbtpro.portfolio.enums.Direction` and
        `vectorbtpro.portfolio.enums.Order.direction`. Will broadcast.
    price (array_like of float): Order price.
        Will broadcast.

        See `vectorbtpro.portfolio.enums.Order.price`. Can be also provided as
        `vectorbtpro.portfolio.enums.PriceType`. Options `PriceType.NextOpen` and `PriceType.NextClose`
        are only applicable per column, that is, they cannot be used inside full arrays.
        In addition, they require the argument `from_ago` to be None.
    fees (float or array_like): Fees in percentage of the order value.
        See `vectorbtpro.portfolio.enums.Order.fees`. Will broadcast.
    fixed_fees (float or array_like): Fixed amount of fees to pay per order.
        See `vectorbtpro.portfolio.enums.Order.fixed_fees`. Will broadcast.
    slippage (float or array_like): Slippage in percentage of price.
        See `vectorbtpro.portfolio.enums.Order.slippage`. Will broadcast.
    min_size (float or array_like): Minimum size for an order to be accepted.
        See `vectorbtpro.portfolio.enums.Order.min_size`. Will broadcast.
    max_size (float or array_like): Maximum size for an order.
        See `vectorbtpro.portfolio.enums.Order.max_size`. Will broadcast.

        Will be partially filled if exceeded.
    size_granularity (float or array_like): Granularity of the size.
        See `vectorbtpro.portfolio.enums.Order.size_granularity`. Will broadcast.
    leverage (float or array_like): Leverage.
        See `vectorbtpro.portfolio.enums.Order.leverage`. Will broadcast.
    leverage_mode (LeverageMode or array_like): Leverage mode.
        See `vectorbtpro.portfolio.enums.Order.leverage_mode`. Will broadcast.
    reject_prob (float or array_like): Order rejection probability.
        See `vectorbtpro.portfolio.enums.Order.reject_prob`. Will broadcast.
    price_area_vio_mode (PriceAreaVioMode or array_like): See `vectorbtpro.portfolio.enums.PriceAreaVioMode`.
        Will broadcast.
    allow_partial (bool or array_like): Whether to allow partial fills.
        See `vectorbtpro.portfolio.enums.Order.allow_partial`. Will broadcast.

        Does not apply when size is `np.inf`.
    raise_reject (bool or array_like): Whether to raise an exception if order gets rejected.
        See `vectorbtpro.portfolio.enums.Order.raise_reject`. Will broadcast.
    log (bool or array_like): Whether to log orders.
        See `vectorbtpro.portfolio.enums.Order.log`. Will broadcast.
    val_price (array_like of float): Asset valuation price.
        Will broadcast.

        Can be also provided as `vectorbtpro.portfolio.enums.ValPriceType`.

        * Any `-np.inf` element is replaced by the latest valuation price
            (`open` or the latest known valuation price if `ffill_val_price`).
        * Any `np.inf` element is replaced by the current order price.

        Used at the time of decision making to calculate value of each asset in the group,
        for example, to convert target value into target amount.

        !!! note
            In contrast to `Portfolio.from_order_func`, order price is known beforehand (kind of),
            thus `val_price` is set to the current order price (using `np.inf`) by default.
            To valuate using previous close, set it in the settings to `-np.inf`.

        !!! note
            Make sure to use timestamp for `val_price` that comes before timestamps of
            all orders in the group with cash sharing (previous `close` for example),
            otherwise you're cheating yourself.
    open (array_like of float): First asset price at each time step.
        Defaults to `np.nan`. Will broadcast.

        Used as a price boundary (see `vectorbtpro.portfolio.enums.PriceArea`).
    high (array_like of float): Highest asset price at each time step.
        Defaults to `np.nan`. Will broadcast.

        Used as a price boundary (see `vectorbtpro.portfolio.enums.PriceArea`).
    low (array_like of float): Lowest asset price at each time step.
        Defaults to `np.nan`. Will broadcast.

        Used as a price boundary (see `vectorbtpro.portfolio.enums.PriceArea`).
    init_cash (InitCashMode, float or array_like): Initial capital.

        By default, will broadcast to the final number of columns.
        But if cash sharing is enabled, will broadcast to the number of groups.
        See `vectorbtpro.portfolio.enums.InitCashMode` to find optimal initial cash.

        !!! note
            Mode `InitCashMode.AutoAlign` is applied after the portfolio is initialized
            to set the same initial cash for all columns/groups. Changing grouping
            will change the initial cash, so be aware when indexing.
    init_position (float or array_like): Initial position.

        By default, will broadcast to the final number of columns.
    init_price (float or array_like): Initial position price.

        By default, will broadcast to the final number of columns.
    cash_deposits (float or array_like): Cash to be deposited/withdrawn at each timestamp.
        Will broadcast to the final shape. Must have the same number of columns as `init_cash`.

        Applied at the beginning of each timestamp.
    cash_earnings (float or array_like): Earnings in cash to be added at each timestamp.
        Will broadcast to the final shape.

        Applied at the end of each timestamp.
    cash_dividends (float or array_like): Dividends in cash to be added at each timestamp.
        Will broadcast to the final shape.

        Gets multiplied by the position and saved into `cash_earnings`.

        Applied at the end of each timestamp.
    cash_sharing (bool): Whether to share cash within the same group.

        If `group_by` is None and `cash_sharing` is True, `group_by` becomes True to form a single
        group with cash sharing.

        !!! warning
            Introduces cross-asset dependencies.

            This method presumes that in a group of assets that share the same capital all
            orders will be executed within the same tick and retain their price regardless
            of their position in the queue, even though they depend upon each other and thus
            cannot be executed in parallel.
    from_ago (int or array_like): Take order information from a number of bars ago.
        Will broadcast.

        Negative numbers will be cast to positive to avoid the look-ahead bias. Defaults to 0.
        Remember to account of it if you're using a custom signal function!
    sim_start (int, datetime_like, or array_like): Simulation start row or index (inclusive).

        Can be "auto", which will be substituted by the index of the first non-NA size value.
    sim_end (int, datetime_like, or array_like): Simulation end row or index (exclusive).

        Can be "auto", which will be substituted by the index of the first non-NA size value.
    call_seq (CallSeqType or array_like): Default sequence of calls per row and group.

        Each value in this sequence must indicate the position of column in the group to
        call next. Processing of `call_seq` goes always from left to right.
        For example, `[2, 0, 1]` would first call column 'c', then 'a', and finally 'b'.

        Supported are multiple options:

        * Set to None to generate the default call sequence on the fly. Will create a
            full array only if `attach_call_seq` is True.
        * Use `vectorbtpro.portfolio.enums.CallSeqType` to create a full array of a specific type.
        * Set to array to specify a custom call sequence.

        If `CallSeqType.Auto` selected, rearranges calls dynamically based on order value.
        Calculates value of all orders per row and group, and sorts them by this value.
        Sell orders will be executed first to release funds for buy orders.

        !!! warning
            `CallSeqType.Auto` should be used with caution:

            * It not only presumes that order prices are known beforehand, but also that
                orders can be executed in arbitrary order and still retain their price.
                In reality, this is hardly the case: after processing one asset, some time
                has passed and the price for other assets might have already changed.
            * Even if you're able to specify a slippage large enough to compensate for
                this behavior, slippage itself should depend upon execution order.
                This method doesn't let you do that.
            * Orders in the same queue are executed regardless of whether previous orders
                have been filled, which can leave them without required funds.

            For more control, use `Portfolio.from_order_func`.
    attach_call_seq (bool): Whether to attach `call_seq` to the instance.

        Makes sense if you want to analyze the simulation order. Otherwise, just takes memory.
    ffill_val_price (bool): Whether to track valuation price only if it's known.

        Otherwise, unknown `close` will lead to NaN in valuation price at the next timestamp.
    update_value (bool): Whether to update group value after each filled order.
    save_state (bool): Whether to save the state.

        The arrays will be available as `cash`, `position`, `debt`, `locked_cash`, and `free_cash` in in-outputs.
    save_value (bool): Whether to save the value.

        The array will be available as `value` in in-outputs.
    save_returns (bool): Whether to save the returns.

        The array will be available as `returns` in in-outputs.
    max_order_records (int): The max number of order records expected to be filled at each column.
        Defaults to the maximum number of non-NaN values across all columns of the size array.

        Set to a lower number if you run out of memory, and to 0 to not fill.
    max_log_records (int): The max number of log records expected to be filled at each column.
        Defaults to the maximum number of True values across all columns of the log array.

        Set to a lower number if you run out of memory, and to 0 to not fill.
    seed (int): Seed to be set for both `call_seq` and at the beginning of the simulation.
    group_by (any): Group columns. See `vectorbtpro.base.grouping.base.Grouper`.
    broadcast_kwargs (dict): Keyword arguments passed to `vectorbtpro.base.reshaping.broadcast`.
    jitted (any): See `vectorbtpro.utils.jitting.resolve_jitted_option`.
    chunked (any): See `vectorbtpro.utils.chunking.resolve_chunked_option`.
    bm_close (array_like): Latest benchmark price at each time step.
        Will broadcast.

        If not provided, will use `close`. If False, will not use any benchmark.
    records (array_like): Records to construct arrays from.

        See `vectorbtpro.base.indexing.IdxRecords`.
    return_preparer (bool): Whether to return the preparer of the type
        `vectorbtpro.portfolio.preparing.FOPreparer`.

        !!! note
            Seed won't be set in this case, you need to explicitly call `preparer.set_seed()`.
    return_prep_result (bool): Whether to return the preparer result of the type
        `vectorbtpro.portfolio.preparing.PFPrepResult`.
    return_sim_out (bool): Whether to return the simulation output of the type
        `vectorbtpro.portfolio.enums.SimulationOutput`.
    **kwargs: Keyword arguments passed to the `Portfolio` constructor.

All broadcastable arguments will broadcast using `vectorbtpro.base.reshaping.broadcast`
but keep original shape to utilize flexible indexing and to save memory.

For defaults, see `vectorbtpro._settings.portfolio`. Those defaults are not used to fill
NaN values after reindexing: vectorbt uses its own sensible defaults, which are usually NaN
for floating arrays and default flags for integer arrays. Use `vectorbtpro.base.reshaping.BCO`
with `fill_value` to override.

!!! note
    When `call_seq` is not `CallSeqType.Auto`, at each timestamp, processing of the assets in
    a group goes strictly in order defined in `call_seq`. This order can't be changed dynamically.

    This has one big implication for this particular method: the last asset in the call stack
    cannot be processed until other assets are processed. This is the reason why rebalancing
    cannot work properly in this setting: one has to specify percentages for all assets beforehand
    and then tweak the processing order to sell to-be-sold assets first in order to release funds
    for to-be-bought assets. This can be automatically done by using `CallSeqType.Auto`.

!!! hint
    All broadcastable arguments can be set per frame, series, row, column, or element.

Usage:
    * Buy 10 units each tick:

    ```pycon
    >>> close = pd.Series([1, 2, 3, 4, 5])
    >>> pf = vbt.Portfolio.from_orders(close, 10)

    >>> pf.assets
    0    10.0
    1    20.0
    2    30.0
    3    40.0
    4    40.0
    dtype: float64
    >>> pf.cash
    0    90.0
    1    70.0
    2    40.0
    3     0.0
    4     0.0
    dtype: float64
    ```

    * Reverse each position by first closing it:

    ```pycon
    >>> size = [1, 0, -1, 0, 1]
    >>> pf = vbt.Portfolio.from_orders(close, size, size_type='targetpercent')

    >>> pf.assets
    0    100.000000
    1      0.000000
    2    -66.666667
    3      0.000000
    4     26.666667
    dtype: float64
    >>> pf.cash
    0      0.000000
    1    200.000000
    2    400.000000
    3    133.333333
    4      0.000000
    dtype: float64
    ```

    * Regularly deposit cash at open and invest it within the same bar at close:

    ```pycon
    >>> close = pd.Series([1, 2, 3, 4, 5])
    >>> cash_deposits = pd.Series([10., 0., 10., 0., 10.])
    >>> pf = vbt.Portfolio.from_orders(
    ...     close,
    ...     size=cash_deposits,  # invest the amount deposited
    ...     size_type='value',
    ...     cash_deposits=cash_deposits
    ... )

    >>> pf.cash
    0    100.0
    1    100.0
    2    100.0
    3    100.0
    4    100.0
    dtype: float64

    >>> pf.asset_flow
    0    10.000000
    1     0.000000
    2     3.333333
    3     0.000000
    4     2.000000
    dtype: float64
    ```

    * Equal-weighted portfolio as in `vectorbtpro.portfolio.nb.from_order_func.from_order_func_nb` example
    (it's more compact but has less control over execution):

    ```pycon
    >>> np.random.seed(42)
    >>> close = pd.DataFrame(np.random.uniform(1, 10, size=(5, 3)))
    >>> size = pd.Series(np.full(5, 1/3))  # each column 33.3%
    >>> size[1::2] = np.nan  # skip every second tick

    >>> pf = vbt.Portfolio.from_orders(
    ...     close,  # acts both as reference and order price here
    ...     size,
    ...     size_type='targetpercent',
    ...     direction='longonly',
    ...     call_seq='auto',  # first sell then buy
    ...     group_by=True,  # one group
    ...     cash_sharing=True,  # assets share the same cash
    ...     fees=0.001, fixed_fees=1., slippage=0.001  # costs
    ... )

    >>> pf.get_asset_value(group_by=False).vbt.plot().show()
    ```

    ![](/assets/images/api/from_orders.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/from_orders.dark.svg#only-dark){: .iimg loading=lazy }

    * Test 10 random weight combinations:

    ```pycon
    >>> np.random.seed(42)
    >>> close = pd.DataFrame(
    ...     np.random.uniform(1, 10, size=(5, 3)),
    ...     columns=pd.Index(['a', 'b', 'c'], name='asset'))

    >>> # Generate random weight combinations
    >>> rand_weights = []
    >>> for i in range(10):
    ...     rand_weights.append(np.random.dirichlet(np.ones(close.shape[1]), size=1)[0])
    >>> rand_weights
    [array([0.15474873, 0.27706078, 0.5681905 ]),
     array([0.30468598, 0.18545189, 0.50986213]),
     array([0.15780486, 0.36292607, 0.47926907]),
     array([0.25697713, 0.64902589, 0.09399698]),
     array([0.43310548, 0.53836359, 0.02853093]),
     array([0.78628605, 0.15716865, 0.0565453 ]),
     array([0.37186671, 0.42150531, 0.20662798]),
     array([0.22441579, 0.06348919, 0.71209502]),
     array([0.41619664, 0.09338007, 0.49042329]),
     array([0.01279537, 0.87770864, 0.10949599])]

    >>> # Bring close and rand_weights to the same shape
    >>> rand_weights = np.concatenate(rand_weights)
    >>> close = close.vbt.tile(10, keys=pd.Index(np.arange(10), name='weights_vector'))
    >>> size = vbt.broadcast_to(weights, close).copy()
    >>> size[1::2] = np.nan
    >>> size
    weights_vector                            0  ...                               9
    asset                  a         b        c  ...           a         b         c
    0               0.154749  0.277061  0.56819  ...    0.012795  0.877709  0.109496
    1                    NaN       NaN      NaN  ...         NaN       NaN       NaN
    2               0.154749  0.277061  0.56819  ...    0.012795  0.877709  0.109496
    3                    NaN       NaN      NaN  ...         NaN       NaN       NaN
    4               0.154749  0.277061  0.56819  ...    0.012795  0.877709  0.109496

    [5 rows x 30 columns]

    >>> pf = vbt.Portfolio.from_orders(
    ...     close,
    ...     size,
    ...     size_type='targetpercent',
    ...     direction='longonly',
    ...     call_seq='auto',
    ...     group_by='weights_vector',  # group by column level
    ...     cash_sharing=True,
    ...     fees=0.001, fixed_fees=1., slippage=0.001
    ... )

    >>> pf.total_return
    weights_vector
    0   -0.294372
    1    0.139207
    2   -0.281739
    3    0.041242
    4    0.467566
    5    0.829925
    6    0.320672
    7   -0.087452
    8    0.376681
    9   -0.702773
    Name: total_return, dtype: float64
    ```
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: from_signals
```
Simulate portfolio from entry and exit signals.

Supports the following modes:

1. `entries` and `exits`:
    Uses `vectorbtpro.portfolio.nb.from_signals.dir_signal_func_nb` as `signal_func_nb`
    if an adjustment function is provided (not cacheable), otherwise translates signals using
    `vectorbtpro.portfolio.nb.from_signals.dir_to_ls_signals_nb` then simulates statically (cacheable)
2. `entries` (acting as long), `exits` (acting as long), `short_entries`, and `short_exits`:
    Uses `vectorbtpro.portfolio.nb.from_signals.ls_signal_func_nb` as `signal_func_nb`
    if an adjustment function is provided (not cacheable), otherwise simulates statically (cacheable)
3. `order_mode=True` without signals:
    Uses `vectorbtpro.portfolio.nb.from_signals.order_signal_func_nb` as `signal_func_nb` (not cacheable)
4. `signal_func_nb` and `signal_args`: Custom signal function (not cacheable)

Prepared by `vectorbtpro.portfolio.preparing.FSPreparer`.

Args:
    close (array_like, OHLCDataMixin, FSPreparer, or PFPrepResult): See `Portfolio.from_orders`.
    entries (array_like of bool): Boolean array of entry signals.
        Defaults to True if all other signal arrays are not set, otherwise False. Will broadcast.

        * If `short_entries` and `short_exits` are not set: Acts as a long signal if `direction`
            is 'all' or 'longonly', otherwise short.
        * If `short_entries` or `short_exits` are set: Acts as `long_entries`.
    exits (array_like of bool): Boolean array of exit signals.
        Defaults to False. Will broadcast.

        * If `short_entries` and `short_exits` are not set: Acts as a short signal if `direction`
            is 'all' or 'longonly', otherwise long.
        * If `short_entries` or `short_exits` are set: Acts as `long_exits`.
    direction (Direction or array_like): See `Portfolio.from_orders`.

        Takes only effect if `short_entries` and `short_exits` are not set.
    long_entries (array_like of bool): Boolean array of long entry signals.
        Defaults to False. Will broadcast.
    long_exits (array_like of bool): Boolean array of long exit signals.
        Defaults to False. Will broadcast.
    short_entries (array_like of bool): Boolean array of short entry signals.
        Defaults to False. Will broadcast.
    short_exits (array_like of bool): Boolean array of short exit signals.
        Defaults to False. Will broadcast.
    adjust_func_nb (path_like or callable): User-defined function to adjust the current simulation state.
        Defaults to `vectorbtpro.portfolio.nb.from_signals.no_adjust_func_nb`.

        Passed as argument to `vectorbtpro.portfolio.nb.from_signals.dir_signal_func_nb`,
        `vectorbtpro.portfolio.nb.from_signals.ls_signal_func_nb`, and
        `vectorbtpro.portfolio.nb.from_signals.order_signal_func_nb`. Has no effect
        when using other signal functions.

        Can be a path to a module when using staticizing.
    adjust_args (tuple): Packed arguments passed to `adjust_func_nb`.
    signal_func_nb (path_like or callable): Function called to generate signals.

        See `vectorbtpro.portfolio.nb.from_signals.from_signal_func_nb`.

        Can be a path to a module when using staticizing.
    signal_args (tuple): Packed arguments passed to `signal_func_nb`.
    post_segment_func_nb (path_like or callable): Post-segment function.

        See `vectorbtpro.portfolio.nb.from_signals.from_signal_func_nb`.

        Can be a path to a module when using staticizing.
    post_segment_args (tuple): Packed arguments passed to `post_segment_func_nb`.
    order_mode (bool): Whether to simulate as orders without signals.
    size (float or array_like): See `Portfolio.from_orders`.

        !!! note
            Negative size is not allowed. You must express direction using signals.
    size_type (SizeType or array_like): See `Portfolio.from_orders`.

        Only `SizeType.Amount`, `SizeType.Value`, `SizeType.Percent(100)`, and
        `SizeType.ValuePercent(100)` are supported. Other modes such as target percentage
        are not compatible with signals since their logic may contradict the direction of the signal.

        !!! note
            `SizeType.Percent(100)` does not support position reversal. Switch to a single
            direction or use `OppositeEntryMode.Close` to close the position first.

        See warning in `Portfolio.from_orders`.
    price (array_like of float): See `Portfolio.from_orders`.
    fees (float or array_like): See `Portfolio.from_orders`.
    fixed_fees (float or array_like): See `Portfolio.from_orders`.
    slippage (float or array_like): See `Portfolio.from_orders`.
    min_size (float or array_like): See `Portfolio.from_orders`.
    max_size (float or array_like): See `Portfolio.from_orders`.

        Will be partially filled if exceeded. You might not be able to properly close
        the position if accumulation is enabled and `max_size` is too low.
    size_granularity (float or array_like): See `Portfolio.from_orders`.
    leverage (float or array_like): See `Portfolio.from_orders`.
    leverage_mode (LeverageMode or array_like): See `Portfolio.from_orders`.
    reject_prob (float or array_like): See `Portfolio.from_orders`.
    price_area_vio_mode (PriceAreaVioMode or array_like): See `Portfolio.from_orders`.
    allow_partial (bool or array_like): See `Portfolio.from_orders`.
    raise_reject (bool or array_like): See `Portfolio.from_orders`.
    log (bool or array_like): See `Portfolio.from_orders`.
    val_price (array_like of float): See `Portfolio.from_orders`.
    accumulate (bool, AccumulationMode or array_like): See `vectorbtpro.portfolio.enums.AccumulationMode`.
        If True, becomes 'both'. If False, becomes 'disabled'. Will broadcast.

        When enabled, `Portfolio.from_signals` behaves similarly to `Portfolio.from_orders`.
    upon_long_conflict (ConflictMode or array_like): Conflict mode for long signals.
        See `vectorbtpro.portfolio.enums.ConflictMode`. Will broadcast.
    upon_short_conflict (ConflictMode or array_like): Conflict mode for short signals.
        See `vectorbtpro.portfolio.enums.ConflictMode`. Will broadcast.
    upon_dir_conflict (DirectionConflictMode or array_like): See `vectorbtpro.portfolio.enums.DirectionConflictMode`.
        Will broadcast.
    upon_opposite_entry (OppositeEntryMode or array_like): See `vectorbtpro.portfolio.enums.OppositeEntryMode`.
        Will broadcast.
    order_type (OrderType or array_like): See `vectorbtpro.portfolio.enums.OrderType`.

        Only one active limit order is allowed at a time.
    limit_delta (float or array_like): Delta from `price` to build the limit price.
        Will broadcast.

        If NaN, `price` becomes the limit price. Otherwise, applied on top of `price` depending
        on the current direction: if the direction-aware size is positive (= buying), a positive delta
        will decrease the limit price; if the direction-aware size is negative (= selling), a positive delta
        will increase the limit price. Delta can be negative.

        Set an element to `np.nan` to disable. Use `delta_format` to specify the format.
    limit_tif (frequency_like or array_like): Time in force for limit signals.
        Will broadcast.

        Any frequency-like object is converted using `vectorbtpro.utils.datetime_.to_timedelta64`.
        Any array must either contain timedeltas or integers, and will be cast into integer format
        after broadcasting. If the object provided is of data type `object`, will be converted
        to timedelta automatically.

        Measured in the distance after the open time of the signal bar. If the expiration time happens
        in the middle of the current bar, we pessimistically assume that the order has been expired.
        The check is performed at the beginning of the bar, and the first check is performed at the
        next bar after the signal. For example, if the format is `TimeDeltaFormat.Rows`, 0 or 1 means
        the order must execute at the same bar or not at all; 2 means the order must execute at the
        same or next bar or not at all.

        Set an element to `-1` to disable. Use `time_delta_format` to specify the format.
    limit_expiry (frequency_like, datetime_like, or array_like): Expiration time.
        Will broadcast.

        Any frequency-like object is used to build a period index, such that each timestamp in the original
        index is pointing to the timestamp where the period ends. For example, providing "d" will
        make any limit order expire on the next day. Any array must either contain timestamps or integers
        (not timedeltas!), and will be cast into integer format after broadcasting. If the object
        provided is of data type `object`, will be converted to datetime and its timezone will
        be removed automatically (as done on the index).

        Behaves in a similar way as `limit_tif`.

        Set an element to `-1` or `pd.Timestamp.max` to disable. Use `time_delta_format` to specify the format.
    limit_reverse (bool or array_like): Whether to reverse the price hit detection.
        Will broadcast.

        If True, a buy/sell limit price will be checked against high/low (not low/high).
        Also, the limit delta will be applied above/below (not below/above) the initial price.
    limit_order_price (LimitOrderPrice or array_like): See `vectorbtpro.portfolio.enums.LimitOrderPrice`.
        Will broadcast.

        If provided on per-element basis, gets applied upon order creation. If a positive value is provided,
        used directly as a price, otherwise used as an enumerated value.
    upon_adj_limit_conflict (PendingConflictMode or array_like): Conflict mode for limit and user-defined
        signals of adjacent sign. See `vectorbtpro.portfolio.enums.PendingConflictMode`. Will broadcast.
    upon_opp_limit_conflict (PendingConflictMode or array_like): Conflict mode for limit and user-defined
        signals of opposite sign. See `vectorbtpro.portfolio.enums.PendingConflictMode`. Will broadcast.
    use_stops (bool): Whether to use stops.
        Defaults to None, which becomes True if any of the stops are not NaN or
        the adjustment function is not the default one.

        Disable this to make simulation a bit faster for simple use cases.
    stop_ladder (bool or StopLadderMode): Whether and which kind of stop laddering to use.
        See `vectorbtpro.portfolio.enums.StopLadderMode`.

        If so, rows in the supplied arrays will become ladder steps. Make sure that
        they are increasing. If one column should have less steps, pad it with NaN
        for price-based stops and -1 for time-based stops.

        Rows in each array can be of an arbitrary length but columns must broadcast against
        the number of columns in the data. Applied on all stop types.
    sl_stop (array_like of float): Stop loss.
        Will broadcast.

        Set an element to `np.nan` to disable. Use `delta_format` to specify the format.
    tsl_stop (array_like of float): Trailing stop loss for the trailing stop loss.
        Will broadcast.

        Set an element to `np.nan` to disable. Use `delta_format` to specify the format.
    tsl_th (array_like of float): Take profit threshold for the trailing stop loss.
        Will broadcast.

        Set an element to `np.nan` to disable. Use `delta_format` to specify the format.
    tp_stop (array_like of float): Take profit.
        Will broadcast.

        Set an element to `np.nan` to disable. Use `delta_format` to specify the format.
    td_stop (frequency_like or array_like): Timedelta-stop.
        Will broadcast.

        Set an element to `-1` to disable. Use `time_delta_format` to specify the format.
    dt_stop (frequency_like, datetime_like, or array_like): Datetime-stop.
        Will broadcast.

        Set an element to `-1` to disable. Use `time_delta_format` to specify the format.
    stop_entry_price (StopEntryPrice or array_like): See `vectorbtpro.portfolio.enums.StopEntryPrice`.
        Will broadcast.

        If provided on per-element basis, gets applied upon entry. If a positive value is provided,
        used directly as a price, otherwise used as an enumerated value.
    stop_exit_price (StopExitPrice or array_like): See `vectorbtpro.portfolio.enums.StopExitPrice`.
        Will broadcast.

        If provided on per-element basis, gets applied upon entry. If a positive value is provided,
        used directly as a price, otherwise used as an enumerated value.
    stop_exit_type (StopExitType or array_like): See `vectorbtpro.portfolio.enums.StopExitType`.
        Will broadcast.

        If provided on per-element basis, gets applied upon entry.
    stop_order_type (OrderType or array_like): Similar to `order_type` but for stop orders.
        Will broadcast.

        If provided on per-element basis, gets applied upon entry.
    stop_limit_delta (float or array_like): Similar to `limit_delta` but for stop orders.
        Will broadcast.
    upon_stop_update (StopUpdateMode or array_like): See `vectorbtpro.portfolio.enums.StopUpdateMode`.
        Will broadcast.

        Only has effect if accumulation is enabled.

        If provided on per-element basis, gets applied upon repeated entry.
    upon_adj_stop_conflict (PendingConflictMode or array_like): Conflict mode for stop and user-defined
        signals of adjacent sign. See `vectorbtpro.portfolio.enums.PendingConflictMode`. Will broadcast.
    upon_opp_stop_conflict (PendingConflictMode or array_like): Conflict mode for stop and user-defined
        signals of opposite sign. See `vectorbtpro.portfolio.enums.PendingConflictMode`. Will broadcast.
    delta_format (DeltaFormat or array_like): See `vectorbtpro.portfolio.enums.DeltaFormat`.
        Will broadcast.
    time_delta_format (TimeDeltaFormat or array_like): See `vectorbtpro.portfolio.enums.TimeDeltaFormat`.
        Will broadcast.
    open (array_like of float): See `Portfolio.from_orders`.

        For stop signals, `np.nan` gets replaced by `close`.
    high (array_like of float): See `Portfolio.from_orders`.

        For stop signals, `np.nan` replaced by the maximum out of `open` and `close`.
    low (array_like of float): See `Portfolio.from_orders`.

        For stop signals, `np.nan` replaced by the minimum out of `open` and `close`.
    init_cash (InitCashMode, float or array_like): See `Portfolio.from_orders`.
    init_position (float or array_like): See `Portfolio.from_orders`.
    init_price (float or array_like): See `Portfolio.from_orders`.
    cash_deposits (float or array_like): See `Portfolio.from_orders`.
    cash_earnings (float or array_like): See `Portfolio.from_orders`.
    cash_dividends (float or array_like): See `Portfolio.from_orders`.
    cash_sharing (bool): See `Portfolio.from_orders`.
    from_ago (int or array_like): See `Portfolio.from_orders`.

        Take effect only for user-defined signals, not for stop signals.
    sim_start (int, datetime_like, or array_like): Simulation start row or index (inclusive).

        Can be "auto", which will be substituted by the index of the first signal across
        long and short entries and long and short exits.
    sim_end (int, datetime_like, or array_like): Simulation end row or index (exclusive).

        Can be "auto", which will be substituted by the index of the last signal across
        long and short entries and long and short exits.
    call_seq (CallSeqType or array_like): See `Portfolio.from_orders`.
    attach_call_seq (bool): See `Portfolio.from_orders`.
    ffill_val_price (bool): See `Portfolio.from_orders`.
    update_value (bool): See `Portfolio.from_orders`.
    fill_pos_info (bool): fill_pos_info (bool): Whether to fill position record.

        Disable this to make simulation faster for simple use cases.
    save_state (bool): See `Portfolio.from_orders`.
    save_value (bool): See `Portfolio.from_orders`.
    save_returns (bool): See `Portfolio.from_orders`.
    max_order_records (int): See `Portfolio.from_orders`.
    max_log_records (int): See `Portfolio.from_orders`.
    in_outputs (mapping_like): Mapping with in-output objects. Only for flexible mode.

        Will be available via `Portfolio.in_outputs` as a named tuple.

        To substitute `Portfolio` attributes, provide already broadcasted and grouped objects,
        for example, by using `broadcast_named_args` and templates. Also see
        `Portfolio.in_outputs_indexing_func` on how in-output objects are indexed.

        When chunking, make sure to provide the chunk taking specification and the merging function.
        See `vectorbtpro.portfolio.chunking.merge_sim_outs`.

        !!! note
            When using Numba below 0.54, `in_outputs` cannot be a mapping, but must be a named tuple
            defined globally so Numba can introspect its attributes for pickling.
    seed (int): See `Portfolio.from_orders`.
    group_by (any): See `Portfolio.from_orders`.
    broadcast_named_args (dict): Dictionary with named arguments to broadcast.

        You can then pass argument names wrapped with `vectorbtpro.utils.template.Rep`
        and this method will substitute them by their corresponding broadcasted objects.
    broadcast_kwargs (dict): See `Portfolio.from_orders`.
    template_context (mapping): Context used to substitute templates in arguments.
    jitted (any): See `Portfolio.from_orders`.
    chunked (any): See `Portfolio.from_orders`.
    staticized (bool, dict, hashable, or callable): Keyword arguments or task id for staticizing.

        If True or dictionary, will be passed as keyword arguments to
        `vectorbtpro.utils.cutting.cut_and_save_func` to save a cacheable version of the
        simulator to a file. If a hashable or callable, will be used as a task id of an
        already registered jittable and chunkable simulator. Dictionary allows additional options
        `override` and `reload` to override and reload an already existing module respectively.
    bm_close (array_like): See `Portfolio.from_orders`.
    records (array_like): See `Portfolio.from_orders`.
    return_preparer (bool): See `Portfolio.from_orders`.
    return_prep_result (bool): See `Portfolio.from_orders`.
    return_sim_out (bool): See `Portfolio.from_orders`.
    **kwargs: Keyword arguments passed to the `Portfolio` constructor.

All broadcastable arguments will broadcast using `vectorbtpro.base.reshaping.broadcast`
but keep original shape to utilize flexible indexing and to save memory.

For defaults, see `vectorbtpro._settings.portfolio`. Those defaults are not used to fill
NaN values after reindexing: vectorbt uses its own sensible defaults, which are usually NaN
for floating arrays and default flags for integer arrays. Use `vectorbtpro.base.reshaping.BCO`
with `fill_value` to override.

Also see notes and hints for `Portfolio.from_orders`.

Usage:
    * By default, if all signal arrays are None, `entries` becomes True,
    which opens a position at the very first tick and does nothing else:

    ```pycon
    >>> close = pd.Series([1, 2, 3, 4, 5])
    >>> pf = vbt.Portfolio.from_signals(close, size=1)
    >>> pf.asset_flow
    0    1.0
    1    0.0
    2    0.0
    3    0.0
    4    0.0
    dtype: float64
    ```

    * Entry opens long, exit closes long:

    ```pycon
    >>> pf = vbt.Portfolio.from_signals(
    ...     close,
    ...     entries=pd.Series([True, True, True, False, False]),
    ...     exits=pd.Series([False, False, True, True, True]),
    ...     size=1,
    ...     direction='longonly'
    ... )
    >>> pf.asset_flow
    0    1.0
    1    0.0
    2    0.0
    3   -1.0
    4    0.0
    dtype: float64

    >>> # Using direction-aware arrays instead of `direction`
    >>> pf = vbt.Portfolio.from_signals(
    ...     close,
    ...     entries=pd.Series([True, True, True, False, False]),  # long_entries
    ...     exits=pd.Series([False, False, True, True, True]),  # long_exits
    ...     short_entries=False,
    ...     short_exits=False,
    ...     size=1
    ... )
    >>> pf.asset_flow
    0    1.0
    1    0.0
    2    0.0
    3   -1.0
    4    0.0
    dtype: float64
    ```

    Notice how both `short_entries` and `short_exits` are provided as constants - as any other
    broadcastable argument, they are treated as arrays where each element is False.

    * Entry opens short, exit closes short:

    ```pycon
    >>> pf = vbt.Portfolio.from_signals(
    ...     close,
    ...     entries=pd.Series([True, True, True, False, False]),
    ...     exits=pd.Series([False, False, True, True, True]),
    ...     size=1,
    ...     direction='shortonly'
    ... )
    >>> pf.asset_flow
    0   -1.0
    1    0.0
    2    0.0
    3    1.0
    4    0.0
    dtype: float64

    >>> # Using direction-aware arrays instead of `direction`
    >>> pf = vbt.Portfolio.from_signals(
    ...     close,
    ...     entries=False,  # long_entries
    ...     exits=False,  # long_exits
    ...     short_entries=pd.Series([True, True, True, False, False]),
    ...     short_exits=pd.Series([False, False, True, True, True]),
    ...     size=1
    ... )
    >>> pf.asset_flow
    0   -1.0
    1    0.0
    2    0.0
    3    1.0
    4    0.0
    dtype: float64
    ```

    * Entry opens long and closes short, exit closes long and opens short:

    ```pycon
    >>> pf = vbt.Portfolio.from_signals(
    ...     close,
    ...     entries=pd.Series([True, True, True, False, False]),
    ...     exits=pd.Series([False, False, True, True, True]),
    ...     size=1,
    ...     direction='both'
    ... )
    >>> pf.asset_flow
    0    1.0
    1    0.0
    2    0.0
    3   -2.0
    4    0.0
    dtype: float64

    >>> # Using direction-aware arrays instead of `direction`
    >>> pf = vbt.Portfolio.from_signals(
    ...     close,
    ...     entries=pd.Series([True, True, True, False, False]),  # long_entries
    ...     exits=False,  # long_exits
    ...     short_entries=pd.Series([False, False, True, True, True]),
    ...     short_exits=False,
    ...     size=1
    ... )
    >>> pf.asset_flow
    0    1.0
    1    0.0
    2    0.0
    3   -2.0
    4    0.0
    dtype: float64
    ```

    * More complex signal combinations are best expressed using direction-aware arrays.
    For example, ignore opposite signals as long as the current position is open:

    ```pycon
    >>> pf = vbt.Portfolio.from_signals(
    ...     close,
    ...     entries      =pd.Series([True, False, False, False, False]),  # long_entries
    ...     exits        =pd.Series([False, False, True, False, False]),  # long_exits
    ...     short_entries=pd.Series([False, True, False, True, False]),
    ...     short_exits  =pd.Series([False, False, False, False, True]),
    ...     size=1,
    ...     upon_opposite_entry='ignore'
    ... )
    >>> pf.asset_flow
    0    1.0
    1    0.0
    2   -1.0
    3   -1.0
    4    1.0
    dtype: float64
    ```

    * First opposite signal closes the position, second one opens a new position:

    ```pycon
    >>> pf = vbt.Portfolio.from_signals(
    ...     close,
    ...     entries=pd.Series([True, True, True, False, False]),
    ...     exits=pd.Series([False, False, True, True, True]),
    ...     size=1,
    ...     direction='both',
    ...     upon_opposite_entry='close'
    ... )
    >>> pf.asset_flow
    0    1.0
    1    0.0
    2    0.0
    3   -1.0
    4   -1.0
    dtype: float64
    ```

    * If both long entry and exit signals are True (a signal conflict), choose exit:

    ```pycon
    >>> pf = vbt.Portfolio.from_signals(
    ...     close,
    ...     entries=pd.Series([True, True, True, False, False]),
    ...     exits=pd.Series([False, False, True, True, True]),
    ...     size=1.,
    ...     direction='longonly',
    ...     upon_long_conflict='exit')
    >>> pf.asset_flow
    0    1.0
    1    0.0
    2   -1.0
    3    0.0
    4    0.0
    dtype: float64
    ```

    * If both long entry and short entry signal are True (a direction conflict), choose short:

    ```pycon
    >>> pf = vbt.Portfolio.from_signals(
    ...     close,
    ...     entries=pd.Series([True, True, True, False, False]),
    ...     exits=pd.Series([False, False, True, True, True]),
    ...     size=1.,
    ...     direction='both',
    ...     upon_dir_conflict='short')
    >>> pf.asset_flow
    0    1.0
    1    0.0
    2   -2.0
    3    0.0
    4    0.0
    dtype: float64
    ```

    !!! note
        Remember that when direction is set to 'both', entries become `long_entries` and exits become
        `short_entries`, so this becomes a conflict of directions rather than signals.

    * If there are both signal and direction conflicts:

    ```pycon
    >>> pf = vbt.Portfolio.from_signals(
    ...     close,
    ...     entries=True,  # long_entries
    ...     exits=True,  # long_exits
    ...     short_entries=True,
    ...     short_exits=True,
    ...     size=1,
    ...     upon_long_conflict='entry',
    ...     upon_short_conflict='entry',
    ...     upon_dir_conflict='short'
    ... )
    >>> pf.asset_flow
    0   -1.0
    1    0.0
    2    0.0
    3    0.0
    4    0.0
    dtype: float64
    ```

    * Turn on accumulation of signals. Entry means long order, exit means short order
    (acts similar to `from_orders`):

    ```pycon
    >>> pf = vbt.Portfolio.from_signals(
    ...     close,
    ...     entries=pd.Series([True, True, True, False, False]),
    ...     exits=pd.Series([False, False, True, True, True]),
    ...     size=1.,
    ...     direction='both',
    ...     accumulate=True)
    >>> pf.asset_flow
    0    1.0
    1    1.0
    2    0.0
    3   -1.0
    4   -1.0
    dtype: float64
    ```

    * Allow increasing a position (of any direction), deny decreasing a position:

    ```pycon
    >>> pf = vbt.Portfolio.from_signals(
    ...     close,
    ...     entries=pd.Series([True, True, True, False, False]),
    ...     exits=pd.Series([False, False, True, True, True]),
    ...     size=1.,
    ...     direction='both',
    ...     accumulate='addonly')
    >>> pf.asset_flow
    0    1.0  << open a long position
    1    1.0  << add to the position
    2    0.0
    3   -3.0  << close and open a short position
    4   -1.0  << add to the position
    dtype: float64
    ```

    * Test multiple parameters via regular broadcasting:

    ```pycon
    >>> pf = vbt.Portfolio.from_signals(
    ...     close,
    ...     entries=pd.Series([True, True, True, False, False]),
    ...     exits=pd.Series([False, False, True, True, True]),
    ...     direction=[list(Direction)],
    ...     broadcast_kwargs=dict(columns_from=pd.Index(vbt.pf_enums.Direction._fields, name='direction')))
    >>> pf.asset_flow
    direction  LongOnly  ShortOnly   Both
    0             100.0     -100.0  100.0
    1               0.0        0.0    0.0
    2               0.0        0.0    0.0
    3            -100.0       50.0 -200.0
    4               0.0        0.0    0.0
    ```

    * Test multiple parameters via `vectorbtpro.base.reshaping.BCO`:

    ```pycon
    >>> pf = vbt.Portfolio.from_signals(
    ...     close,
    ...     entries=pd.Series([True, True, True, False, False]),
    ...     exits=pd.Series([False, False, True, True, True]),
    ...     direction=vbt.Param(Direction))
    >>> pf.asset_flow
    direction  LongOnly  ShortOnly   Both
    0             100.0     -100.0  100.0
    1               0.0        0.0    0.0
    2               0.0        0.0    0.0
    3            -100.0       50.0 -200.0
    4               0.0        0.0    0.0
    ```

    * Set risk/reward ratio by passing trailing stop loss and take profit thresholds:

    ```pycon
    >>> close = pd.Series([10, 11, 12, 11, 10, 9])
    >>> entries = pd.Series([True, False, False, False, False, False])
    >>> exits = pd.Series([False, False, False, False, False, True])
    >>> pf = vbt.Portfolio.from_signals(
    ...     close, entries, exits,
    ...     tsl_stop=0.1, tp_stop=0.2)  # take profit hit
    >>> pf.asset_flow
    0    10.0
    1     0.0
    2   -10.0
    3     0.0
    4     0.0
    5     0.0
    dtype: float64

    >>> pf = vbt.Portfolio.from_signals(
    ...     close, entries, exits,
    ...     tsl_stop=0.1, tp_stop=0.3)  # trailing stop loss hit
    >>> pf.asset_flow
    0    10.0
    1     0.0
    2     0.0
    3     0.0
    4   -10.0
    5     0.0
    dtype: float64

    >>> pf = vbt.Portfolio.from_signals(
    ...     close, entries, exits,
    ...     tsl_stop=np.inf, tp_stop=np.inf)  # nothing hit, exit as usual
    >>> pf.asset_flow
    0    10.0
    1     0.0
    2     0.0
    3     0.0
    4     0.0
    5   -10.0
    dtype: float64
    ```

    * Test different stop combinations:

    ```pycon
    >>> pf = vbt.Portfolio.from_signals(
    ...     close, entries, exits,
    ...     tsl_stop=vbt.Param([0.1, 0.2]),
    ...     tp_stop=vbt.Param([0.2, 0.3])
    ... )
    >>> pf.asset_flow
    tsl_stop   0.1         0.2
    tp_stop    0.2   0.3   0.2   0.3
    0         10.0  10.0  10.0  10.0
    1          0.0   0.0   0.0   0.0
    2        -10.0   0.0 -10.0   0.0
    3          0.0   0.0   0.0   0.0
    4          0.0 -10.0   0.0   0.0
    5          0.0   0.0   0.0 -10.0
    ```

    This works because `pd.Index` automatically translates into `vectorbtpro.base.reshaping.BCO`
    with `product` set to True.

    * We can implement our own stop loss or take profit, or adjust the existing one at each time step.
    Let's implement [stepped stop-loss](https://www.freqtrade.io/en/stable/strategy-advanced/#stepped-stoploss):

    ```pycon
    >>> @njit
    ... def adjust_func_nb(c):
    ...     val_price_now = c.last_val_price[c.col]
    ...     tsl_init_price = c.last_tsl_info["init_price"][c.col]
    ...     current_profit = (val_price_now - tsl_init_price) / tsl_init_price
    ...     if current_profit >= 0.40:
    ...         c.last_tsl_info["stop"][c.col] = 0.25
    ...     elif current_profit >= 0.25:
    ...         c.last_tsl_info["stop"][c.col] = 0.15
    ...     elif current_profit >= 0.20:
    ...         c.last_tsl_info["stop"][c.col] = 0.07

    >>> close = pd.Series([10, 11, 12, 11, 10])
    >>> pf = vbt.Portfolio.from_signals(close, adjust_func_nb=adjust_func_nb)
    >>> pf.asset_flow
    0    10.0
    1     0.0
    2     0.0
    3   -10.0  # 7% from 12 hit
    4    11.16
    dtype: float64
    ```

    * Sometimes there is a need to provide or transform signals dynamically. For this, we can implement
    a custom signal function `signal_func_nb`. For example, let's implement a signal function that
    takes two numerical arrays - long and short one - and transforms them into 4 direction-aware boolean
    arrays that vectorbt understands:

    ```pycon
    >>> @njit
    ... def signal_func_nb(c, long_num_arr, short_num_arr):
    ...     long_num = vbt.pf_nb.select_nb(c, long_num_arr)
    ...     short_num = vbt.pf_nb.select_nb(c, short_num_arr)
    ...     is_long_entry = long_num > 0
    ...     is_long_exit = long_num < 0
    ...     is_short_entry = short_num > 0
    ...     is_short_exit = short_num < 0
    ...     return is_long_entry, is_long_exit, is_short_entry, is_short_exit

    >>> pf = vbt.Portfolio.from_signals(
    ...     pd.Series([1, 2, 3, 4, 5]),
    ...     signal_func_nb=signal_func_nb,
    ...     signal_args=(vbt.Rep('long_num_arr'), vbt.Rep('short_num_arr')),
    ...     broadcast_named_args=dict(
    ...         long_num_arr=pd.Series([1, 0, -1, 0, 0]),
    ...         short_num_arr=pd.Series([0, 1, 0, 1, -1])
    ...     ),
    ...     size=1,
    ...     upon_opposite_entry='ignore'
    ... )
    >>> pf.asset_flow
    0    1.0
    1    0.0
    2   -1.0
    3   -1.0
    4    1.0
    dtype: float64
    ```

    Passing both arrays as `broadcast_named_args` broadcasts them internally as any other array,
    so we don't have to worry about their dimensions every time we change our data.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: from_holding
```
Simulate portfolio from plain holding using signals.

If `close_at_end` is True, will place an opposite signal at the very end.

`**kwargs` are passed to the class method `Portfolio.from_signals`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: from_random_signals
```
Simulate portfolio from random entry and exit signals.

Generates signals based either on the number of signals `n` or the probability
of encountering a signal `prob`.

* If `n` is set, see `vectorbtpro.signals.generators.randnx.RANDNX`.
* If `prob` is set, see `vectorbtpro.signals.generators.rprobnx.RPROBNX`.

Based on `Portfolio.from_signals`.

!!! note
    To generate random signals, the shape of `close` is used. Broadcasting with other
    arrays happens after the generation.

Usage:
    * Test multiple combinations of random entries and exits:

    ```pycon
    >>> close = pd.Series([1, 2, 3, 4, 5])
    >>> pf = vbt.Portfolio.from_random_signals(close, n=[2, 1, 0], seed=42)
    >>> pf.orders.count()
    randnx_n
    2    4
    1    2
    0    0
    Name: count, dtype: int64
    ```

    * Test the Cartesian product of entry and exit encounter probabilities:

    ```pycon
    >>> pf = vbt.Portfolio.from_random_signals(
    ...     close,
    ...     entry_prob=[0, 0.5, 1],
    ...     exit_prob=[0, 0.5, 1],
    ...     param_product=True,
    ...     seed=42)
    >>> pf.orders.count()
    rprobnx_entry_prob  rprobnx_exit_prob
    0.0                 0.0                  0
                        0.5                  0
                        1.0                  0
    0.5                 0.0                  1
                        0.5                  4
                        1.0                  3
    1.0                 0.0                  1
                        0.5                  4
                        1.0                  5
    Name: count, dtype: int64
    ```
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: from_optimizer
```
Build portfolio from an optimizer of type `vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer`.

Uses `Portfolio.from_orders` as the base simulation method.

The size type is 'targetpercent'. If there are positive and negative values, the direction
is automatically set to 'both', otherwise to 'longonly' for positive-only and `shortonly`
for negative-only values. Also, the cash sharing is set to True, the call sequence is set
to 'auto', and the grouper is set to the grouper of the optimizer by default.

Usage:
    ```pycon
    >>> close = pd.DataFrame({
    ...     "MSFT": [1, 2, 3, 4, 5],
    ...     "GOOG": [5, 4, 3, 2, 1],
    ...     "AAPL": [1, 2, 3, 2, 1]
    ... }, index=pd.date_range(start="2020-01-01", periods=5))

    >>> pfo = vbt.PortfolioOptimizer.from_random(
    ...     close.vbt.wrapper,
    ...     every="2D",
    ...     seed=42
    ... )
    >>> pfo.fill_allocations()
                     MSFT      GOOG      AAPL
    2020-01-01   0.182059  0.462129  0.355812
    2020-01-02        NaN       NaN       NaN
    2020-01-03   0.657381  0.171323  0.171296
    2020-01-04        NaN       NaN       NaN
    2020-01-05   0.038078  0.567845  0.394077

    >>> pf = vbt.Portfolio.from_optimizer(close, pfo)
    >>> pf.get_asset_value(group_by=False).vbt / pf.value
    alloc_group                         group
                     MSFT      GOOG      AAPL
    2020-01-01   0.182059  0.462129  0.355812  << rebalanced
    2020-01-02   0.251907  0.255771  0.492322
    2020-01-03   0.657381  0.171323  0.171296  << rebalanced
    2020-01-04   0.793277  0.103369  0.103353
    2020-01-05   0.038078  0.567845  0.394077  << rebalanced
    ```
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: from_order_func
```
Build portfolio from a custom order function.

!!! hint
    See `vectorbtpro.portfolio.nb.from_order_func.from_order_func_nb` for illustrations and argument definitions.

For more details on individual simulation functions:

* `order_func_nb`: See `vectorbtpro.portfolio.nb.from_order_func.from_order_func_nb`
* `order_func_nb` and `row_wise`: See `vectorbtpro.portfolio.nb.from_order_func.from_order_func_rw_nb`
* `flex_order_func_nb`: See `vectorbtpro.portfolio.nb.from_order_func.from_flex_order_func_nb`
* `flex_order_func_nb` and `row_wise`: See `vectorbtpro.portfolio.nb.from_order_func.from_flex_order_func_rw_nb`

Prepared by `vectorbtpro.portfolio.preparing.FOFPreparer`.

Args:
    close (array_like, OHLCDataMixin, FOFPreparer, or PFPrepResult): Latest asset price at each time step.
        Will broadcast.

        If an instance of `vectorbtpro.data.base.OHLCDataMixin`, will extract the open, high,
        low, and close price.

        Used for calculating unrealized PnL and portfolio value.
    init_cash (InitCashMode, float or array_like): See `Portfolio.from_orders`.
    init_position (float or array_like): See `Portfolio.from_orders`.
    init_price (float or array_like): See `Portfolio.from_orders`.
    cash_deposits (float or array_like): See `Portfolio.from_orders`.
    cash_earnings (float or array_like): See `Portfolio.from_orders`.
    cash_sharing (bool): Whether to share cash within the same group.

        If `group_by` is None, `group_by` becomes True to form a single group with cash sharing.
    sim_start (int, datetime_like, or array_like): Simulation start row or index (inclusive).
    sim_end (int, datetime_like, or array_like): Simulation end row or index (exclusive).
    call_seq (CallSeqType or array_like): Default sequence of calls per row and group.

        * Use `vectorbtpro.portfolio.enums.CallSeqType` to select a sequence type.
        * Set to array to specify custom sequence. Will not broadcast.

        !!! note
            CallSeqType.Auto must be implemented manually.
            Use `vectorbtpro.portfolio.nb.from_order_func.sort_call_seq_1d_nb`
            or `vectorbtpro.portfolio.nb.from_order_func.sort_call_seq_out_1d_nb` in `pre_segment_func_nb`.
    attach_call_seq (bool): See `Portfolio.from_orders`.
    segment_mask (int or array_like of bool): Mask of whether a particular segment should be executed.

        Supplying an integer will activate every n-th row.
        Supplying a boolean or an array of boolean will broadcast to the number of rows and groups.

        Does not broadcast together with `close` and `broadcast_named_args`, only against the final shape.
    call_pre_segment (bool): Whether to call `pre_segment_func_nb` regardless of `segment_mask`.
    call_post_segment (bool): Whether to call `post_segment_func_nb` regardless of `segment_mask`.
    pre_sim_func_nb (callable): Function called before simulation.
        Defaults to `vectorbtpro.portfolio.nb.from_order_func.no_pre_func_nb`.
    pre_sim_args (tuple): Packed arguments passed to `pre_sim_func_nb`.
    post_sim_func_nb (callable): Function called after simulation.
        Defaults to `vectorbtpro.portfolio.nb.from_order_func.no_post_func_nb`.
    post_sim_args (tuple): Packed arguments passed to `post_sim_func_nb`.
    pre_group_func_nb (callable): Function called before each group.
        Defaults to `vectorbtpro.portfolio.nb.from_order_func.no_pre_func_nb`.

        Called only if `row_wise` is False.
    pre_group_args (tuple): Packed arguments passed to `pre_group_func_nb`.
    post_group_func_nb (callable): Function called after each group.
        Defaults to `vectorbtpro.portfolio.nb.from_order_func.no_post_func_nb`.

        Called only if `row_wise` is False.
    post_group_args (tuple): Packed arguments passed to `post_group_func_nb`.
    pre_row_func_nb (callable): Function called before each row.
        Defaults to `vectorbtpro.portfolio.nb.from_order_func.no_pre_func_nb`.

        Called only if `row_wise` is True.
    pre_row_args (tuple): Packed arguments passed to `pre_row_func_nb`.
    post_row_func_nb (callable): Function called after each row.
        Defaults to `vectorbtpro.portfolio.nb.from_order_func.no_post_func_nb`.

        Called only if `row_wise` is True.
    post_row_args (tuple): Packed arguments passed to `post_row_func_nb`.
    pre_segment_func_nb (callable): Function called before each segment.
        Defaults to `vectorbtpro.portfolio.nb.from_order_func.no_pre_func_nb`.
    pre_segment_args (tuple): Packed arguments passed to `pre_segment_func_nb`.
    post_segment_func_nb (callable): Function called after each segment.
        Defaults to `vectorbtpro.portfolio.nb.from_order_func.no_post_func_nb`.
    post_segment_args (tuple): Packed arguments passed to `post_segment_func_nb`.
    order_func_nb (callable): Order generation function.
    order_args: Packed arguments passed to `order_func_nb`.
    flex_order_func_nb (callable): Flexible order generation function.
    flex_order_args: Packed arguments passed to `flex_order_func_nb`.
    post_order_func_nb (callable): Callback that is called after the order has been processed.
    post_order_args (tuple): Packed arguments passed to `post_order_func_nb`.
    open (array_like of float): See `Portfolio.from_orders`.
    high (array_like of float): See `Portfolio.from_orders`.
    low (array_like of float): See `Portfolio.from_orders`.
    ffill_val_price (bool): Whether to track valuation price only if it's known.

        Otherwise, unknown `close` will lead to NaN in valuation price at the next timestamp.
    update_value (bool): Whether to update group value after each filled order.
    fill_pos_info (bool): Whether to fill position record.

        Disable this to make simulation faster for simple use cases.
    track_value (bool): Whether to track value metrics such as
        the current valuation price, value, and return.

        Disable this to make simulation faster for simple use cases.
    row_wise (bool): Whether to iterate over rows rather than columns/groups.
    max_order_records (int): The max number of order records expected to be filled at each column.
        Defaults to the number of rows in the broadcasted shape.

        Set to a lower number if you run out of memory, to 0 to not fill, and to a higher number
        if there are more than one order expected at each timestamp.
    max_log_records (int): The max number of log records expected to be filled at each column.
        Defaults to the number of rows in the broadcasted shape.

        Set to a lower number if you run out of memory, to 0 to not fill, and to a higher number
        if there are more than one order expected at each timestamp.
    in_outputs (mapping_like): Mapping with in-output objects.

        Will be available via `Portfolio.in_outputs` as a named tuple.

        To substitute `Portfolio` attributes, provide already broadcasted and grouped objects,
        for example, by using `broadcast_named_args` and templates. Also see
        `Portfolio.in_outputs_indexing_func` on how in-output objects are indexed.

        When chunking, make sure to provide the chunk taking specification and the merging function.
        See `vectorbtpro.portfolio.chunking.merge_sim_outs`.

        !!! note
            When using Numba below 0.54, `in_outputs` cannot be a mapping, but must be a named tuple
            defined globally so Numba can introspect its attributes for pickling.
    seed (int): See `Portfolio.from_orders`.
    group_by (any): See `Portfolio.from_orders`.
    broadcast_named_args (dict): See `Portfolio.from_signals`.
    broadcast_kwargs (dict): See `Portfolio.from_orders`.
    template_context (mapping): See `Portfolio.from_signals`.
    keep_inout_flex (bool): Whether to keep arrays that can be edited in-place raw when broadcasting.

        Disable this to be able to edit `segment_mask`, `cash_deposits`, and
        `cash_earnings` during the simulation.
    jitted (any): See `Portfolio.from_orders`.

        !!! note
            Disabling jitting will not disable jitter (such as Numba) on other functions,
            only on the main (simulation) function. If neccessary, you should ensure that every other
            function is not compiled as well. For example, when working with Numba, you can do this
            by using the `py_func` attribute of that function. Or, you can disable Numba
            entirely by running `os.environ['NUMBA_DISABLE_JIT'] = '1'` before importing vectorbtpro.

        !!! warning
            Parallelization assumes that groups are independent and there is no data flowing between them.
    chunked (any): See `vectorbtpro.utils.chunking.resolve_chunked_option`.
    staticized (bool, dict, hashable, or callable): Keyword arguments or task id for staticizing.

        If True or dictionary, will be passed as keyword arguments to
        `vectorbtpro.utils.cutting.cut_and_save_func` to save a cacheable version of the
        simulator to a file. If a hashable or callable, will be used as a task id of an
        already registered jittable and chunkable simulator. Dictionary allows additional options
        `override` and `reload` to override and reload an already existing module respectively.
    bm_close (array_like): See `Portfolio.from_orders`.
    records (array_like): See `Portfolio.from_orders`.
    return_preparer (bool): See `Portfolio.from_orders`.
    return_prep_result (bool): See `Portfolio.from_orders`.
    return_sim_out (bool): See `Portfolio.from_orders`.
    **kwargs: Keyword arguments passed to the `Portfolio` constructor.

For defaults, see `vectorbtpro._settings.portfolio`. Those defaults are not used to fill
NaN values after reindexing: vectorbt uses its own sensible defaults, which are usually NaN
for floating arrays and default flags for integer arrays. Use `vectorbtpro.base.reshaping.BCO`
with `fill_value` to override.

!!! note
    All passed functions must be Numba-compiled if Numba is enabled.

    Also see notes on `Portfolio.from_orders`.

!!! note
    In contrast to other methods, the valuation price is previous `close` instead of the order price
    since the price of an order is unknown before the call (which is more realistic by the way).
    You can still override the valuation price in `pre_segment_func_nb`.

Usage:
    * Buy 10 units each tick using closing price:

    ```pycon
    >>> @njit
    ... def order_func_nb(c, size):
    ...     return vbt.pf_nb.order_nb(size=size)

    >>> close = pd.Series([1, 2, 3, 4, 5])
    >>> pf = vbt.Portfolio.from_order_func(
    ...     close,
    ...     order_func_nb=order_func_nb,
    ...     order_args=(10,)
    ... )

    >>> pf.assets
    0    10.0
    1    20.0
    2    30.0
    3    40.0
    4    40.0
    dtype: float64
    >>> pf.cash
    0    90.0
    1    70.0
    2    40.0
    3     0.0
    4     0.0
    dtype: float64
    ```

    * Reverse each position by first closing it. Keep state of last position to determine
    which position to open next (just as an example, there are easier ways to do this):

    ```pycon
    >>> @njit
    ... def pre_group_func_nb(c):
    ...     last_pos_state = np.array([-1])
    ...     return (last_pos_state,)

    >>> @njit
    ... def order_func_nb(c, last_pos_state):
    ...     if c.position_now != 0:
    ...         return vbt.pf_nb.close_position_nb()
    ...
    ...     if last_pos_state[0] == 1:
    ...         size = -np.inf  # open short
    ...         last_pos_state[0] = -1
    ...     else:
    ...         size = np.inf  # open long
    ...         last_pos_state[0] = 1
    ...     return vbt.pf_nb.order_nb(size=size)

    >>> pf = vbt.Portfolio.from_order_func(
    ...     close,
    ...     order_func_nb=order_func_nb,
    ...     pre_group_func_nb=pre_group_func_nb
    ... )

    >>> pf.assets
    0    100.000000
    1      0.000000
    2    -66.666667
    3      0.000000
    4     26.666667
    dtype: float64
    >>> pf.cash
    0      0.000000
    1    200.000000
    2    400.000000
    3    133.333333
    4      0.000000
    dtype: float64
    ```

    * Equal-weighted portfolio as in the example under `vectorbtpro.portfolio.nb.from_order_func.from_order_func_nb`:

    ```pycon
    >>> @njit
    ... def pre_group_func_nb(c):
    ...     order_value_out = np.empty(c.group_len, dtype=np.float_)
    ...     return (order_value_out,)

    >>> @njit
    ... def pre_segment_func_nb(c, order_value_out, size, price, size_type, direction):
    ...     for col in range(c.from_col, c.to_col):
    ...         c.last_val_price[col] = vbt.pf_nb.select_from_col_nb(c, col, price)
    ...     vbt.pf_nb.sort_call_seq_nb(c, size, size_type, direction, order_value_out)
    ...     return ()

    >>> @njit
    ... def order_func_nb(c, size, price, size_type, direction, fees, fixed_fees, slippage):
    ...     return vbt.pf_nb.order_nb(
    ...         size=vbt.pf_nb.select_nb(c, size),
    ...         price=vbt.pf_nb.select_nb(c, price),
    ...         size_type=vbt.pf_nb.select_nb(c, size_type),
    ...         direction=vbt.pf_nb.select_nb(c, direction),
    ...         fees=vbt.pf_nb.select_nb(c, fees),
    ...         fixed_fees=vbt.pf_nb.select_nb(c, fixed_fees),
    ...         slippage=vbt.pf_nb.select_nb(c, slippage)
    ...     )

    >>> np.random.seed(42)
    >>> close = np.random.uniform(1, 10, size=(5, 3))
    >>> size_template = vbt.RepEval('np.array([[1 / group_lens[0]]])')

    >>> pf = vbt.Portfolio.from_order_func(
    ...     close,
    ...     order_func_nb=order_func_nb,
    ...     order_args=(
    ...         size_template,
    ...         vbt.Rep('price'),
    ...         vbt.Rep('size_type'),
    ...         vbt.Rep('direction'),
    ...         vbt.Rep('fees'),
    ...         vbt.Rep('fixed_fees'),
    ...         vbt.Rep('slippage'),
    ...     ),
    ...     segment_mask=2,  # rebalance every second tick
    ...     pre_group_func_nb=pre_group_func_nb,
    ...     pre_segment_func_nb=pre_segment_func_nb,
    ...     pre_segment_args=(
    ...         size_template,
    ...         vbt.Rep('price'),
    ...         vbt.Rep('size_type'),
    ...         vbt.Rep('direction')
    ...     ),
    ...     broadcast_named_args=dict(  # broadcast against each other
    ...         price=close,
    ...         size_type=vbt.pf_enums.SizeType.TargetPercent,
    ...         direction=vbt.pf_enums.Direction.LongOnly,
    ...         fees=0.001,
    ...         fixed_fees=1.,
    ...         slippage=0.001
    ...     ),
    ...     template_context=dict(np=np),  # required by size_template
    ...     cash_sharing=True, group_by=True,  # one group with cash sharing
    ... )

    >>> pf.get_asset_value(group_by=False).vbt.plot().show()
    ```

    ![](/assets/images/api/from_order_func.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/from_order_func.dark.svg#only-dark){: .iimg loading=lazy }

    Templates are a very powerful tool to prepare any custom arguments after they are broadcast and
    before they are passed to the simulation function. In the example above, we use `broadcast_named_args`
    to broadcast some arguments against each other and templates to pass those objects to callbacks.
    Additionally, we used an evaluation template to compute the size based on the number of assets in each group.

    You may ask: why should we bother using broadcasting and templates if we could just pass `size=1/3`?
    Because of flexibility those features provide: we can now pass whatever parameter combinations we want
    and it will work flawlessly. For example, to create two groups of equally-allocated positions,
    we need to change only two parameters:

    ```pycon
    >>> close = np.random.uniform(1, 10, size=(5, 6))  # 6 columns instead of 3
    >>> group_by = ['g1', 'g1', 'g1', 'g2', 'g2', 'g2']  # 2 groups instead of 1
    >>> # Replace close and group_by in the example above

    >>> pf['g1'].get_asset_value(group_by=False).vbt.plot().show()
    ```

    ![](/assets/images/api/from_order_func_g1.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/from_order_func_g1.dark.svg#only-dark){: .iimg loading=lazy }

    ```pycon
    >>> pf['g2'].get_asset_value(group_by=False).vbt.plot().show()
    ```

    ![](/assets/images/api/from_order_func_g2.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/from_order_func_g2.dark.svg#only-dark){: .iimg loading=lazy }

    * Combine multiple exit conditions. Exit early if the price hits some threshold before an actual exit:

    ```pycon
    >>> @njit
    ... def pre_sim_func_nb(c):
    ...     # We need to define stop price per column once
    ...     stop_price = np.full(c.target_shape[1], np.nan, dtype=np.float_)
    ...     return (stop_price,)

    >>> @njit
    ... def order_func_nb(c, stop_price, entries, exits, size):
    ...     # Select info related to this order
    ...     entry_now = vbt.pf_nb.select_nb(c, entries)
    ...     exit_now = vbt.pf_nb.select_nb(c, exits)
    ...     size_now = vbt.pf_nb.select_nb(c, size)
    ...     price_now = vbt.pf_nb.select_nb(c, c.close)
    ...     stop_price_now = stop_price[c.col]
    ...
    ...     # Our logic
    ...     if entry_now:
    ...         if c.position_now == 0:
    ...             return vbt.pf_nb.order_nb(
    ...                 size=size_now,
    ...                 price=price_now,
    ...                 direction=vbt.pf_enums.Direction.LongOnly)
    ...     elif exit_now or price_now >= stop_price_now:
    ...         if c.position_now > 0:
    ...             return vbt.pf_nb.order_nb(
    ...                 size=-size_now,
    ...                 price=price_now,
    ...                 direction=vbt.pf_enums.Direction.LongOnly)
    ...     return vbt.pf_enums.NoOrder

    >>> @njit
    ... def post_order_func_nb(c, stop_price, stop):
    ...     # Same broadcasting as for size
    ...     stop_now = vbt.pf_nb.select_nb(c, stop)
    ...
    ...     if c.order_result.status == vbt.pf_enums.OrderStatus.Filled:
    ...         if c.order_result.side == vbt.pf_enums.OrderSide.Buy:
    ...             # Position entered: Set stop condition
    ...             stop_price[c.col] = (1 + stop_now) * c.order_result.price
    ...         else:
    ...             # Position exited: Remove stop condition
    ...             stop_price[c.col] = np.nan

    >>> def simulate(close, entries, exits, size, stop):
    ...     return vbt.Portfolio.from_order_func(
    ...         close,
    ...         order_func_nb=order_func_nb,
    ...         order_args=(vbt.Rep('entries'), vbt.Rep('exits'), vbt.Rep('size')),
    ...         pre_sim_func_nb=pre_sim_func_nb,
    ...         post_order_func_nb=post_order_func_nb,
    ...         post_order_args=(vbt.Rep('stop'),),
    ...         broadcast_named_args=dict(  # broadcast against each other
    ...             entries=entries,
    ...             exits=exits,
    ...             size=size,
    ...             stop=stop
    ...         )
    ...     )

    >>> close = pd.Series([10, 11, 12, 13, 14])
    >>> entries = pd.Series([True, True, False, False, False])
    >>> exits = pd.Series([False, False, False, True, True])
    >>> simulate(close, entries, exits, np.inf, 0.1).asset_flow
    0    10.0
    1     0.0
    2   -10.0
    3     0.0
    4     0.0
    dtype: float64

    >>> simulate(close, entries, exits, np.inf, 0.2).asset_flow
    0    10.0
    1     0.0
    2   -10.0
    3     0.0
    4     0.0
    dtype: float64

    >>> simulate(close, entries, exits, np.inf, np.nan).asset_flow
    0    10.0
    1     0.0
    2     0.0
    3   -10.0
    4     0.0
    dtype: float64
    ```

    The reason why stop of 10% does not result in an order at the second time step is because
    it comes at the same time as entry, so it must wait until no entry is present.
    This can be changed by replacing the statement "elif" with "if", which would execute
    an exit regardless if an entry is present (similar to using `ConflictMode.Opposite` in
    `Portfolio.from_signals`).

    We can also test the parameter combinations above all at once (thanks to broadcasting
    using `vectorbtpro.base.reshaping.broadcast`):

    ```pycon
    >>> stop = pd.DataFrame([[0.1, 0.2, np.nan]])
    >>> simulate(close, entries, exits, np.inf, stop).asset_flow
          0     1     2
    0  10.0  10.0  10.0
    1   0.0   0.0   0.0
    2 -10.0 -10.0   0.0
    3   0.0   0.0 -10.0
    4   0.0   0.0   0.0
    ```

    Or much simpler using Cartesian product:

    ```pycon
    >>> stop = vbt.Param([0.1, 0.2, np.nan])
    >>> simulate(close, entries, exits, np.inf, stop).asset_flow
    threshold   0.1   0.2   NaN
    0          10.0  10.0  10.0
    1           0.0   0.0   0.0
    2         -10.0 -10.0   0.0
    3           0.0   0.0 -10.0
    4           0.0   0.0   0.0
    ```

    This works because `pd.Index` automatically translates into `vectorbtpro.base.reshaping.BCO`
    with `product` set to True.

    * Let's illustrate how to generate multiple orders per symbol and bar.
    For each bar, buy at open and sell at close:

    ```pycon
    >>> @njit
    ... def flex_order_func_nb(c, size):
    ...     if c.call_idx == 0:
    ...         return c.from_col, vbt.pf_nb.order_nb(size=size, price=c.open[c.i, c.from_col])
    ...     if c.call_idx == 1:
    ...         return c.from_col, vbt.pf_nb.close_position_nb(price=c.close[c.i, c.from_col])
    ...     return -1, vbt.pf_enums.NoOrder

    >>> open = pd.DataFrame({'a': [1, 2, 3], 'b': [4, 5, 6]})
    >>> close = pd.DataFrame({'a': [2, 3, 4], 'b': [3, 4, 5]})
    >>> size = 1
    >>> pf = vbt.Portfolio.from_order_func(
    ...     close,
    ...     flex_order_func_nb=flex_order_func_nb,
    ...     flex_order_args=(size,),
    ...     open=open,
    ...     max_order_records=close.shape[0] * 2
    ... )

    >>> pf.orders.readable
        Order Id Column  Timestamp  Size  Price  Fees  Side
    0          0      a          0   1.0    1.0   0.0   Buy
    1          1      a          0   1.0    2.0   0.0  Sell
    2          2      a          1   1.0    2.0   0.0   Buy
    3          3      a          1   1.0    3.0   0.0  Sell
    4          4      a          2   1.0    3.0   0.0   Buy
    5          5      a          2   1.0    4.0   0.0  Sell
    6          0      b          0   1.0    4.0   0.0   Buy
    7          1      b          0   1.0    3.0   0.0  Sell
    8          2      b          1   1.0    5.0   0.0   Buy
    9          3      b          1   1.0    4.0   0.0  Sell
    10         4      b          2   1.0    6.0   0.0   Buy
    11         5      b          2   1.0    5.0   0.0  Sell
    ```

    !!! warning
        Each bar is effectively a black box - we don't know how the price moves in-between.
        Since trades should come in an order that closely replicates that of the real world, the only
        pieces of information that always remain in the correct order are the opening and closing price.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: from_def_order_func
```
Build portfolio from the default order function.

Default order function takes size, price, fees, and other available information, and issues
an order at each column and time step. Additionally, it uses a segment preprocessing function
that overrides the valuation price and sorts the call sequence. This way, it behaves similarly to
`Portfolio.from_orders`, but allows injecting pre- and postprocessing functions to have more
control over the execution. It also knows how to chunk each argument. The only disadvantage is
that `Portfolio.from_orders` is more optimized towards performance (up to 5x).

If `flexible` is True:

* `pre_segment_func_nb` is `vectorbtpro.portfolio.nb.from_order_func.def_flex_pre_segment_func_nb`
* `flex_order_func_nb` is `vectorbtpro.portfolio.nb.from_order_func.def_flex_order_func_nb`

If `flexible` is False:

* `pre_segment_func_nb` is `vectorbtpro.portfolio.nb.from_order_func.def_pre_segment_func_nb`
* `order_func_nb` is `vectorbtpro.portfolio.nb.from_order_func.def_order_func_nb`

Prepared by `vectorbtpro.portfolio.preparing.FDOFPreparer`.

For details on other arguments, see `Portfolio.from_orders` and `Portfolio.from_order_func`.

Usage:
    * Working with `Portfolio.from_def_order_func` is a similar experience as working
    with `Portfolio.from_orders`:

    ```pycon
    >>> close = pd.Series([1, 2, 3, 4, 5])
    >>> pf = vbt.Portfolio.from_def_order_func(close, 10)

    >>> pf.assets
    0    10.0
    1    20.0
    2    30.0
    3    40.0
    4    40.0
    dtype: float64
    >>> pf.cash
    0    90.0
    1    70.0
    2    40.0
    3     0.0
    4     0.0
    dtype: float64
    ```

    * Equal-weighted portfolio as in the example under `Portfolio.from_order_func`
    but much less verbose and with asset value pre-computed during the simulation (= faster):

    ```pycon
    >>> np.random.seed(42)
    >>> close = np.random.uniform(1, 10, size=(5, 3))

    >>> @njit
    ... def post_segment_func_nb(c):
    ...     for col in range(c.from_col, c.to_col):
    ...         c.in_outputs.asset_value_pc[c.i, col] = c.last_position[col] * c.last_val_price[col]

    >>> pf = vbt.Portfolio.from_def_order_func(
    ...     close,
    ...     size=1/3,
    ...     size_type='targetpercent',
    ...     direction='longonly',
    ...     fees=0.001,
    ...     fixed_fees=1.,
    ...     slippage=0.001,
    ...     segment_mask=2,
    ...     cash_sharing=True,
    ...     group_by=True,
    ...     call_seq='auto',
    ...     post_segment_func_nb=post_segment_func_nb,
    ...     call_post_segment=True,
    ...     in_outputs=dict(asset_value_pc=vbt.RepEval('np.empty_like(close)'))
    ... )

    >>> asset_value = pf.wrapper.wrap(pf.in_outputs.asset_value_pc, group_by=False)
    >>> asset_value.vbt.plot().show()
    ```

    ![](/assets/images/api/from_def_order_func.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/from_def_order_func.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: regroup
```
Regroup this object.

See `vectorbtpro.base.wrapping.Wrapping.regroup`.

!!! note
    All cached objects will be lost.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: cash_sharing
```
Whether to share cash within the same group.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: in_outputs
```
Named tuple with in-output objects.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: use_in_outputs
```
Whether to return in-output objects when calling properties.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: fillna_close
```
Whether to forward-backward fill NaN values in `Portfolio.close`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: year_freq
```
Year frequency.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: returns_acc_defaults
```
Defaults for `vectorbtpro.returns.accessors.ReturnsAccessor`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: trades_type
```
Default `vectorbtpro.portfolio.trades.Trades` to use across `Portfolio`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: orders_cls
```
Class for wrapping order records.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: logs_cls
```
Class for wrapping log records.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: trades_cls
```
Class for wrapping trade records.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: entry_trades_cls
```
Class for wrapping entry trade records.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: exit_trades_cls
```
Class for wrapping exit trade records.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: positions_cls
```
Class for wrapping position records.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: drawdowns_cls
```
Class for wrapping drawdown records.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: call_seq
```
Sequence of calls per row and group.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: cash_deposits_as_input
```
Whether to add cash deposits to the input value when calculating returns.

Otherwise, will subtract them from the output value.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: open_flex
```
`Portfolio.open` in a format suitable for flexible indexing.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: high_flex
```
`Portfolio.high` in a format suitable for flexible indexing.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: low_flex
```
`Portfolio.low` in a format suitable for flexible indexing.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: close_flex
```
`Portfolio.close` in a format suitable for flexible indexing.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: open
```
Open price of each bar.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: high
```
High price of each bar.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: low
```
Low price of each bar.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: close
```
Last asset price at each time step.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_filled_close
```
Get forward and backward filled closing price.

See `vectorbtpro.generic.nb.base.fbfill_nb`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: bm_close
```
Benchmark price per unit series.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_filled_bm_close
```
Get forward and backward filled benchmark closing price.

See `vectorbtpro.generic.nb.base.fbfill_nb`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_weights
```
Get asset weights.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: apply_weights
```
Get view of portfolio with asset weights applied and optionally rescaled.

If `rescale` is True, weights are rescaled in respect to other weights in the same group.
For example, weights 0.5 and 0.5 are rescaled to 1.0 and 1.0 respectively, while
weights 0.7 and 0.3 are rescaled to 1.4 (1.4 * 0.5 = 0.7) and 0.6 (0.6 * 0.5 = 0.3) respectively.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: disable_weights
```
Get view of portfolio with asset weights disabled.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_long_view
```
Get view of portfolio with long positions only.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_short_view
```
Get view of portfolio with short positions only.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: order_records
```
A structured NumPy array of order records.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_orders
```
Get order records.

See `vectorbtpro.portfolio.orders.Orders`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: log_records
```
A structured NumPy array of log records.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_logs
```
Get log records.

See `vectorbtpro.portfolio.logs.Logs`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_entry_trades
```
Get entry trade records.

See `vectorbtpro.portfolio.trades.EntryTrades`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_exit_trades
```
Get exit trade records.

See `vectorbtpro.portfolio.trades.ExitTrades`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_positions
```
Get position records.

See `vectorbtpro.portfolio.trades.Positions`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_trades
```
Get trade/position records depending upon `Portfolio.trades_type`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_trade_history
```
Get order history merged with entry and exit trades as a readable DataFrame.

!!! note
    The P&L and return aggregated across the DataFrame may not match the actual total P&L and
    return, as this DataFrame annotates entry and exit orders with the performance relative to
    their respective trade types. To obtain accurate total statistics, aggregate only the statistics
    of either trade type. Additionally, entry orders include open statistics, whereas exit orders do not.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_drawdowns
```
Get drawdown records from `Portfolio.get_value`.

See `vectorbtpro.generic.drawdowns.Drawdowns`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_init_position
```
Get initial position per column.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_asset_flow
```
Get asset flow series per column.

Returns the total transacted amount of assets at each time step.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_assets
```
Get asset series per column.

Returns the position at each time step.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_position_mask
```
Get position mask per column or group.

An element is True if there is a position at the given time step.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_position_coverage
```
Get position coverage per column or group.

Position coverage is the number of time steps in the market divided by the total number of time steps.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_position_entry_price
```
Get the position's entry price at each time step.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_position_exit_price
```
Get the position's exit price at each time step.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_cash_deposits
```
Get cash deposit series per column or group.

Set `keep_flex` to True to keep format suitable for flexible indexing.
This consumes less memory.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_total_cash_deposits
```
Get total cash deposit series per column or group.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_cash_earnings
```
Get earnings in cash series per column or group.

Set `keep_flex` to True to keep format suitable for flexible indexing.
This consumes less memory.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_total_cash_earnings
```
Get total cash earning series per column or group.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_cash_flow
```
Get cash flow series per column or group.

Use `free` to return the flow of the free cash, which never goes above the initial level,
because an operation always costs money.

!!! note
    Does not include cash deposits, but includes earnings.

    Using `free` yields the same result as during the simulation only when `leverage=1`.
    For anything else, prefill the state instead of reconstructing it.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_init_cash
```
Get initial amount of cash per column or group.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_cash
```
Get cash balance series per column or group.

For `free`, see `Portfolio.get_cash_flow`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_init_price
```
Get initial price per column.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_init_position_value
```
Get initial position value per column.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_init_value
```
Get initial value per column or group.

Includes initial cash and the value of initial position.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_input_value
```
Get total input value per column or group.

Includes initial value and any cash deposited at any point in time.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_asset_value
```
Get asset value series per column or group.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_value
```
Get portfolio value series per column or group.

By default, will generate portfolio value for each asset based on cash flows and thus
independent of other assets, with the initial cash balance and position being that of the
entire group. Useful for generating returns and comparing assets within the same group.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_gross_exposure
```
Get gross exposure.

!!! note
    When both directions, `asset_value` must include the addition of the absolute long-only
    and short-only asset values.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_net_exposure
```
Get net exposure.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_allocations
```
Get portfolio allocation series per column.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_total_profit
```
Get total profit per column or group.

Calculated directly from order records (fast).
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_final_value
```
Get total profit per column or group.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_total_return
```
Get total return per column or group.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_returns
```
Get return series per column or group based on portfolio value.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_asset_pnl
```
Get asset (realized and unrealized) PnL series per column or group.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_asset_returns
```
Get asset return series per column or group.

This type of returns is based solely on cash flows and asset value rather than portfolio
value. It ignores passive cash and thus it will return the same numbers irrespective of the amount of
cash currently available, even `np.inf`. The scale of returns is comparable to that of going
all in and keeping available cash at zero.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_market_value
```
Get market value series per column or group.

If grouped, evenly distributes the initial cash among assets in the group.

!!! note
    Does not take into account fees and slippage. For this, create a separate portfolio.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_market_returns
```
Get market return series per column or group.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_total_market_return
```
Get total market return.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_bm_value
```
Get benchmark value series per column or group.

Based on `Portfolio.bm_close` and `Portfolio.get_market_value`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_bm_returns
```
Get benchmark return series per column or group.

Based on `Portfolio.bm_close` and `Portfolio.get_market_returns`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_returns_acc
```
Get returns accessor of type `vectorbtpro.returns.accessors.ReturnsAccessor`.

!!! hint
    You can find most methods of this accessor as (cacheable) attributes of this portfolio.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: returns_acc
```
`Portfolio.get_returns_acc` with default arguments.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: get_qs
```
Get quantstats adapter of type `vectorbtpro.returns.qs_adapter.QSAdapter`.

`**kwargs` are passed to the adapter constructor.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: qs
```
`Portfolio.get_qs` with default arguments.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: self_aliases
```
Names to associate with this object.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: pre_resolve_attr
```
Pre-process an attribute before resolution.

Uses the following keys:

* `use_asset_returns`: Whether to use `Portfolio.get_asset_returns` when resolving `returns` argument.
* `trades_type`: Which trade type to use when resolving `trades` argument.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: post_resolve_attr
```
Post-process an object after resolution.

Uses the following keys:

* `incl_open`: Whether to include open trades/positions when resolving an argument
    that is an instance of `vectorbtpro.portfolio.trades.Trades`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: resolve_shortcut_attr
```
Resolve an attribute that may have shortcut properties.

If `attr_name` has a prefix `get_`, checks whether the respective shortcut property can be called.
This way, complex call hierarchies can utilize cacheable properties.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: stats_defaults
```
Defaults for `Portfolio.stats`.

Merges `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats_defaults` and
`stats` from `vectorbtpro._settings.portfolio`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: returns_stats
```
Compute various statistics on returns of this portfolio.

See `Portfolio.returns_acc` and `vectorbtpro.returns.accessors.ReturnsAccessor.metrics`.

`kwargs` will be passed to `vectorbtpro.returns.accessors.ReturnsAccessor.stats` method.
If `bm_returns` is not set, uses `Portfolio.get_market_returns`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: plot_orders
```
Plot one column of orders.

`**kwargs` are passed to `vectorbtpro.generic.orders.Orders.plot`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: plot_trades
```
Plot one column of trades.

`**kwargs` are passed to `vectorbtpro.portfolio.trades.Trades.plot`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: plot_trade_pnl
```
Plot one column of trade P&L.

`**kwargs` are passed to `vectorbtpro.portfolio.trades.Trades.plot_pnl`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: plot_trade_signals
```
Plot one column or group of trade signals.

Markers and shapes are colored by trade direction (green = long, red = short).
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: plot_cash_flow
```
Plot one column or group of cash flow.

`**kwargs` are passed to `vectorbtpro.generic.accessors.GenericAccessor.plot`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: plot_cash
```
Plot one column or group of cash balance.

`**kwargs` are passed to `vectorbtpro.generic.accessors.GenericSRAccessor.plot_against`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: plot_asset_flow
```
Plot one column of asset flow.

`**kwargs` are passed to `vectorbtpro.generic.accessors.GenericAccessor.plot`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: plot_assets
```
Plot one column of assets.

`**kwargs` are passed to `vectorbtpro.generic.accessors.GenericSRAccessor.plot_against`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: plot_asset_value
```
Plot one column or group of asset value.

`**kwargs` are passed to `vectorbtpro.generic.accessors.GenericSRAccessor.plot_against`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: plot_value
```
Plot one column or group of value.

`**kwargs` are passed to `vectorbtpro.generic.accessors.GenericSRAccessor.plot_against`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: plot_cum_returns
```
Plot one column or group of cumulative returns.

If `bm_returns` is None, will use `Portfolio.get_market_returns`.

`**kwargs` are passed to `vectorbtpro.returns.accessors.ReturnsSRAccessor.plot_cumulative`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: plot_drawdowns
```
Plot one column or group of drawdowns.

`**kwargs` are passed to `vectorbtpro.generic.drawdowns.Drawdowns.plot`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: plot_underwater
```
Plot one column or group of underwater.

`**kwargs` are passed to `vectorbtpro.generic.accessors.GenericAccessor.plot`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: plot_gross_exposure
```
Plot one column or group of gross exposure.

`**kwargs` are passed to `vectorbtpro.generic.accessors.GenericSRAccessor.plot_against`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: plot_net_exposure
```
Plot one column or group of net exposure.

`**kwargs` are passed to `vectorbtpro.generic.accessors.GenericSRAccessor.plot_against`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: plot_allocations
```
Plot one group of allocations.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: plots_defaults
```
Defaults for `Portfolio.plot`.

Merges `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots_defaults` and
`plots` from `vectorbtpro._settings.portfolio`.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: build_in_output_config_doc
```
Build in-output config documentation.
```

---

# Pasta: portfolio
### Arquivo: base.py
#### Classe: Portfolio
#### Função: override_in_output_config_doc
```
Call this method on each subclass that overrides `Portfolio.in_output_config`.
```

---

# Pasta: portfolio
### Arquivo: call_seq.py
#### Docstring do Módulo
```
Functions for working with call sequence arrays.
```

---

# Pasta: portfolio
### Arquivo: call_seq.py
#### Função: shuffle_call_seq_nb
```
Shuffle the call sequence array.
```

---

# Pasta: portfolio
### Arquivo: call_seq.py
#### Função: build_call_seq_nb
```
Build a new call sequence array.
```

---

# Pasta: portfolio
### Arquivo: call_seq.py
#### Função: require_call_seq
```
Force the call sequence array to pass our requirements.
```

---

# Pasta: portfolio
### Arquivo: call_seq.py
#### Função: build_call_seq
```
Not compiled but faster version of `build_call_seq_nb`.
```

---

# Pasta: portfolio
### Arquivo: chunking.py
#### Docstring do Módulo
```
Extensions for chunking of portfolio.
```

---

# Pasta: portfolio
### Arquivo: chunking.py
#### Função: get_init_cash_slicer
```
Get slicer for `init_cash` based on cash sharing.
```

---

# Pasta: portfolio
### Arquivo: chunking.py
#### Função: get_cash_deposits_slicer
```
Get slicer for `cash_deposits` based on cash sharing.
```

---

# Pasta: portfolio
### Arquivo: chunking.py
#### Função: in_outputs_merge_func
```
Merge chunks of in-output objects.

Concatenates 1-dim arrays, stacks columns of 2-dim arrays, and fixes and concatenates record arrays
using `vectorbtpro.records.chunking.merge_records`. Other objects will throw an error.
```

---

# Pasta: portfolio
### Arquivo: chunking.py
#### Função: merge_sim_outs
```
Merge chunks of `vectorbtpro.portfolio.enums.SimulationOutput` instances.

If `SimulationOutput.in_outputs` is not None, must provide `in_outputs_merge_func` or similar.
```

---

# Pasta: portfolio
### Arquivo: decorators.py
#### Docstring do Módulo
```
Class decorators for portfolio.
```

---

# Pasta: portfolio
### Arquivo: decorators.py
#### Função: attach_returns_acc_methods
```
Class decorator to attach returns accessor methods.

`config` must contain target method names (keys) and settings (values) with the following keys:

* `source_name`: Name of the source method. Defaults to the target name.
* `docstring`: Method docstring.

The class must be a subclass of `vectorbtpro.portfolio.base.Portfolio`.
```

---

# Pasta: portfolio
### Arquivo: decorators.py
#### Função: attach_shortcut_properties
```
Class decorator to attach shortcut properties.

`config` must contain target property names (keys) and settings (values) with the following keys:

* `method_name`: Name of the source method. Defaults to the target name prepended with the prefix `get_`.
* `use_in_outputs`: Whether the property can return an in-place output. Defaults to True.
* `method_kwargs`: Keyword arguments passed to the source method. Defaults to None.
* `decorator`: Defaults to `vectorbtpro.utils.decorators.cached_property` for object types
    'records' and 'red_array'. Otherwise, to `vectorbtpro.utils.decorators.cacheable_property`.
* `docstring`: Method docstring.
* Other keyword arguments are passed to the decorator and can include settings for wrapping,
    indexing, resampling, stacking, etc.

The class must be a subclass of `vectorbtpro.portfolio.base.Portfolio`.
```

---

# Pasta: portfolio
### Arquivo: enums.py
#### Docstring do Módulo
```
Named tuples and enumerated types for portfolio.

Defines enums and other schemas for `vectorbtpro.portfolio`.
```

---

# Pasta: portfolio
### Arquivo: enums.py
#### Classe: RejectedOrderError
```
Rejected order error.
```

---

# Pasta: portfolio
### Arquivo: logs.py
#### Docstring do Módulo
```
Base class for working with log records.

Order records capture information on simulation logs. Logs are populated when
simulating a portfolio and can be accessed as `vectorbtpro.portfolio.base.Portfolio.logs`.

```pycon
>>> from vectorbtpro import *

>>> np.random.seed(42)
>>> price = pd.DataFrame({
...     'a': np.random.uniform(1, 2, size=100),
...     'b': np.random.uniform(1, 2, size=100)
... }, index=[datetime(2020, 1, 1) + timedelta(days=i) for i in range(100)])
>>> size = pd.DataFrame({
...     'a': np.random.uniform(-100, 100, size=100),
...     'b': np.random.uniform(-100, 100, size=100),
... }, index=[datetime(2020, 1, 1) + timedelta(days=i) for i in range(100)])
>>> pf = vbt.Portfolio.from_orders(price, size, fees=0.01, freq='d', log=True)
>>> logs = pf.logs

>>> logs.filled.count()
a    88
b    99
Name: count, dtype: int64

>>> logs.ignored.count()
a    0
b    0
Name: count, dtype: int64

>>> logs.rejected.count()
a    12
b     1
Name: count, dtype: int64
```

## Stats

!!! hint
    See `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats` and `Logs.metrics`.

```pycon
>>> logs['a'].stats()
Start                             2020-01-01 00:00:00
End                               2020-04-09 00:00:00
Period                              100 days 00:00:00
Total Records                                     100
Status Counts: None                                 0
Status Counts: Filled                              88
Status Counts: Ignored                              0
Status Counts: Rejected                            12
Status Info Counts: None                           88
Status Info Counts: NoCashLong                     12
Name: a, dtype: object
```

`Logs.stats` also supports (re-)grouping:

```pycon
>>> logs.stats(group_by=True)
Start                             2020-01-01 00:00:00
End                               2020-04-09 00:00:00
Period                              100 days 00:00:00
Total Records                                     200
Status Counts: None                                 0
Status Counts: Filled                             187
Status Counts: Ignored                              0
Status Counts: Rejected                            13
Status Info Counts: None                          187
Status Info Counts: NoCashLong                     13
Name: group, dtype: object
```

## Plots

!!! hint
    See `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots` and `Logs.subplots`.

This class does not have any subplots.
```

---

# Pasta: portfolio
### Arquivo: logs.py
#### Classe: Logs
```
Extends `vectorbtpro.generic.price_records.PriceRecords` for working with log records.
```

---

# Pasta: portfolio
### Arquivo: logs.py
#### Classe: Logs
#### Função: stats_defaults
```
Defaults for `Logs.stats`.

Merges `vectorbtpro.generic.price_records.PriceRecords.stats_defaults` and
`stats` from `vectorbtpro._settings.logs`.
```

---

# Pasta: portfolio
### Arquivo: logs.py
#### Classe: Logs
#### Função: plots_defaults
```
Defaults for `Logs.plots`.

Merges `vectorbtpro.generic.price_records.PriceRecords.plots_defaults` and
`plots` from `vectorbtpro._settings.logs`.
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Docstring do Módulo
```
Base class for working with order records.

Order records capture information on filled orders. Orders are mainly populated when simulating
a portfolio and can be accessed as `vectorbtpro.portfolio.base.Portfolio.orders`.

```pycon
>>> from vectorbtpro import *

>>> price = vbt.RandomData.pull(
...     ['a', 'b'],
...     start=datetime(2020, 1, 1),
...     end=datetime(2020, 3, 1),
...     seed=vbt.key_dict(a=42, b=43)
... ).get()
```

[=100% "100%"]{: .candystripe .candystripe-animate }

```pycon
>>> size = pd.DataFrame({
...     'a': np.random.randint(-1, 2, size=len(price.index)),
...     'b': np.random.randint(-1, 2, size=len(price.index)),
... }, index=price.index, columns=price.columns)
>>> pf = vbt.Portfolio.from_orders(price, size, fees=0.01, freq='d')

>>> pf.orders.side_buy.count()
symbol
a    17
b    15
Name: count, dtype: int64

>>> pf.orders.side_sell.count()
symbol
a    24
b    26
Name: count, dtype: int64
```

## Stats

!!! hint
    See `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats` and `Orders.metrics`.

```pycon
>>> pf.orders['a'].stats()
Start                  2019-12-31 22:00:00+00:00
End                    2020-02-29 22:00:00+00:00
Period                          61 days 00:00:00
Total Records                                 41
Side Counts: Buy                              17
Side Counts: Sell                             24
Size: Min              0 days 19:33:05.006182372
Size: Median                     1 days 00:00:00
Size: Max                        1 days 00:00:00
Fees: Min              0 days 20:26:25.905776572
Fees: Median           0 days 22:46:22.693324744
Fees: Max              1 days 01:04:25.541681491
Weighted Buy Price                      94.69917
Weighted Sell Price                    95.742148
Name: a, dtype: object
```

`Orders.stats` also supports (re-)grouping:

```pycon
>>> pf.orders.stats(group_by=True)
Start                  2019-12-31 22:00:00+00:00
End                    2020-02-29 22:00:00+00:00
Period                          61 days 00:00:00
Total Records                                 82
Side Counts: Buy                              32
Side Counts: Sell                             50
Size: Min              0 days 19:33:05.006182372
Size: Median                     1 days 00:00:00
Size: Max                        1 days 00:00:00
Fees: Min              0 days 20:26:25.905776572
Fees: Median           0 days 23:58:29.773897679
Fees: Max              1 days 02:29:08.904770159
Weighted Buy Price                     98.804452
Weighted Sell Price                    99.969934
Name: group, dtype: object
```

## Plots

!!! hint
    See `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots` and `Orders.subplots`.

`Orders` class has a single subplot based on `Orders.plot`:

```pycon
>>> pf.orders['a'].plots().show()
```

![](/assets/images/api/orders_plots.light.svg#only-light){: .iimg loading=lazy }
![](/assets/images/api/orders_plots.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Classe: Orders
```
Extends `vectorbtpro.generic.price_records.PriceRecords` for working with order records.
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Classe: FSOrders
```
Extends `Orders` for working with order records generated from signals.
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Classe: Orders
#### Função: get_long_view
```
See `vectorbtpro.portfolio.nb.records.get_long_view_orders_nb`.
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Classe: Orders
#### Função: get_short_view
```
See `vectorbtpro.portfolio.nb.records.get_short_view_orders_nb`.
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Classe: Orders
#### Função: get_signed_size
```
Get signed size.
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Classe: Orders
#### Função: get_value
```
Get value.
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Classe: Orders
#### Função: get_weighted_price
```
Get size-weighted price average.
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Classe: Orders
#### Função: get_price_status
```
See `vectorbtpro.portfolio.nb.records.price_status_nb`.
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Classe: Orders
#### Função: stats_defaults
```
Defaults for `Orders.stats`.

Merges `vectorbtpro.generic.price_records.PriceRecords.stats_defaults` and
`stats` from `vectorbtpro._settings.orders`.
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Classe: Orders
#### Função: plot
```
Plot orders.

Args:
    column (str): Name of the column to plot.
    plot_ohlc (bool): Whether to plot OHLC.
    plot_close (bool): Whether to plot close.
    ohlc_type: Either 'OHLC', 'Candlestick' or Plotly trace.

        Pass None to use the default.
    ohlc_trace_kwargs (dict): Keyword arguments passed to `ohlc_type`.
    close_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `Orders.close`.
    buy_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Buy" markers.
    sell_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Sell" markers.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> index = pd.date_range("2020", periods=5)
    >>> price = pd.Series([1., 2., 3., 2., 1.], index=index)
    >>> size = pd.Series([1., 1., 1., 1., -1.], index=index)
    >>> orders = vbt.Portfolio.from_orders(price, size).orders

    >>> orders.plot().show()
    ```

    ![](/assets/images/api/orders_plot.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/orders_plot.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Classe: Orders
#### Função: plots_defaults
```
Defaults for `Orders.plots`.

Merges `vectorbtpro.generic.price_records.PriceRecords.plots_defaults` and
`plots` from `vectorbtpro._settings.orders`.
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Classe: FSOrders
#### Função: get_stop_orders
```
Get stop orders.
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Classe: FSOrders
#### Função: get_ranges
```
Get records of type `vectorbtpro.generic.ranges.Ranges` for signal-to-fill ranges.
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Classe: FSOrders
#### Função: get_creation_ranges
```
Get records of type `vectorbtpro.generic.ranges.Ranges` for signal-to-creation ranges.
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Classe: FSOrders
#### Função: get_fill_ranges
```
Get records of type `vectorbtpro.generic.ranges.Ranges` for creation-to-fill ranges.
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Classe: FSOrders
#### Função: get_signal_to_creation_duration
```
Get duration between signal and creation.
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Classe: FSOrders
#### Função: get_creation_to_fill_duration
```
Get duration between creation and fill.
```

---

# Pasta: portfolio
### Arquivo: orders.py
#### Classe: FSOrders
#### Função: get_signal_to_fill_duration
```
Get duration between signal and fill.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Docstring do Módulo
```
Classes for preparing portfolio simulations.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Função: valid_price_from_ago_1d_nb
```
Parse from_ago from a valid price.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: PFPrepResult
```
Result of preparation.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
```
Base class for preparing portfolio simulations.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOPreparer
```
Class for preparing `vectorbtpro.portfolio.base.Portfolio.from_orders`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
```
Class for preparing `vectorbtpro.portfolio.base.Portfolio.from_signals`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOFPreparer
```
Class for preparing `vectorbtpro.portfolio.base.Portfolio.from_order_func`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FDOFPreparer
```
Class for preparing `vectorbtpro.portfolio.base.Portfolio.from_def_order_func`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: PFPrepResult
#### Função: target_func
```
Target function.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: PFPrepResult
#### Função: target_args
```
Target arguments.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: PFPrepResult
#### Função: pf_args
```
Portfolio arguments.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: init_cash_mode
```
Initial cash mode.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: group_by
```
Argument `group_by`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: auto_call_seq
```
Whether automatic call sequence is enabled.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: parse_data
```
Parse an instance with OHLC features.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: data
```
Argument `data`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: _pre_open
```
Argument `open` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: _pre_high
```
Argument `high` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: _pre_low
```
Argument `low` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: _pre_close
```
Argument `close` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: _pre_bm_close
```
Argument `bm_close` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: _pre_init_cash
```
Argument `init_cash` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: _pre_init_position
```
Argument `init_position` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: _pre_init_price
```
Argument `init_price` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: _pre_cash_deposits
```
Argument `cash_deposits` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: _pre_freq
```
Argument `freq` before casting to nanosecond format.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: _pre_call_seq
```
Argument `call_seq` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: _pre_in_outputs
```
Argument `in_outputs` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: cs_group_lens
```
Cash sharing aware group lengths.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: group_lens
```
Group lengths.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: sim_group_lens
```
Simulation group lengths.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: align_pc_arr
```
Align a per-column array.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: init_cash
```
Argument `init_cash`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: init_position
```
Argument `init_position`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: init_price
```
Argument `init_price`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: cash_deposits
```
Argument `cash_deposits`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: auto_sim_start
```
Get automatic `sim_start`
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: auto_sim_end
```
Get automatic `sim_end`
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: sim_start
```
Argument `sim_start`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: sim_end
```
Argument `sim_end`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: call_seq
```
Argument `call_seq`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: in_outputs
```
Argument `in_outputs`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: pf_args
```
Arguments to be passed to the portfolio.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: BasePFPreparer
#### Função: result
```
Result as an instance of `PFPrepResult`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOPreparer
#### Função: staticized
```
Argument `staticized`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOPreparer
#### Função: _pre_from_ago
```
Argument `from_ago` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOPreparer
#### Função: _pre_max_order_records
```
Argument `max_order_records` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOPreparer
#### Função: _pre_max_log_records
```
Argument `max_log_records` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOPreparer
#### Função: price_and_from_ago
```
Arguments `price` and `from_ago` after broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOPreparer
#### Função: price
```
Argument `price`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOPreparer
#### Função: from_ago
```
Argument `from_ago`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOPreparer
#### Função: max_order_records
```
Argument `max_order_records`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOPreparer
#### Função: max_log_records
```
Argument `max_log_records`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: _pre_staticized
```
Argument `staticized` before its resolution.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: order_mode
```
Argument `order_mode`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: dynamic_mode
```
Whether the dynamic mode is enabled.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: implicit_mode
```
Whether the explicit mode is enabled.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: explicit_mode
```
Whether the explicit mode is enabled.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: _pre_ls_mode
```
Whether direction-aware mode is enabled before resolution.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: _pre_signals_mode
```
Whether signals mode is enabled before resolution.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: ls_mode
```
Whether direction-aware mode is enabled.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: signals_mode
```
Whether signals mode is enabled.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: signal_func_mode
```
Whether signal function mode is enabled.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: adjust_func_nb
```
Argument `adjust_func_nb`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: signal_func_nb
```
Argument `signal_func_nb`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: post_signal_func_nb
```
Argument `post_signal_func_nb`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: post_segment_func_nb
```
Argument `post_segment_func_nb`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: staticized
```
Argument `staticized`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: _pre_chunked
```
Argument `chunked` before template substitution.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: _pre_entries
```
Argument `entries` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: _pre_exits
```
Argument `exits` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: _pre_long_entries
```
Argument `long_entries` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: _pre_long_exits
```
Argument `long_exits` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: _pre_short_entries
```
Argument `short_entries` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: _pre_short_exits
```
Argument `short_exits` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: _pre_from_ago
```
Argument `from_ago` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: _pre_max_log_records
```
Argument `max_log_records` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: init_in_outputs
```
Initialize `vectorbtpro.portfolio.enums.FSInOutputs`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: signals
```
Arguments `entries`, `exits`, `short_entries`, and `short_exits` after broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: long_entries
```
Argument `long_entries`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: long_exits
```
Argument `long_exits`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: short_entries
```
Argument `short_entries`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: short_exits
```
Argument `short_exits`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: combined_mask
```
Signals combined using the OR rule into a mask.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: price_and_from_ago
```
Arguments `price` and `from_ago` after broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: price
```
Argument `price`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: from_ago
```
Argument `from_ago`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: max_log_records
```
Argument `max_log_records`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: use_stops
```
Argument `use_stops`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: use_limit_orders
```
Whether to use limit orders.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: basic_mode
```
Whether the basic mode is enabled.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: signal_args
```
Argument `signal_args`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FSPreparer
#### Função: post_segment_args
```
Argument `post_segment_args`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOFPreparer
#### Função: _pre_staticized
```
Argument `staticized` before its resolution.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOFPreparer
#### Função: flexible
```
Whether the flexible mode is enabled.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOFPreparer
#### Função: pre_sim_func_nb
```
Argument `pre_sim_func_nb`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOFPreparer
#### Função: post_sim_func_nb
```
Argument `post_sim_func_nb`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOFPreparer
#### Função: pre_group_func_nb
```
Argument `pre_group_func_nb`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOFPreparer
#### Função: post_group_func_nb
```
Argument `post_group_func_nb`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOFPreparer
#### Função: pre_row_func_nb
```
Argument `pre_row_func_nb`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOFPreparer
#### Função: post_row_func_nb
```
Argument `post_row_func_nb`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOFPreparer
#### Função: pre_segment_func_nb
```
Argument `pre_segment_func_nb`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOFPreparer
#### Função: post_segment_func_nb
```
Argument `post_segment_func_nb`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOFPreparer
#### Função: order_func_nb
```
Argument `order_func_nb`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOFPreparer
#### Função: flex_order_func_nb
```
Argument `flex_order_func_nb`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOFPreparer
#### Função: post_order_func_nb
```
Argument `post_order_func_nb`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOFPreparer
#### Função: staticized
```
Argument `staticized`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOFPreparer
#### Função: _pre_segment_mask
```
Argument `segment_mask` before broadcasting.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FOFPreparer
#### Função: segment_mask
```
Argument `segment_mask`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FDOFPreparer
#### Função: pre_segment_func_nb
```
Argument `pre_segment_func_nb`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FDOFPreparer
#### Função: order_func_nb
```
Argument `order_func_nb`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FDOFPreparer
#### Função: flex_order_func_nb
```
Argument `flex_order_func_nb`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FDOFPreparer
#### Função: _pre_chunked
```
Argument `chunked` before template substitution.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FDOFPreparer
#### Função: pre_segment_args
```
Argument `pre_segment_args`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FDOFPreparer
#### Função: _order_args
```
Either `order_args` or `flex_order_args`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FDOFPreparer
#### Função: order_args
```
Argument `order_args`.
```

---

# Pasta: portfolio
### Arquivo: preparing.py
#### Classe: FDOFPreparer
#### Função: flex_order_args
```
Argument `flex_order_args`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Docstring do Módulo
```
Base class for working with trade records.

Trade records capture information on trades.

In vectorbt, a trade is a sequence of orders that starts with an opening order and optionally ends
with a closing order. Every pair of opposite orders can be represented by a trade. Each trade has a PnL
info attached to quickly assess its performance. An interesting effect of this representation
is the ability to aggregate trades: if two or more trades are happening one after another in time,
they can be aggregated into a bigger trade. This way, for example, single-order trades can be aggregated
into positions; but also multiple positions can be aggregated into a single blob that reflects the performance
of the entire symbol.

!!! warning
    All classes return both closed AND open trades/positions, which may skew your performance results.
    To only consider closed trades/positions, you should explicitly query the `status_closed` attribute.

## Trade types

There are three main types of trades.

### Entry trades

An entry trade is created from each order that opens or adds to a position.

For example, if we have a single large buy order and 100 smaller sell orders, we will see
a single trade with the entry information copied from the buy order and the exit information being
a size-weighted average over the exit information of all sell orders. On the other hand,
if we have 100 smaller buy orders and a single sell order, we will see 100 trades,
each with the entry information copied from the buy order and the exit information being
a size-based fraction of the exit information of the sell order.

Use `vectorbtpro.portfolio.trades.EntryTrades.from_orders` to build entry trades from orders.
Also available as `vectorbtpro.portfolio.base.Portfolio.entry_trades`.

### Exit trades

An exit trade is created from each order that closes or removes from a position.

Use `vectorbtpro.portfolio.trades.ExitTrades.from_orders` to build exit trades from orders.
Also available as `vectorbtpro.portfolio.base.Portfolio.exit_trades`.

### Positions

A position is created from a sequence of entry or exit trades.

Use `vectorbtpro.portfolio.trades.Positions.from_trades` to build positions from entry or exit trades.
Also available as `vectorbtpro.portfolio.base.Portfolio.positions`.

## Example

* Increasing position:

```pycon
>>> from vectorbtpro import *

>>> # Entry trades
>>> pf_kwargs = dict(
...     close=pd.Series([1., 2., 3., 4., 5.]),
...     size=pd.Series([1., 1., 1., 1., -4.]),
...     fixed_fees=1.
... )
>>> entry_trades = vbt.Portfolio.from_orders(**pf_kwargs).entry_trades
>>> entry_trades.readable
   Entry Trade Id  Column  Size  Entry Order Id  Entry Index  Avg Entry Price  \
0               0       0   1.0               0            0              1.0
1               1       0   1.0               1            1              2.0
2               2       0   1.0               2            2              3.0
3               3       0   1.0               3            3              4.0

   Entry Fees  Exit Order Id  Exit Index  Avg Exit Price  Exit Fees   PnL  \
0         1.0              4           4             5.0       0.25  2.75
1         1.0              4           4             5.0       0.25  1.75
2         1.0              4           4             5.0       0.25  0.75
3         1.0              4           4             5.0       0.25 -0.25

   Return Direction  Status  Position Id
0  2.7500      Long  Closed            0
1  0.8750      Long  Closed            0
2  0.2500      Long  Closed            0
3 -0.0625      Long  Closed            0

>>> # Exit trades
>>> exit_trades = vbt.Portfolio.from_orders(**pf_kwargs).exit_trades
>>> exit_trades.readable
   Exit Trade Id  Column  Size  Entry Order Id  Entry Index  Avg Entry Price  \
0              0       0   4.0               0            0              2.5

   Entry Fees  Exit Order Id  Exit Index  Avg Exit Price  Exit Fees  PnL  \
0         4.0              4           4             5.0        1.0  5.0

   Return Direction  Status  Position Id
0     0.5      Long  Closed            0

>>> # Positions
>>> positions = vbt.Portfolio.from_orders(**pf_kwargs).positions
>>> positions.readable
   Position Id  Column  Size  Entry Order Id  Entry Index  Avg Entry Price  \
0            0       0   4.0               0            0              2.5

   Entry Fees  Exit Order Id  Exit Index  Avg Exit Price  Exit Fees  PnL  \
0         4.0              4           4             5.0        1.0  5.0

   Return Direction  Status
0     0.5      Long  Closed

>>> entry_trades.pnl.sum() == exit_trades.pnl.sum() == positions.pnl.sum()
True
```

* Decreasing position:

```pycon
>>> # Entry trades
>>> pf_kwargs = dict(
...     close=pd.Series([1., 2., 3., 4., 5.]),
...     size=pd.Series([4., -1., -1., -1., -1.]),
...     fixed_fees=1.
... )
>>> entry_trades = vbt.Portfolio.from_orders(**pf_kwargs).entry_trades
>>> entry_trades.readable
   Entry Trade Id  Column  Size  Entry Order Id  Entry Index  Avg Entry Price  \
0               0       0   4.0               0            0              1.0

   Entry Fees  Exit Order Id  Exit Index  Avg Exit Price  Exit Fees  PnL  \
0         1.0              4           4             3.5        4.0  5.0

   Return Direction  Status  Position Id
0    1.25      Long  Closed            0

>>> # Exit trades
>>> exit_trades = vbt.Portfolio.from_orders(**pf_kwargs).exit_trades
>>> exit_trades.readable
   Exit Trade Id  Column  Size  Entry Order Id  Entry Index  Avg Entry Price  \
0              0       0   1.0               0            0              1.0
1              1       0   1.0               0            0              1.0
2              2       0   1.0               0            0              1.0
3              3       0   1.0               0            0              1.0

   Entry Fees  Exit Order Id  Exit Index  Avg Exit Price  Exit Fees   PnL  \
0        0.25              1           1             2.0        1.0 -0.25
1        0.25              2           2             3.0        1.0  0.75
2        0.25              3           3             4.0        1.0  1.75
3        0.25              4           4             5.0        1.0  2.75

   Return Direction  Status  Position Id
0   -0.25      Long  Closed            0
1    0.75      Long  Closed            0
2    1.75      Long  Closed            0
3    2.75      Long  Closed            0

>>> # Positions
>>> positions = vbt.Portfolio.from_orders(**pf_kwargs).positions
>>> positions.readable
   Position Id  Column  Size  Entry Order Id  Entry Index  Avg Entry Price  \
0            0       0   4.0               0            0              1.0

   Entry Fees  Exit Order Id  Exit Index  Avg Exit Price  Exit Fees  PnL  \
0         1.0              4           4             3.5        4.0  5.0

   Return Direction  Status
0    1.25      Long  Closed

>>> entry_trades.pnl.sum() == exit_trades.pnl.sum() == positions.pnl.sum()
True
```

* Multiple reversing positions:

```pycon
>>> # Entry trades
>>> pf_kwargs = dict(
...     close=pd.Series([1., 2., 3., 4., 5.]),
...     size=pd.Series([1., -2., 2., -2., 1.]),
...     fixed_fees=1.
... )
>>> entry_trades = vbt.Portfolio.from_orders(**pf_kwargs).entry_trades
>>> entry_trades.readable
   Entry Trade Id  Column  Size  Entry Order Id  Entry Index  Avg Entry Price  \
0               0       0   1.0               0            0              1.0
1               1       0   1.0               1            1              2.0
2               2       0   1.0               2            2              3.0
3               3       0   1.0               3            3              4.0

   Entry Fees  Exit Order Id  Exit Index  Avg Exit Price  Exit Fees  PnL  \
0         1.0              1           1             2.0        0.5 -0.5
1         0.5              2           2             3.0        0.5 -2.0
2         0.5              3           3             4.0        0.5  0.0
3         0.5              4           4             5.0        1.0 -2.5

   Return Direction  Status  Position Id
0  -0.500      Long  Closed            0
1  -1.000     Short  Closed            1
2   0.000      Long  Closed            2
3  -0.625     Short  Closed            3

>>> # Exit trades
>>> exit_trades = vbt.Portfolio.from_orders(**pf_kwargs).exit_trades
>>> exit_trades.readable
   Exit Trade Id  Column  Size  Entry Order Id  Entry Index  Avg Entry Price  \
0              0       0   1.0               0            0              1.0
1              1       0   1.0               1            1              2.0
2              2       0   1.0               2            2              3.0
3              3       0   1.0               3            3              4.0

   Entry Fees  Exit Order Id  Exit Index  Avg Exit Price  Exit Fees  PnL  \
0         1.0              1           1             2.0        0.5 -0.5
1         0.5              2           2             3.0        0.5 -2.0
2         0.5              3           3             4.0        0.5  0.0
3         0.5              4           4             5.0        1.0 -2.5

   Return Direction  Status  Position Id
0  -0.500      Long  Closed            0
1  -1.000     Short  Closed            1
2   0.000      Long  Closed            2
3  -0.625     Short  Closed            3

>>> # Positions
>>> positions = vbt.Portfolio.from_orders(**pf_kwargs).positions
>>> positions.readable
   Position Id  Column  Size  Entry Order Id  Entry Index  Avg Entry Price  \
0            0       0   1.0               0            0              1.0
1            1       0   1.0               1            1              2.0
2            2       0   1.0               2            2              3.0
3            3       0   1.0               3            3              4.0

   Entry Fees  Exit Order Id  Exit Index  Avg Exit Price  Exit Fees  PnL  \
0         1.0              1           1             2.0        0.5 -0.5
1         0.5              2           2             3.0        0.5 -2.0
2         0.5              3           3             4.0        0.5  0.0
3         0.5              4           4             5.0        1.0 -2.5

   Return Direction  Status
0  -0.500      Long  Closed
1  -1.000     Short  Closed
2   0.000      Long  Closed
3  -0.625     Short  Closed

>>> entry_trades.pnl.sum() == exit_trades.pnl.sum() == positions.pnl.sum()
True
```

* Open position:

```pycon
>>> # Entry trades
>>> pf_kwargs = dict(
...     close=pd.Series([1., 2., 3., 4., 5.]),
...     size=pd.Series([1., 0., 0., 0., 0.]),
...     fixed_fees=1.
... )
>>> entry_trades = vbt.Portfolio.from_orders(**pf_kwargs).entry_trades
>>> entry_trades.readable
   Entry Trade Id  Column  Size  Entry Order Id  Entry Index  Avg Entry Price  \
0               0       0   1.0               0            0              1.0

   Entry Fees  Exit Order Id  Exit Index  Avg Exit Price  Exit Fees  PnL  \
0         1.0             -1           4             5.0        0.0  3.0

   Return Direction Status  Position Id
0     3.0      Long   Open            0

>>> # Exit trades
>>> exit_trades = vbt.Portfolio.from_orders(**pf_kwargs).exit_trades
>>> exit_trades.readable
   Exit Trade Id  Column  Size  Entry Order Id  Entry Index  Avg Entry Price  \
0              0       0   1.0               0            0              1.0

   Entry Fees  Exit Order Id  Exit Index  Avg Exit Price  Exit Fees  PnL  \
0         1.0             -1           4             5.0        0.0  3.0

   Return Direction Status  Position Id
0     3.0      Long   Open            0

>>> # Positions
>>> positions = vbt.Portfolio.from_orders(**pf_kwargs).positions
>>> positions.readable
   Position Id  Column  Size  Entry Order Id  Entry Index  Avg Entry Price  \
0            0       0   1.0               0            0              1.0

   Entry Fees  Exit Order Id  Exit Index  Avg Exit Price  Exit Fees  PnL  \
0         1.0             -1           4             5.0        0.0  3.0

   Return Direction Status
0     3.0      Long   Open

>>> entry_trades.pnl.sum() == exit_trades.pnl.sum() == positions.pnl.sum()
True
```

Get trade count, trade PnL, and winning trade PnL:

```pycon
>>> price = pd.Series([1., 2., 3., 4., 3., 2., 1.])
>>> size = pd.Series([1., -0.5, -0.5, 2., -0.5, -0.5, -0.5])
>>> trades = vbt.Portfolio.from_orders(price, size).trades

>>> trades.count()
6

>>> trades.pnl.sum()
-3.0

>>> trades.winning.count()
2

>>> trades.winning.pnl.sum()
1.5
```

Get count and PnL of trades with duration of more than 2 days:

```pycon
>>> mask = (trades.records['exit_idx'] - trades.records['entry_idx']) > 2
>>> trades_filtered = trades.apply_mask(mask)
>>> trades_filtered.count()
2

>>> trades_filtered.pnl.sum()
-3.0
```

## Stats

!!! hint
    See `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats` and `Trades.metrics`.

```pycon
>>> price = vbt.RandomData.pull(
...     ['a', 'b'],
...     start=datetime(2020, 1, 1),
...     end=datetime(2020, 3, 1),
...     seed=vbt.symbol_dict(a=42, b=43)
... ).get()
```

[=100% "100%"]{: .candystripe .candystripe-animate }

```pycon
>>> size = pd.DataFrame({
...     'a': np.random.randint(-1, 2, size=len(price.index)),
...     'b': np.random.randint(-1, 2, size=len(price.index)),
... }, index=price.index, columns=price.columns)
>>> pf = vbt.Portfolio.from_orders(price, size, fees=0.01, init_cash="auto")

>>> pf.trades['a'].stats()
Start                          2019-12-31 23:00:00+00:00
End                            2020-02-29 23:00:00+00:00
Period                                  61 days 00:00:00
First Trade Start              2019-12-31 23:00:00+00:00
Last Trade End                 2020-02-29 23:00:00+00:00
Coverage                                60 days 00:00:00
Overlap Coverage                        49 days 00:00:00
Total Records                                       19.0
Total Long Trades                                    2.0
Total Short Trades                                  17.0
Total Closed Trades                                 18.0
Total Open Trades                                    1.0
Open Trade PnL                                    16.063
Win Rate [%]                                   61.111111
Max Win Streak                                      11.0
Max Loss Streak                                      7.0
Best Trade [%]                                  3.526377
Worst Trade [%]                                -6.543679
Avg Winning Trade [%]                           2.225861
Avg Losing Trade [%]                           -3.601313
Avg Winning Trade Duration    32 days 19:38:10.909090909
Avg Losing Trade Duration                5 days 00:00:00
Profit Factor                                   1.022425
Expectancy                                      0.028157
SQN                                             0.039174
Name: agg_stats, dtype: object
```

Positions share almost identical metrics with trades:

```pycon
>>> pf.positions['a'].stats()
Start                         2019-12-31 23:00:00+00:00
End                           2020-02-29 23:00:00+00:00
Period                                 61 days 00:00:00
First Trade Start             2019-12-31 23:00:00+00:00
Last Trade End                2020-02-29 23:00:00+00:00
Coverage                               60 days 00:00:00
Overlap Coverage                        0 days 00:00:00
Total Records                                       5.0
Total Long Trades                                   2.0
Total Short Trades                                  3.0
Total Closed Trades                                 4.0
Total Open Trades                                   1.0
Open Trade PnL                                38.356823
Win Rate [%]                                        0.0
Max Win Streak                                      0.0
Max Loss Streak                                     4.0
Best Trade [%]                                -1.529613
Worst Trade [%]                               -6.543679
Avg Winning Trade [%]                               NaN
Avg Losing Trade [%]                          -3.786739
Avg Winning Trade Duration                          NaT
Avg Losing Trade Duration               4 days 00:00:00
Profit Factor                                       0.0
Expectancy                                    -5.446748
SQN                                           -1.794214
Name: agg_stats, dtype: object
```

To also include open trades/positions when calculating metrics such as win rate, pass `incl_open=True`:

```pycon
>>> pf.trades['a'].stats(settings=dict(incl_open=True))
Start                         2019-12-31 23:00:00+00:00
End                           2020-02-29 23:00:00+00:00
Period                                 61 days 00:00:00
First Trade Start             2019-12-31 23:00:00+00:00
Last Trade End                2020-02-29 23:00:00+00:00
Coverage                               60 days 00:00:00
Overlap Coverage                       49 days 00:00:00
Total Records                                      19.0
Total Long Trades                                   2.0
Total Short Trades                                 17.0
Total Closed Trades                                18.0
Total Open Trades                                   1.0
Open Trade PnL                                   16.063
Win Rate [%]                                  61.111111
Max Win Streak                                     12.0
Max Loss Streak                                     7.0
Best Trade [%]                                 3.526377
Worst Trade [%]                               -6.543679
Avg Winning Trade [%]                          2.238896
Avg Losing Trade [%]                          -3.601313
Avg Winning Trade Duration             33 days 18:00:00
Avg Losing Trade Duration               5 days 00:00:00
Profit Factor                                  1.733143
Expectancy                                     0.872096
SQN                                            0.804714
Name: agg_stats, dtype: object
```

`Trades.stats` also supports (re-)grouping:

```pycon
>>> pf.trades.stats(group_by=True)
Start                          2019-12-31 23:00:00+00:00
End                            2020-02-29 23:00:00+00:00
Period                                  61 days 00:00:00
First Trade Start              2019-12-31 23:00:00+00:00
Last Trade End                 2020-02-29 23:00:00+00:00
Coverage                                61 days 00:00:00
Overlap Coverage                        61 days 00:00:00
Total Records                                         37
Total Long Trades                                      5
Total Short Trades                                    32
Total Closed Trades                                   35
Total Open Trades                                      2
Open Trade PnL                                  1.336259
Win Rate [%]                                   37.142857
Max Win Streak                                        11
Max Loss Streak                                       10
Best Trade [%]                                  3.526377
Worst Trade [%]                                -8.710238
Avg Winning Trade [%]                           1.907799
Avg Losing Trade [%]                           -3.259135
Avg Winning Trade Duration    28 days 14:46:09.230769231
Avg Losing Trade Duration               14 days 00:00:00
Profit Factor                                   0.340493
Expectancy                                     -1.292596
SQN                                            -2.509223
Name: group, dtype: object
```

## Plots

!!! hint
    See `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots` and `Trades.subplots`.

`Trades` class has two subplots based on `Trades.plot` and `Trades.plot_pnl`:

```pycon
>>> pf.trades['a'].plots().show()
```

![](/assets/images/api/trades_plots.light.svg#only-light){: .iimg loading=lazy }
![](/assets/images/api/trades_plots.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
```
Extends `vectorbtpro.generic.ranges.Ranges` for working with trade-like records, such as
entry trades, exit trades, and positions.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: EntryTrades
```
Extends `Trades` for working with entry trade records.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: ExitTrades
```
Extends `Trades` for working with exit trade records.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Positions
```
Extends `Trades` for working with position records.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_ranges
```
Get records of type `vectorbtpro.generic.ranges.Ranges`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_winning
```
Get winning trades.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_losing
```
Get losing trades.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_winning_streak
```
Get winning streak at each trade in the current column.

See `vectorbtpro.portfolio.nb.records.trade_winning_streak_nb`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_losing_streak
```
Get losing streak at each trade in the current column.

See `vectorbtpro.portfolio.nb.records.trade_losing_streak_nb`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_win_rate
```
Get rate of winning trades.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_profit_factor
```
Get profit factor.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_expectancy
```
Get average profitability.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_sqn
```
Get System Quality Number (SQN).
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_best_price
```
Get best price.

See `vectorbtpro.portfolio.nb.records.best_price_nb`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_worst_price
```
Get worst price.

See `vectorbtpro.portfolio.nb.records.worst_price_nb`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_best_price_idx
```
Get (relative) index of best price.

See `vectorbtpro.portfolio.nb.records.best_price_idx_nb`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_worst_price_idx
```
Get (relative) index of worst price.

See `vectorbtpro.portfolio.nb.records.worst_price_idx_nb`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_expanding_best_price
```
Get expanding best price.

See `vectorbtpro.portfolio.nb.records.expanding_best_price_nb`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_expanding_worst_price
```
Get expanding worst price.

See `vectorbtpro.portfolio.nb.records.expanding_worst_price_nb`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_mfe
```
Get MFE.

See `vectorbtpro.portfolio.nb.records.mfe_nb`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_mae
```
Get MAE.

See `vectorbtpro.portfolio.nb.records.mae_nb`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_expanding_mfe
```
Get expanding MFE.

See `vectorbtpro.portfolio.nb.records.expanding_mfe_nb`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_expanding_mae
```
Get expanding MAE.

See `vectorbtpro.portfolio.nb.records.expanding_mae_nb`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_edge_ratio
```
Get edge ratio.

See `vectorbtpro.portfolio.nb.records.edge_ratio_nb`.

If `volatility` is None, calculates the 14-period ATR if both high and low are provided,
otherwise the 14-period rolling standard deviation.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: get_running_edge_ratio
```
Get running edge ratio.

See `vectorbtpro.portfolio.nb.records.running_edge_ratio_nb`.

If `volatility` is None, calculates the 14-period ATR if both high and low are provided,
otherwise the 14-period rolling standard deviation.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: stats_defaults
```
Defaults for `Trades.stats`.

Merges `vectorbtpro.generic.ranges.Ranges.stats_defaults` and
`stats` from `vectorbtpro._settings.trades`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: plot_pnl
```
Plot trade PnL or returns.

Args:
    column (str): Name of the column to plot.
    group_by (any): Group columns. See `vectorbtpro.base.grouping.base.Grouper`.
    pct_scale (bool): Whether to set y-axis to `Trades.returns`, otherwise to `Trades.pnl`.
    marker_size_range (tuple): Range of marker size.
    opacity_range (tuple): Range of marker opacity.
    closed_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Closed" markers.
    closed_profit_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Closed - Profit" markers.
    closed_loss_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Closed - Loss" markers.
    open_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Open" markers.
    hline_shape_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Figure.add_shape` for zeroline.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    xref (str): X coordinate axis.
    yref (str): Y coordinate axis.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> index = pd.date_range("2020", periods=7)
    >>> price = pd.Series([1., 2., 3., 4., 3., 2., 1.], index=index)
    >>> orders = pd.Series([1., -0.5, -0.5, 2., -0.5, -0.5, -0.5], index=index)
    >>> pf = vbt.Portfolio.from_orders(price, orders)
    >>> pf.trades.plot_pnl().show()
    ```

    ![](/assets/images/api/trades_plot_pnl.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/trades_plot_pnl.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: plot_against_pnl
```
Plot a field against PnL or returns.

Args:
    field (str, MappedArray, or array_like): Field to be plotted.

        Can be also provided as a mapped array or 1-dim array.
    field_label (str): Label of the field.
    column (str): Name of the column to plot.
    group_by (any): Group columns. See `vectorbtpro.base.grouping.base.Grouper`.
    pct_scale (bool): Whether to set x-axis to `Trades.returns`, otherwise to `Trades.pnl`.
    field_pct_scale (bool): Whether to make y-axis a percentage scale.
    closed_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Closed" markers.
    closed_profit_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Closed - Profit" markers.
    closed_loss_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Closed - Loss" markers.
    open_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Open" markers.
    hline_shape_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Figure.add_shape` for horizontal zeroline.
    vline_shape_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Figure.add_shape` for vertical zeroline.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    xref (str): X coordinate axis.
    yref (str): Y coordinate axis.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> index = pd.date_range("2020", periods=10)
    >>> price = pd.Series([1., 2., 3., 4., 5., 6., 5., 3., 2., 1.], index=index)
    >>> orders = pd.Series([1., -0.5, 0., -0.5, 2., 0., -0.5, -0.5, 0., -0.5], index=index)
    >>> pf = vbt.Portfolio.from_orders(price, orders)
    >>> trades = pf.trades
    >>> trades.plot_against_pnl("MFE").show()
    ```

    ![](/assets/images/api/trades_plot_against_pnl.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/trades_plot_against_pnl.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: plot_expanding
```
Plot projections of an expanding field.

Args:
    field (str or array_like): Field to be plotted.

         Can be also provided as a 2-dim array.
    field_label (str): Label of the field.
    column (str): Name of the column to plot. Optional.
    group_by (any): Group columns. See `vectorbtpro.base.grouping.base.Grouper`.
    plot_bands (bool): See `vectorbtpro.generic.accessors.GenericDFAccessor.plot_projections`.
    colorize (bool, str or callable): See `vectorbtpro.generic.accessors.GenericDFAccessor.plot_projections`.
    field_pct_scale (bool): Whether to make y-axis a percentage scale.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    fig (Figure or FigureWidget): Figure to add traces to.
    **kwargs: Keyword arguments passed to `vectorbtpro.generic.accessors.GenericDFAccessor.plot_projections`.

Usage:
    ```pycon
    >>> index = pd.date_range("2020", periods=10)
    >>> price = pd.Series([1., 2., 3., 2., 4., 5., 6., 5., 6., 7.], index=index)
    >>> orders = pd.Series([1., 0., 0., -2., 0., 0., 2., 0., 0., -1.], index=index)
    >>> pf = vbt.Portfolio.from_orders(price, orders)
    >>> pf.trades.plot_expanding("MFE").show()
    ```

    ![](/assets/images/api/trades_plot_expanding.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/trades_plot_expanding.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: plot_running_edge_ratio
```
Plot one column/group of edge ratio.

`**kwargs` are passed to `vectorbtpro.generic.accessors.GenericSRAccessor.plot_against`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: plot
```
Plot trades.

Args:
    column (str): Name of the column to plot.
    plot_ohlc (bool): Whether to plot OHLC.
    plot_close (bool): Whether to plot close.
    plot_markers (bool): Whether to plot markers.
    plot_zones (bool): Whether to plot zones.
    plot_by_type (bool): Whether to plot exit trades by type.

        Otherwise, the appearance will be controlled using `exit_trace_kwargs`.
    ohlc_type: Either 'OHLC', 'Candlestick' or Plotly trace.

        Pass None to use the default.
    ohlc_trace_kwargs (dict): Keyword arguments passed to `ohlc_type`.
    close_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `Trades.close`.
    entry_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Entry" markers.
    exit_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Exit" markers.
    exit_profit_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Exit - Profit" markers.
    exit_loss_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Exit - Loss" markers.
    active_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Active" markers.
    profit_shape_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Figure.add_shape` for profit zones.
    loss_shape_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Figure.add_shape` for loss zones.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    xref (str): X coordinate axis.
    yref (str): Y coordinate axis.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> index = pd.date_range("2020", periods=7)
    >>> price = pd.Series([1., 2., 3., 4., 3., 2., 1.], index=index)
    >>> size = pd.Series([1., -0.5, -0.5, 2., -0.5, -0.5, -0.5], index=index)
    >>> pf = vbt.Portfolio.from_orders(price, size)
    >>> pf.trades.plot().show()
    ```

    ![](/assets/images/api/trades_plot.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/trades_plot.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Trades
#### Função: plots_defaults
```
Defaults for `Trades.plots`.

Merges `vectorbtpro.generic.ranges.Ranges.plots_defaults` and
`plots` from `vectorbtpro._settings.trades`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: EntryTrades
#### Função: from_orders
```
Build `EntryTrades` from `vectorbtpro.portfolio.orders.Orders`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: EntryTrades
#### Função: plot_signals
```
Plot entry trade signals.

Args:
    column (str): Name of the column to plot.
    plot_ohlc (bool): Whether to plot OHLC.
    plot_close (bool): Whether to plot close.
    ohlc_type: Either 'OHLC', 'Candlestick' or Plotly trace.

        Pass None to use the default.
    ohlc_trace_kwargs (dict): Keyword arguments passed to `ohlc_type`.
    close_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `EntryTrades.close`.
    long_entry_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Long Entry" markers.
    short_entry_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Short Entry" markers.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> index = pd.date_range("2020", periods=7)
    >>> price = pd.Series([1, 2, 3, 2, 3, 4, 3], index=index)
    >>> orders = pd.Series([1, 0, -1, 0, -1, 2, -2], index=index)
    >>> pf = vbt.Portfolio.from_orders(price, orders)
    >>> pf.entry_trades.plot_signals().show()
    ```

    ![](/assets/images/api/entry_trades_plot_signals.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/entry_trades_plot_signals.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: ExitTrades
#### Função: from_orders
```
Build `ExitTrades` from `vectorbtpro.portfolio.orders.Orders`.
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: ExitTrades
#### Função: plot_signals
```
Plot exit trade signals.

Args:
    column (str): Name of the column to plot.
    plot_ohlc (bool): Whether to plot OHLC.
    plot_close (bool): Whether to plot close.
    ohlc_type: Either 'OHLC', 'Candlestick' or Plotly trace.

        Pass None to use the default.
    ohlc_trace_kwargs (dict): Keyword arguments passed to `ohlc_type`.
    close_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for `ExitTrades.close`.
    long_exit_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Long Exit" markers.
    short_exit_trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter` for "Short Exit" markers.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> index = pd.date_range("2020", periods=7)
    >>> price = pd.Series([1, 2, 3, 2, 3, 4, 3], index=index)
    >>> orders = pd.Series([1, 0, -1, 0, -1, 2, -2], index=index)
    >>> pf = vbt.Portfolio.from_orders(price, orders)
    >>> pf.exit_trades.plot_signals().show()
    ```

    ![](/assets/images/api/exit_trades_plot_signals.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/exit_trades_plot_signals.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: portfolio
### Arquivo: trades.py
#### Classe: Positions
#### Função: from_trades
```
Build `Positions` from `Trades`.
```

---

# Pasta: portfolio
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules for working with portfolio.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Docstring do Módulo
```
Numba-compiled functions for portfolio analysis.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: get_long_size_nb
```
Get long size.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: get_short_size_nb
```
Get short size.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: asset_flow_nb
```
Get asset flow series per column.

Returns the total transacted amount of assets at each time step.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: assets_nb
```
Get asset series per column.

Returns the current position at each time step.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: position_mask_nb
```
Get position mask per column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: position_mask_grouped_nb
```
Get position mask per group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: position_coverage_nb
```
Get position mask per column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: position_coverage_grouped_nb
```
Get position coverage per group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: cash_deposits_nb
```
Get cash deposit series per column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: cash_deposits_grouped_nb
```
Get cash deposit series per group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: cash_earnings_nb
```
Get cash earning series per column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: cash_earnings_grouped_nb
```
Get cash earning series per group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: get_free_cash_diff_nb
```
Get updated debt and free cash flow.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: cash_flow_nb
```
Get (free) cash flow series per column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: cash_flow_grouped_nb
```
Get (free) cash flow series per group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: align_init_cash_nb
```
Align initial cash to the maximum negative free cash flow per column or group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: init_cash_nb
```
Get initial cash per column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: init_cash_grouped_nb
```
Get initial cash per group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: cash_nb
```
Get cash series per column or group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: init_position_value_nb
```
Get initial position value per column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: init_position_value_grouped_nb
```
Get initial position value per group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: init_value_nb
```
Get initial value per column or group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: asset_value_nb
```
Get asset value series per column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: asset_value_grouped_nb
```
Get asset value series per group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: value_nb
```
Get value series per column or group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: gross_exposure_nb
```
Get gross exposure series per column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: net_exposure_nb
```
Get net exposure series per column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: allocations_nb
```
Get allocations per column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: total_profit_nb
```
Get total profit per column.

A much faster version than the one based on `value_nb`.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: total_profit_grouped_nb
```
Get total profit per group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: returns_nb
```
Get return series per column or group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: get_asset_pnl_nb
```
Get asset PnL from the input and output asset value, and the cash flow.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: asset_pnl_nb
```
Get asset (realized and unrealized) PnL series per column or group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: get_asset_return_nb
```
Get asset return from the input and output asset value, and the cash flow.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: asset_returns_nb
```
Get asset return series per column or group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: market_value_nb
```
Get market value per column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: market_value_grouped_nb
```
Get market value per group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: analysis.py
#### Função: total_market_return_nb
```
Get total market return per column or group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Docstring do Módulo
```
Core Numba-compiled functions for portfolio simulation.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: order_not_filled_nb
```
Return `OrderResult` for order that hasn't been filled.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: check_adj_price_nb
```
Check whether adjusted price is within price boundaries.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: approx_long_buy_value_nb
```
Approximate value of a long-buy operation.

Positive value means spending (for sorting reasons).
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: adj_size_granularity_nb
```
Whether to adjust the size with the size granularity.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: cast_account_state_nb
```
Cast account state to float.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: long_buy_nb
```
Open or increase a long position.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: approx_long_sell_value_nb
```
Approximate value of a long-sell operation.

Positive value means spending (for sorting reasons).
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: long_sell_nb
```
Decrease or close a long position.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: approx_short_sell_value_nb
```
Approximate value of a short-sell operation.

Positive value means spending (for sorting reasons).
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: short_sell_nb
```
Open or increase a short position.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: approx_short_buy_value_nb
```
Approximate value of a short-buy operation.

Positive value means spending (for sorting reasons).
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: short_buy_nb
```
Decrease or close a short position.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: approx_buy_value_nb
```
Approximate value of a buy operation.

Positive value means spending (for sorting reasons).
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: buy_nb
```
Buy.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: approx_sell_value_nb
```
Approximate value of a sell operation.

Positive value means spending (for sorting reasons).
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: sell_nb
```
Sell.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: update_value_nb
```
Update valuation price and value.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: get_diraware_size_nb
```
Get direction-aware size.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: resolve_size_nb
```
Resolve size into an absolute amount of assets and percentage of resources.

Percentage is only set if the option `SizeType.Percent(100)` is used.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: approx_order_value_nb
```
Approximate the value of an order.

Assumes that cash is infinite.

Positive value means spending (for sorting reasons).
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: execute_order_nb
```
Execute an order given the current state.

Args:
    exec_state (ExecState): See `vectorbtpro.portfolio.enums.ExecState`.
    order (Order): See `vectorbtpro.portfolio.enums.Order`.
    price_area (OrderPriceArea): See `vectorbtpro.portfolio.enums.PriceArea`.
    update_value (bool): Whether to update the value.

Error is thrown if an input has value that is not expected.
Order is ignored if its execution has no effect on the current balance.
Order is rejected if an input goes over a limit or against a restriction.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: fill_log_record_nb
```
Fill a log record.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: fill_order_record_nb
```
Fill an order record.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: raise_rejected_order_nb
```
Raise an `vectorbtpro.portfolio.enums.RejectedOrderError`.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: process_order_nb
```
Process an order by executing it, saving relevant information to the logs, and returning a new state.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: order_nb
```
Create an order.

See `vectorbtpro.portfolio.enums.Order` for details on arguments.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: close_position_nb
```
Close the current position.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: order_nothing_nb
```
Convenience function to order nothing.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: check_group_lens_nb
```
Check `group_lens`.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: is_grouped_nb
```
Check if columm,ns are grouped, that is, more than one column per group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: prepare_records_nb
```
Prepare records.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: prepare_last_cash_nb
```
Prepare `last_cash`.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: prepare_last_position_nb
```
Prepare `last_position`.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: prepare_last_value_nb
```
Prepare `last_value`.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: prepare_last_pos_info_nb
```
Prepare `last_pos_info`.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: prepare_sim_out_nb
```
Prepare simulation output.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: get_trade_stats_nb
```
Get trade statistics.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: update_open_pos_info_stats_nb
```
Update statistics of an open position record using custom price.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: fill_init_pos_info_nb
```
Fill position record for an initial position.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: update_pos_info_nb
```
Update position record after filling an order.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: resolve_hl_nb
```
Resolve the current high and low.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: check_price_hit_nb
```
Check whether a target price was hit.

If `hard_price` is False, and `can_use_ohlc` and `check_open` are True and the target price
is hit by open, returns open. Otherwise, returns the actual target price.

Returns the stop price, whether it was hit by open, and whether it was hit during this bar.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: resolve_stop_exit_price_nb
```
Resolve the exit price of a stop order.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: is_limit_active_nb
```
Check whether a limit order is active.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: is_stop_active_nb
```
Check whether a stop order is active.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: is_time_stop_active_nb
```
Check whether a time stop order is active.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: should_update_stop_nb
```
Whether to update stop.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: should_update_time_stop_nb
```
Whether to update time stop.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: check_limit_expired_nb
```
Check whether limit is expired by comparing the current index with the creation index.

Returns whether the limit expires already on open, and whether the limit expires during this bar.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: resolve_limit_price_nb
```
Resolve the limit price.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: check_limit_hit_nb
```
Resolve the limit price using `resolve_limit_price_nb` and check whether it was hit.

Returns the limit price, whether it was hit before open, and whether it was hit during this bar.

If `can_use_ohlc` and `check_open` is True and the stop is hit before open, returns open.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: resolve_limit_order_price_nb
```
Resolve the limit order price of a limit order.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: resolve_stop_price_nb
```
Resolve the stop price.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: check_stop_hit_nb
```
Resolve the stop price using `resolve_stop_price_nb` and check whether it was hit.

See `check_price_hit_nb`.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: check_td_stop_hit_nb
```
Check whether TD stop was hit by comparing the current index with the initial index.

Returns whether the stop was hit already on open, and whether the stop was hit during this bar.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: check_dt_stop_hit_nb
```
Check whether DT stop was hit by comparing the current index with the initial index.

Returns whether the stop was hit already on open, and whether the stop was hit during this bar.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: check_tsl_th_hit_nb
```
Resolve the TSL threshold price using `resolve_stop_price_nb` and check whether it was hit.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: resolve_dyn_limit_price_nb
```
Resolve price dynamically.

Uses the valuation price as the left bound and order price as the right bound.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: resolve_dyn_stop_entry_price_nb
```
Resolve stop entry price dynamically.

Uses the valuation/open price as the left bound and order price as the right bound.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: get_stop_ladder_exit_size_nb
```
Get the exit size corresponding to the current step in the ladder.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: get_time_stop_ladder_exit_size_nb
```
Get the exit size corresponding to the current step in the ladder.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: is_limit_info_active_nb
```
Check whether information record for a limit order is active.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: is_stop_info_active_nb
```
Check whether information record for a stop order is active.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: is_time_stop_info_active_nb
```
Check whether information record for a time stop order is active.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: is_stop_info_ladder_active_nb
```
Check whether information record for a stop ladder is active.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: set_limit_info_nb
```
Set limit order information.

See `vectorbtpro.portfolio.enums.limit_info_dt`.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: clear_limit_info_nb
```
Clear limit order information.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: set_sl_info_nb
```
Set SL order information.

See `vectorbtpro.portfolio.enums.sl_info_dt`.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: clear_sl_info_nb
```
Clear SL order information.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: set_tsl_info_nb
```
Set TSL/TTP order information.

See `vectorbtpro.portfolio.enums.tsl_info_dt`.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: clear_tsl_info_nb
```
Clear TSL/TTP order information.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: set_tp_info_nb
```
Set TP order information.

See `vectorbtpro.portfolio.enums.tp_info_dt`.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: clear_tp_info_nb
```
Clear TP order information.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: set_time_info_nb
```
Set time order information.

See `vectorbtpro.portfolio.enums.time_info_dt`.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: clear_time_info_nb
```
Clear time order information.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: get_limit_info_target_price_nb
```
Get target price from limit order information.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: get_sl_info_target_price_nb
```
Get target price from SL order information.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: get_tsl_info_target_price_nb
```
Get target price from TSL/TTP order information.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: core.py
#### Função: get_tp_info_target_price_nb
```
Get target price from TP order information.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Docstring do Módulo
```
Numba-compiled context helper functions for portfolio simulation.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_position_nb
```
Get position of a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_position_nb
```
Get position of the current column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: col_in_position_nb
```
Check whether a column is in a position.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: in_position_nb
```
Check whether the current column is in a position.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: col_in_long_position_nb
```
Check whether a column is in a long position.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: in_long_position_nb
```
Check whether the current column is in a long position.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: col_in_short_position_nb
```
Check whether a column is in a short position.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: in_short_position_nb
```
Check whether the current column is in a short position.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_n_active_positions_nb
```
Get the number of active positions in the current group (regardless of cash sharing).

To calculate across all groups, set `all_groups` to True.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_cash_nb
```
Get cash of a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_group_cash_nb
```
Get cash of a group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_cash_nb
```
Get cash of the current column or group with cash sharing.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_debt_nb
```
Get debt of a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_debt_nb
```
Get debt of the current column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_locked_cash_nb
```
Get locked cash of a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_locked_cash_nb
```
Get locked cash of the current column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_free_cash_nb
```
Get free cash of a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_group_free_cash_nb
```
Get free cash of a group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_free_cash_nb
```
Get free cash of the current column or group with cash sharing.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: col_has_free_cash_nb
```
Check whether a column has free cash.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: group_has_free_cash_nb
```
Check whether a group has free cash.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: has_free_cash_nb
```
Check whether the current column or group with cash sharing has free cash.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_val_price_nb
```
Get valuation price of a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_val_price_nb
```
Get valuation price of the current column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_value_nb
```
Get value of a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_group_value_nb
```
Get value of a group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_value_nb
```
Get value of the current column or group with cash sharing.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_leverage_nb
```
Get leverage of a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_leverage_nb
```
Get leverage of the current column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_position_value_nb
```
Get position value of a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_group_position_value_nb
```
Get position value of a group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_position_value_nb
```
Get position value of the current column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_allocation_nb
```
Get allocation of a column in its group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_allocation_nb
```
Get allocation of the current column in the current group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_order_count_nb
```
Get number of order records for a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_order_count_nb
```
Get number of order records for the current column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_order_records_nb
```
Get order records for a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_order_records_nb
```
Get order records for the current column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: col_has_orders_nb
```
Check whether there is any order in a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: has_orders_nb
```
Check whether there is any order in the current column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_last_order_nb
```
Get the last order in a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_last_order_nb
```
Get the last order in the current column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: order_filled_nb
```
Check whether the order was filled.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: order_opened_position_nb
```
Check whether the order has opened a new position.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: order_increased_position_nb
```
Check whether the order has opened or increased an existing position.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: order_decreased_position_nb
```
Check whether the order has decreased or closed an existing position.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: order_closed_position_nb
```
Check whether the order has closed out an existing position.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: order_reversed_position_nb
```
Check whether the order has reversed an existing position.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_limit_info_nb
```
Get limit order information of a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_limit_info_nb
```
Get limit order information of the current column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_limit_target_price_nb
```
Get target price of limit order in a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_limit_target_price_nb
```
Get target price of limit order in the current column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_sl_info_nb
```
Get SL order information of a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_sl_info_nb
```
Get SL order information of the current column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_sl_target_price_nb
```
Get target price of SL order in a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_sl_target_price_nb
```
Get target price of SL order in the current column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_tsl_info_nb
```
Get TSL/TTP order information of a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_tsl_info_nb
```
Get TSL/TTP order information of the current column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_tsl_target_price_nb
```
Get target price of TSL/TTP order in a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_tsl_target_price_nb
```
Get target price of TSL/TTP order in the current column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_tp_info_nb
```
Get TP order information of a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_tp_info_nb
```
Get TP order information of the current column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_tp_target_price_nb
```
Get target price of TP order in a column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_tp_target_price_nb
```
Get target price of TP order in the current column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_entry_trade_records_nb
```
Get entry trade records of a column up to this point.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_entry_trade_records_nb
```
Get entry trade records of the current column up to this point.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_exit_trade_records_nb
```
Get exit trade records of a column up to this point.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_exit_trade_records_nb
```
Get exit trade records of the current column up to this point.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_col_position_records_nb
```
Get position records of a column up to this point.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: get_position_records_nb
```
Get position records of the current column up to this point.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: stop_group_sim_nb
```
Stop the simulation of a group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: ctx_helpers.py
#### Função: stop_sim_nb
```
Stop the simulation of the current group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_orders.py
#### Docstring do Módulo
```
Numba-compiled functions for portfolio simulation based on orders.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_orders.py
#### Função: from_orders_nb
```
Creates on order out of each element.

Iterates in the column-major order. Utilizes flexible broadcasting.

!!! note
    Should be only grouped if cash sharing is enabled.

    Single value must be passed as a 0-dim array (for example, by using `np.asarray(value)`).

Usage:
    * Buy and hold using all cash and closing price (default):

    ```pycon
    >>> from vectorbtpro import *
    >>> from vectorbtpro.records.nb import col_map_nb
    >>> from vectorbtpro.portfolio.nb import from_orders_nb, asset_flow_nb

    >>> close = np.array([1, 2, 3, 4, 5])[:, None]
    >>> sim_out = from_orders_nb(
    ...     target_shape=close.shape,
    ...     group_lens=np.array([1]),
    ...     call_seq=np.full(close.shape, 0),
    ...     close=close
    ... )
    >>> col_map = col_map_nb(sim_out.order_records['col'], close.shape[1])
    >>> asset_flow = asset_flow_nb(close.shape, sim_out.order_records, col_map)
    >>> asset_flow
    array([[100.],
           [  0.],
           [  0.],
           [  0.],
           [  0.]])
    ```
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Docstring do Módulo
```
Numba-compiled functions for portfolio simulation based on an order function.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: calc_group_value_nb
```
Calculate group value.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: calc_ctx_group_value_nb
```
Calculate group value from context.

Accepts `vectorbtpro.portfolio.enums.SegmentContext`.

Best called once from `pre_segment_func_nb`. To set the valuation price, change `last_val_price`
of the context in-place.

!!! note
    Cash sharing must be enabled.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: sort_call_seq_out_1d_nb
```
Sort call sequence `call_seq_out` based on the value of each potential order.

Accepts `vectorbtpro.portfolio.enums.SegmentContext` and other arguments, sorts `call_seq_out` in place,
and returns nothing.

Arrays `size`, `size_type`, and `direction` utilize flexible indexing; they must be 1-dim arrays
that broadcast to `group_len`.

The lengths of `order_value_out` and `call_seq_out` must match the number of columns in the group.
Array `order_value_out` must be empty and will contain sorted order values after execution.
Array `call_seq_out` must be filled with integers ranging from 0 to the number of columns in the group
(in this exact order).

Best called once from `pre_segment_func_nb`.

!!! note
    Cash sharing must be enabled and `call_seq_out` must follow `CallSeqType.Default`.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: sort_call_seq_1d_nb
```
Sort call sequence attached to `vectorbtpro.portfolio.enums.SegmentContext`.

See `sort_call_seq_out_1d_nb`.

!!! note
    Can only be used in non-flexible simulation functions.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: sort_call_seq_out_nb
```
Same as `sort_call_seq_out_1d_nb` but with `size`, `size_type`, and `direction` being 2-dim arrays.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: sort_call_seq_nb
```
Sort call sequence attached to `vectorbtpro.portfolio.enums.SegmentContext`.

See `sort_call_seq_out_nb`.

!!! note
    Can only be used in non-flexible simulation functions.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: try_order_nb
```
Execute an order without persistence.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: no_pre_func_nb
```
Placeholder preprocessing function that forwards received arguments down the stack.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: no_order_func_nb
```
Placeholder order function that returns no order.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: no_post_func_nb
```
Placeholder postprocessing function that returns nothing.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: from_order_func_nb
```
Fill order and log records by iterating over a shape and calling a range of user-defined functions.

Starting with initial cash `init_cash`, iterates over each group and column in `target_shape`,
and for each data point, generates an order using `order_func_nb`. Tries then to fulfill that
order. Upon success, updates the current state including the cash balance and the position.
Returns `vectorbtpro.portfolio.enums.SimulationOutput`.

As opposed to `from_order_func_rw_nb`, order processing happens in column-major order.
Column-major order means processing the entire column/group with all rows before moving to the next one.
See [Row- and column-major order](https://en.wikipedia.org/wiki/Row-_and_column-major_order).

Args:
    target_shape (tuple): See `vectorbtpro.portfolio.enums.SimulationContext.target_shape`.
    group_lens (array_like of int): See `vectorbtpro.portfolio.enums.SimulationContext.group_lens`.
    cash_sharing (bool): See `vectorbtpro.portfolio.enums.SimulationContext.cash_sharing`.
    call_seq (array_like of int): See `vectorbtpro.portfolio.enums.SimulationContext.call_seq`.
    init_cash (array_like of float): See `vectorbtpro.portfolio.enums.SimulationContext.init_cash`.
    init_position (array_like of float): See `vectorbtpro.portfolio.enums.SimulationContext.init_position`.
    init_price (array_like of float): See `vectorbtpro.portfolio.enums.SimulationContext.init_price`.
    cash_deposits (array_like of float): See `vectorbtpro.portfolio.enums.SimulationContext.cash_deposits`.
    cash_earnings (array_like of float): See `vectorbtpro.portfolio.enums.SimulationContext.cash_earnings`.
    segment_mask (array_like of bool): See `vectorbtpro.portfolio.enums.SimulationContext.segment_mask`.
    call_pre_segment (bool): See `vectorbtpro.portfolio.enums.SimulationContext.call_pre_segment`.
    call_post_segment (bool): See `vectorbtpro.portfolio.enums.SimulationContext.call_post_segment`.
    pre_sim_func_nb (callable): Function called before simulation.

        Can be used for creation of global arrays and setting the seed.

        Must accept `vectorbtpro.portfolio.enums.SimulationContext` and `*pre_sim_args`.
        Must return a tuple of any content, which is then passed to `pre_group_func_nb` and
        `post_group_func_nb`.
    pre_sim_args (tuple): Packed arguments passed to `pre_sim_func_nb`.
    post_sim_func_nb (callable): Function called after simulation.

        Must accept `vectorbtpro.portfolio.enums.SimulationContext` and `*post_sim_args`.
        Must return nothing.
    post_sim_args (tuple): Packed arguments passed to `post_sim_func_nb`.
    pre_group_func_nb (callable): Function called before each group.

        Must accept `vectorbtpro.portfolio.enums.GroupContext`, unpacked tuple from `pre_sim_func_nb`,
        and `*pre_group_args`. Must return a tuple of any content, which is then passed to
        `pre_segment_func_nb` and `post_segment_func_nb`.
    pre_group_args (tuple): Packed arguments passed to `pre_group_func_nb`.
    post_group_func_nb (callable): Function called after each group.

        Must accept `vectorbtpro.portfolio.enums.GroupContext`, unpacked tuple from `pre_sim_func_nb`,
        and `*post_group_args`. Must return nothing.
    post_group_args (tuple): Packed arguments passed to `post_group_func_nb`.
    pre_segment_func_nb (callable): Function called before each segment.

        Called if `segment_mask` or `call_pre_segment` is True.

        Must accept `vectorbtpro.portfolio.enums.SegmentContext`, unpacked tuple from `pre_group_func_nb`,
        and `*pre_segment_args`. Must return a tuple of any content, which is then passed to
        `order_func_nb` and `post_order_func_nb`.

        This is the right place to change call sequence and set the valuation price.
        Group re-valuation and update of the open position stats happens right after this function,
        regardless of whether it has been called.

        !!! note
            To change the call sequence of a segment, access
            `vectorbtpro.portfolio.enums.SegmentContext.call_seq_now` and change it in-place.
            Make sure to not generate any new arrays as it may negatively impact performance.
            Assigning `vectorbtpro.portfolio.enums.SegmentContext.call_seq_now` as any other context
            (named tuple) value is not supported. See `vectorbtpro.portfolio.enums.SegmentContext.call_seq_now`.

        !!! note
            You can override elements of `last_val_price` to manipulate group valuation.
            See `vectorbtpro.portfolio.enums.SimulationContext.last_val_price`.
    pre_segment_args (tuple): Packed arguments passed to `pre_segment_func_nb`.
    post_segment_func_nb (callable): Function called after each segment.

        Called if `segment_mask` or `call_post_segment` is True.

        Addition of cash_earnings, the final group re-valuation, and the final update of the open
        position stats happens right before this function, regardless of whether it has been called.

        The passed context represents the final state of each segment, thus makes sure
        to do any changes before this function is called.

        Must accept `vectorbtpro.portfolio.enums.SegmentContext`, unpacked tuple from `pre_group_func_nb`,
        and `*post_segment_args`. Must return nothing.
    post_segment_args (tuple): Packed arguments passed to `post_segment_func_nb`.
    order_func_nb (callable): Order generation function.

        Used for either generating an order or skipping.

        Must accept `vectorbtpro.portfolio.enums.OrderContext`, unpacked tuple from `pre_segment_func_nb`,
        and `*order_args`. Must return `vectorbtpro.portfolio.enums.Order`.

        !!! note
            If the returned order has been rejected, there is no way of issuing a new order.
            You should make sure that the order passes, for example, by using `try_order_nb`.

            To have a greater freedom in order management, use `from_flex_order_func_nb`.
    order_args (tuple): Arguments passed to `order_func_nb`.
    post_order_func_nb (callable): Callback that is called after the order has been processed.

        Used for checking the order status and doing some post-processing.

        Must accept `vectorbtpro.portfolio.enums.PostOrderContext`, unpacked tuple from
        `pre_segment_func_nb`, and `*post_order_args`. Must return nothing.
    post_order_args (tuple): Arguments passed to `post_order_func_nb`.
    index (array): See `vectorbtpro.portfolio.enums.SimulationContext.index`.
    freq (int): See `vectorbtpro.portfolio.enums.SimulationContext.freq`.
    open (array_like of float): See `vectorbtpro.portfolio.enums.SimulationContext.open`.
    high (array_like of float): See `vectorbtpro.portfolio.enums.SimulationContext.high`.
    low (array_like of float): See `vectorbtpro.portfolio.enums.SimulationContext.low`.
    close (array_like of float): See `vectorbtpro.portfolio.enums.SimulationContext.close`.
    bm_close (array_like of float): See `vectorbtpro.portfolio.enums.SimulationContext.bm_close`.
    ffill_val_price (bool): See `vectorbtpro.portfolio.enums.SimulationContext.ffill_val_price`.
    update_value (bool): See `vectorbtpro.portfolio.enums.SimulationContext.update_value`.
    fill_pos_info (bool): See `vectorbtpro.portfolio.enums.SimulationContext.fill_pos_info`.
    track_value (bool): See `vectorbtpro.portfolio.enums.SimulationContext.track_value`.
    max_order_records (int): The max number of order records expected to be filled at each column.
    max_log_records (int): The max number of log records expected to be filled at each column.
    in_outputs (bool): See `vectorbtpro.portfolio.enums.SimulationContext.in_outputs`.

!!! note
    Remember that indexing of 2-dim arrays in vectorbt follows that of pandas: `a[i, col]`.

!!! warning
    You can only safely access data of columns that are to the left of the current group and
    rows that are to the top of the current row within the same group. Other data points have
    not been processed yet and thus empty. Accessing them will not trigger any errors or warnings,
    but provide you with arbitrary data (see [np.empty](https://numpy.org/doc/stable/reference/generated/numpy.empty.html)).

Call hierarchy:
    Like most things in the vectorbt universe, simulation is also done by iterating over a (imaginary) frame.
    This frame consists of two dimensions: time (rows) and assets/features (columns).
    Each element of this frame is a potential order, which gets generated by calling an order function.

    The question is: how do we move across this frame to simulate trading? There are two movement patterns:
    column-major (as done by `from_order_func_nb`) and row-major order (as done by `from_order_func_rw_nb`).
    In each of these patterns, we are always moving from top to bottom (time axis) and from left to right
    (asset/feature axis); the only difference between them is across which axis we are moving faster:
    do we want to process each column first (thus assuming that columns are independent) or each row?
    Choosing between them is mostly a matter of preference, but it also makes different data being
    available when generating an order.

    The frame is further divided into "blocks": columns, groups, rows, segments, and elements.
    For example, columns can be grouped into groups that may or may not share the same capital.
    Regardless of capital sharing, each collection of elements within a group and a time step is called
    a segment, which simply defines a single context (such as shared capital) for one or multiple orders.
    Each segment can also define a custom sequence (a so-called call sequence) in which orders are executed.

    You can imagine each of these blocks as a rectangle drawn over different parts of the frame,
    and having its own context and pre/post-processing function. The pre-processing function is a
    simple callback that is called before entering the block, and can be provided by the user to, for example,
    prepare arrays or do some custom calculations. It must return a tuple (can be empty) that is then unpacked and
    passed as arguments to the pre- and postprocessing function coming next in the call hierarchy.
    The postprocessing function can be used, for example, to write user-defined arrays such as returns.

    ```plaintext
    1. pre_sim_out = pre_sim_func_nb(SimulationContext, *pre_sim_args)
        2. pre_group_out = pre_group_func_nb(GroupContext, *pre_sim_out, *pre_group_args)
            3. if call_pre_segment or segment_mask: pre_segment_out = pre_segment_func_nb(SegmentContext, *pre_group_out, *pre_segment_args)
                4. if segment_mask: order = order_func_nb(OrderContext, *pre_segment_out, *order_args)
                5. if order: post_order_func_nb(PostOrderContext, *pre_segment_out, *post_order_args)
                ...
            6. if call_post_segment or segment_mask: post_segment_func_nb(SegmentContext, *pre_group_out, *post_segment_args)
            ...
        7. post_group_func_nb(GroupContext, *pre_sim_out, *post_group_args)
        ...
    8. post_sim_func_nb(SimulationContext, *post_sim_args)
    ```

    Let's demonstrate a frame with one group of two columns and one group of one column, and the
    following call sequence:

    ```plaintext
    array([[0, 1, 0],
           [1, 0, 0]])
    ```

    ![](/assets/images/api/from_order_func_nb.svg){: loading=lazy style="width:800px;" }

    And here is the context information available at each step:

    ![](/assets/images/api/context_info.svg){: loading=lazy style="width:700px;" }

Usage:
    * Create a group of three assets together sharing 100$ and simulate an equal-weighted portfolio
    that rebalances every second tick, all without leaving Numba:

    ```pycon
    >>> from vectorbtpro import *

    >>> @njit
    ... def pre_sim_func_nb(c):
    ...     print('before simulation')
    ...     # Create a temporary array and pass it down the stack
    ...     order_value_out = np.empty(c.target_shape[1], dtype=np.float_)
    ...     return (order_value_out,)

    >>> @njit
    ... def pre_group_func_nb(c, order_value_out):
    ...     print('\tbefore group', c.group)
    ...     # Forward down the stack (you can omit pre_group_func_nb entirely)
    ...     return (order_value_out,)

    >>> @njit
    ... def pre_segment_func_nb(c, order_value_out, size, price, size_type, direction):
    ...     print('\t\tbefore segment', c.i)
    ...     for col in range(c.from_col, c.to_col):
    ...         # Here we use order price for group valuation
    ...         c.last_val_price[col] = vbt.pf_nb.select_from_col_nb(c, col, price)
    ...
    ...     # Reorder call sequence of this segment such that selling orders come first and buying last
    ...     # Rearranges c.call_seq_now based on order value (size, size_type, direction, and val_price)
    ...     # Utilizes flexible indexing using select_from_col_nb (as we did above)
    ...     vbt.pf_nb.sort_call_seq_nb(
    ...         c,
    ...         size,
    ...         size_type,
    ...         direction,
    ...         order_value_out[c.from_col:c.to_col]
    ...     )
    ...     # Forward nothing
    ...     return ()

    >>> @njit
    ... def order_func_nb(c, size, price, size_type, direction, fees, fixed_fees, slippage):
    ...     print('\t\t\tcreating order', c.call_idx, 'at column', c.col)
    ...     # Create and return an order
    ...     return vbt.pf_nb.order_nb(
    ...         size=vbt.pf_nb.select_nb(c, size),
    ...         price=vbt.pf_nb.select_nb(c, price),
    ...         size_type=vbt.pf_nb.select_nb(c, size_type),
    ...         direction=vbt.pf_nb.select_nb(c, direction),
    ...         fees=vbt.pf_nb.select_nb(c, fees),
    ...         fixed_fees=vbt.pf_nb.select_nb(c, fixed_fees),
    ...         slippage=vbt.pf_nb.select_nb(c, slippage)
    ...     )

    >>> @njit
    ... def post_order_func_nb(c):
    ...     print('\t\t\t\torder status:', c.order_result.status)
    ...     return None

    >>> @njit
    ... def post_segment_func_nb(c, order_value_out):
    ...     print('\t\tafter segment', c.i)
    ...     return None

    >>> @njit
    ... def post_group_func_nb(c, order_value_out):
    ...     print('\tafter group', c.group)
    ...     return None

    >>> @njit
    ... def post_sim_func_nb(c):
    ...     print('after simulation')
    ...     return None

    >>> target_shape = (5, 3)
    >>> np.random.seed(42)
    >>> group_lens = np.array([3])  # one group of three columns
    >>> cash_sharing = True
    >>> segment_mask = np.array([True, False, True, False, True])[:, None]
    >>> price = close = np.random.uniform(1, 10, size=target_shape)
    >>> size = np.array([[1 / target_shape[1]]])  # custom flexible arrays must be 2-dim
    >>> size_type = np.array([[vbt.pf_enums.SizeType.TargetPercent]])
    >>> direction = np.array([[vbt.pf_enums.Direction.LongOnly]])
    >>> fees = np.array([[0.001]])
    >>> fixed_fees = np.array([[1.]])
    >>> slippage = np.array([[0.001]])

    >>> sim_out = vbt.pf_nb.from_order_func_nb(
    ...     target_shape,
    ...     group_lens,
    ...     cash_sharing,
    ...     segment_mask=segment_mask,
    ...     pre_sim_func_nb=pre_sim_func_nb,
    ...     post_sim_func_nb=post_sim_func_nb,
    ...     pre_group_func_nb=pre_group_func_nb,
    ...     post_group_func_nb=post_group_func_nb,
    ...     pre_segment_func_nb=pre_segment_func_nb,
    ...     pre_segment_args=(size, price, size_type, direction),
    ...     post_segment_func_nb=post_segment_func_nb,
    ...     order_func_nb=order_func_nb,
    ...     order_args=(size, price, size_type, direction, fees, fixed_fees, slippage),
    ...     post_order_func_nb=post_order_func_nb
    ... )
    before simulation
        before group 0
            before segment 0
                creating order 0 at column 0
                    order status: 0
                creating order 1 at column 1
                    order status: 0
                creating order 2 at column 2
                    order status: 0
            after segment 0
            before segment 2
                creating order 0 at column 1
                    order status: 0
                creating order 1 at column 2
                    order status: 0
                creating order 2 at column 0
                    order status: 0
            after segment 2
            before segment 4
                creating order 0 at column 0
                    order status: 0
                creating order 1 at column 2
                    order status: 0
                creating order 2 at column 1
                    order status: 0
            after segment 4
        after group 0
    after simulation

    >>> pd.DataFrame.from_records(sim_out.order_records)
       id  col  idx       size     price      fees  side
    0   0    0    0   7.626262  4.375232  1.033367     0
    1   1    0    2   5.210115  1.524275  1.007942     0
    2   2    0    4   7.899568  8.483492  1.067016     1
    3   0    1    0   3.488053  9.565985  1.033367     0
    4   1    1    2   0.920352  8.786790  1.008087     1
    5   2    1    4  10.713236  2.913963  1.031218     0
    6   0    2    0   3.972040  7.595533  1.030170     0
    7   1    2    2   0.448747  6.403625  1.002874     1
    8   2    2    4  12.378281  2.639061  1.032667     0

    >>> col_map = vbt.rec_nb.col_map_nb(sim_out.order_records['col'], target_shape[1])
    >>> asset_flow = vbt.pf_nb.asset_flow_nb(target_shape, sim_out.order_records, col_map)
    >>> assets = vbt.pf_nb.assets_nb(asset_flow)
    >>> asset_value = vbt.pf_nb.asset_value_nb(close, assets)
    >>> vbt.Scatter(data=asset_value).fig.show()
    ```

    ![](/assets/images/api/from_order_func_nb_example.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/from_order_func_nb_example.dark.svg#only-dark){: .iimg loading=lazy }

    Note that the last order in a group with cash sharing is always disadvantaged
    as it has a bit less funds than the previous orders due to costs, which are not
    included when valuating the group.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: from_order_func_rw_nb
```
Same as `from_order_func_nb`, but iterates in row-major order.

Row-major order means processing the entire row with all groups/columns before moving to the next one.

The main difference is that instead of `pre_group_func_nb` it now exposes `pre_row_func_nb`,
which is executed per entire row. It must accept `vectorbtpro.portfolio.enums.RowContext`.

!!! note
    Function `pre_row_func_nb` is only called if there is at least on active segment in
    the row. Functions `pre_segment_func_nb` and `order_func_nb` are only called if their
    segment is active. If the main task of `pre_row_func_nb` is to activate/deactivate segments,
    all segments must be activated by default to allow `pre_row_func_nb` to be called.

!!! warning
    You can only safely access data points that are to the left of the current group and
    rows that are to the top of the current row.

Call hierarchy:
    ```plaintext
    1. pre_sim_out = pre_sim_func_nb(SimulationContext, *pre_sim_args)
        2. pre_row_out = pre_row_func_nb(RowContext, *pre_sim_out, *pre_row_args)
            3. if call_pre_segment or segment_mask: pre_segment_out = pre_segment_func_nb(SegmentContext, *pre_row_out, *pre_segment_args)
                4. if segment_mask: order = order_func_nb(OrderContext, *pre_segment_out, *order_args)
                5. if order: post_order_func_nb(PostOrderContext, *pre_segment_out, *post_order_args)
                ...
            6. if call_post_segment or segment_mask: post_segment_func_nb(SegmentContext, *pre_row_out, *post_segment_args)
            ...
        7. post_row_func_nb(RowContext, *pre_sim_out, *post_row_args)
        ...
    8. post_sim_func_nb(SimulationContext, *post_sim_args)
    ```

    Let's illustrate the same example as in `from_order_func_nb` but adapted for this function:

    ![](/assets/images/api/from_order_func_rw_nb.svg){: loading=lazy style="width:800px;" }

Usage:
    * Running the same example as in `from_order_func_nb` but adapted for this function:

    ```pycon
    >>> @njit
    ... def pre_row_func_nb(c, order_value_out):
    ...     print('\tbefore row', c.i)
    ...     # Forward down the stack
    ...     return (order_value_out,)

    >>> @njit
    ... def post_row_func_nb(c, order_value_out):
    ...     print('\tafter row', c.i)
    ...     return None

    >>> sim_out = vbt.pf_nb.from_order_func_rw_nb(
    ...     target_shape,
    ...     group_lens,
    ...     cash_sharing,
    ...     segment_mask=segment_mask,
    ...     pre_sim_func_nb=pre_sim_func_nb,
    ...     post_sim_func_nb=post_sim_func_nb,
    ...     pre_row_func_nb=pre_row_func_nb,
    ...     post_row_func_nb=post_row_func_nb,
    ...     pre_segment_func_nb=pre_segment_func_nb,
    ...     pre_segment_args=(size, price, size_type, direction),
    ...     post_segment_func_nb=post_segment_func_nb,
    ...     order_func_nb=order_func_nb,
    ...     order_args=(size, price, size_type, direction, fees, fixed_fees, slippage),
    ...     post_order_func_nb=post_order_func_nb
    ... )
    before simulation
        before row 0
            before segment 0
                creating order 0 at column 0
                    order status: 0
                creating order 1 at column 1
                    order status: 0
                creating order 2 at column 2
                    order status: 0
            after segment 0
        after row 0
        before row 1
        after row 1
        before row 2
            before segment 2
                creating order 0 at column 1
                    order status: 0
                creating order 1 at column 2
                    order status: 0
                creating order 2 at column 0
                    order status: 0
            after segment 2
        after row 2
        before row 3
        after row 3
        before row 4
            before segment 4
                creating order 0 at column 0
                    order status: 0
                creating order 1 at column 2
                    order status: 0
                creating order 2 at column 1
                    order status: 0
            after segment 4
        after row 4
    after simulation
    ```
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: no_flex_order_func_nb
```
Placeholder flexible order function that returns "break" column and no order.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: from_flex_order_func_nb
```
Same as `from_order_func_nb`, but with no predefined call sequence.

In contrast to `order_func_nb` in`from_order_func_nb`, `post_order_func_nb` is a segment-level order function
that returns a column along with the order, and gets repeatedly called until some condition is met.
This allows multiple orders to be issued within a single element and in an arbitrary order.

The order function must accept `vectorbtpro.portfolio.enums.FlexOrderContext`, unpacked tuple from
`pre_segment_func_nb`, and `*flex_order_args`. Must return column and `vectorbtpro.portfolio.enums.Order`.
To break out of the loop, return column of -1.

!!! note
    Since one element can now accommodate multiple orders, you may run into "order_records index out of range"
    exception. In this case, you must increase `max_order_records`. This cannot be done automatically and
    dynamically to avoid performance degradation.

Call hierarchy:
    ```plaintext
    1. pre_sim_out = pre_sim_func_nb(SimulationContext, *pre_sim_args)
        2. pre_group_out = pre_group_func_nb(GroupContext, *pre_sim_out, *pre_group_args)
            3. if call_pre_segment or segment_mask: pre_segment_out = pre_segment_func_nb(SegmentContext, *pre_group_out, *pre_segment_args)
                while col != -1:
                    4. if segment_mask: col, order = flex_order_func_nb(FlexOrderContext, *pre_segment_out, *flex_order_args)
                    5. if order: post_order_func_nb(PostOrderContext, *pre_segment_out, *post_order_args)
                    ...
            6. if call_post_segment or segment_mask: post_segment_func_nb(SegmentContext, *pre_group_out, *post_segment_args)
            ...
        7. post_group_func_nb(GroupContext, *pre_sim_out, *post_group_args)
        ...
    8. post_sim_func_nb(SimulationContext, *post_sim_args)
    ```

    Let's illustrate the same example as in `from_order_func_nb` but adapted for this function:

    ![](/assets/images/api/from_flex_order_func_nb.svg){: loading=lazy style="width:800px;" }

Usage:
    * The same example as in `from_order_func_nb`:

    ```pycon
    >>> from vectorbtpro import *

    >>> @njit
    ... def pre_sim_func_nb(c):
    ...     # Create temporary arrays and pass them down the stack
    ...     print('before simulation')
    ...     order_value_out = np.empty(c.target_shape[1], dtype=np.float_)
    ...     call_seq_out = np.empty(c.target_shape[1], dtype=np.int_)
    ...     return (order_value_out, call_seq_out)

    >>> @njit
    ... def pre_group_func_nb(c, order_value_out, call_seq_out):
    ...     print('\tbefore group', c.group)
    ...     return (order_value_out, call_seq_out)

    >>> @njit
    ... def pre_segment_func_nb(c, order_value_out, call_seq_out, size, price, size_type, direction):
    ...     print('\t\tbefore segment', c.i)
    ...     for col in range(c.from_col, c.to_col):
    ...         # Here we use order price for group valuation
    ...         c.last_val_price[col] = vbt.pf_nb.select_from_col_nb(c, col, price)
    ...
    ...     # Same as for from_order_func_nb, but since we don't have a predefined c.call_seq_now anymore,
    ...     # we need to store our new call sequence somewhere else
    ...     call_seq_out[:] = np.arange(c.group_len)
    ...     vbt.pf_nb.sort_call_seq_out_nb(
    ...         c,
    ...         size,
    ...         size_type,
    ...         direction,
    ...         order_value_out[c.from_col:c.to_col],
    ...         call_seq_out[c.from_col:c.to_col]
    ...     )
    ...
    ...     # Forward the sorted call sequence
    ...     return (call_seq_out,)

    >>> @njit
    ... def flex_order_func_nb(c, call_seq_out, size, price, size_type, direction, fees, fixed_fees, slippage):
    ...     if c.call_idx < c.group_len:
    ...         col = c.from_col + call_seq_out[c.call_idx]
    ...         print('\t\t\tcreating order', c.call_idx, 'at column', col)
    ...         # # Create and return an order
    ...         return col, vbt.pf_nb.order_nb(
    ...             size=vbt.pf_nb.select_from_col_nb(c, col, size),
    ...             price=vbt.pf_nb.select_from_col_nb(c, col, price),
    ...             size_type=vbt.pf_nb.select_from_col_nb(c, col, size_type),
    ...             direction=vbt.pf_nb.select_from_col_nb(c, col, direction),
    ...             fees=vbt.pf_nb.select_from_col_nb(c, col, fees),
    ...             fixed_fees=vbt.pf_nb.select_from_col_nb(c, col, fixed_fees),
    ...             slippage=vbt.pf_nb.select_from_col_nb(c, col, slippage)
    ...         )
    ...     # All columns already processed -> break the loop
    ...     print('\t\t\tbreaking out of the loop')
    ...     return -1, vbt.pf_nb.order_nothing_nb()

    >>> @njit
    ... def post_order_func_nb(c, call_seq_out):
    ...     print('\t\t\t\torder status:', c.order_result.status)
    ...     return None

    >>> @njit
    ... def post_segment_func_nb(c, order_value_out, call_seq_out):
    ...     print('\t\tafter segment', c.i)
    ...     return None

    >>> @njit
    ... def post_group_func_nb(c, order_value_out, call_seq_out):
    ...     print('\tafter group', c.group)
    ...     return None

    >>> @njit
    ... def post_sim_func_nb(c):
    ...     print('after simulation')
    ...     return None

    >>> target_shape = (5, 3)
    >>> np.random.seed(42)
    >>> group_lens = np.array([3])  # one group of three columns
    >>> cash_sharing = True
    >>> segment_mask = np.array([True, False, True, False, True])[:, None]
    >>> price = close = np.random.uniform(1, 10, size=target_shape)
    >>> size = np.array([[1 / target_shape[1]]])  # custom flexible arrays must be 2-dim
    >>> size_type = np.array([[vbt.pf_enums.SizeType.TargetPercent]])
    >>> direction = np.array([[vbt.pf_enums.Direction.LongOnly]])
    >>> fees = np.array([[0.001]])
    >>> fixed_fees = np.array([[1.]])
    >>> slippage = np.array([[0.001]])

    >>> sim_out = vbt.pf_nb.from_flex_order_func_nb(
    ...     target_shape,
    ...     group_lens,
    ...     cash_sharing,
    ...     segment_mask=segment_mask,
    ...     pre_sim_func_nb=pre_sim_func_nb,
    ...     post_sim_func_nb=post_sim_func_nb,
    ...     pre_group_func_nb=pre_group_func_nb,
    ...     post_group_func_nb=post_group_func_nb,
    ...     pre_segment_func_nb=pre_segment_func_nb,
    ...     pre_segment_args=(size, price, size_type, direction),
    ...     post_segment_func_nb=post_segment_func_nb,
    ...     flex_order_func_nb=flex_order_func_nb,
    ...     flex_order_args=(size, price, size_type, direction, fees, fixed_fees, slippage),
    ...     post_order_func_nb=post_order_func_nb
    ... )
    before simulation
        before group 0
            before segment 0
                creating order 0 at column 0
                    order status: 0
                creating order 1 at column 1
                    order status: 0
                creating order 2 at column 2
                    order status: 0
                breaking out of the loop
            after segment 0
            before segment 2
                creating order 0 at column 1
                    order status: 0
                creating order 1 at column 2
                    order status: 0
                creating order 2 at column 0
                    order status: 0
                breaking out of the loop
            after segment 2
            before segment 4
                creating order 0 at column 0
                    order status: 0
                creating order 1 at column 2
                    order status: 0
                creating order 2 at column 1
                    order status: 0
                breaking out of the loop
            after segment 4
        after group 0
    after simulation
    ```
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: from_flex_order_func_rw_nb
```
Same as `from_flex_order_func_nb`, but iterates using row-major order, with the rows
changing fastest, and the columns/groups changing slowest.

Call hierarchy:
    ```plaintext
    1. pre_sim_out = pre_sim_func_nb(SimulationContext, *pre_sim_args)
        2. pre_row_out = pre_row_func_nb(RowContext, *pre_sim_out, *pre_row_args)
            3. if call_pre_segment or segment_mask: pre_segment_out = pre_segment_func_nb(SegmentContext, *pre_row_out, *pre_segment_args)
                while col != -1:
                    4. if segment_mask: col, order = flex_order_func_nb(FlexOrderContext, *pre_segment_out, *flex_order_args)
                    5. if order: post_order_func_nb(PostOrderContext, *pre_segment_out, *post_order_args)
                    ...
            6. if call_post_segment or segment_mask: post_segment_func_nb(SegmentContext, *pre_row_out, *post_segment_args)
            ...
        7. post_row_func_nb(RowContext, *pre_sim_out, *post_row_args)
        ...
    8. post_sim_func_nb(SimulationContext, *post_sim_args)
    ```

    Let's illustrate the same example as in `from_order_func_nb` but adapted for this function:

    ```pycon
    >>> @njit
    ... def pre_row_func_nb(c, order_value_out, call_seq_out):
    ...     print('\tbefore row', c.i)
    ...     return (order_value_out, call_seq_out)

    >>> @njit
    ... def post_row_func_nb(c, order_value_out, call_seq_out):
    ...     print('\tafter row', c.i)
    ...     return None

    >>> sim_out = vbt.pf_nb.from_flex_order_func_rw_nb(
    ...     target_shape,
    ...     group_lens,
    ...     cash_sharing,
    ...     segment_mask=segment_mask,
    ...     pre_sim_func_nb=pre_sim_func_nb,
    ...     post_sim_func_nb=post_sim_func_nb,
    ...     pre_row_func_nb=pre_row_func_nb,
    ...     post_row_func_nb=post_row_func_nb,
    ...     pre_segment_func_nb=pre_segment_func_nb,
    ...     pre_segment_args=(size, price, size_type, direction),
    ...     post_segment_func_nb=post_segment_func_nb,
    ...     flex_order_func_nb=flex_order_func_nb,
    ...     flex_order_args=(size, price, size_type, direction, fees, fixed_fees, slippage),
    ...     post_order_func_nb=post_order_func_nb
    ... )
    ```

    ![](/assets/images/api/from_flex_order_func_rw_nb.svg){: loading=lazy style="width:800px;" }
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: set_val_price_nb
```
Override valuation price in a context.

Allows specifying a valuation price of positive infinity (takes the current price)
and negative infinity (takes the latest valuation price).
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: def_pre_segment_func_nb
```
Pre-segment function that overrides the valuation price and optionally sorts the call sequence.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: def_order_func_nb
```
Order function that creates an order based on default information.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: def_flex_pre_segment_func_nb
```
Flexible pre-segment function that overrides the valuation price and optionally sorts the call sequence.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_order_func.py
#### Função: def_flex_order_func_nb
```
Flexible order function that creates an order based on default information.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Docstring do Módulo
```
Numba-compiled functions for portfolio simulation based on signals.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Função: resolve_pending_conflict_nb
```
Resolve any conflict between a pending signal and a user-defined signal.

Returns whether to keep the pending signal and execute the user signal.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Função: generate_stop_signal_nb
```
Generate stop signal and change accumulation if needed.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Função: resolve_signal_conflict_nb
```
Resolve any conflict between an entry and an exit.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Função: resolve_dir_conflict_nb
```
Resolve any direction conflict between a long entry and a short entry.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Função: resolve_opposite_entry_nb
```
Resolve opposite entry.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Função: signal_to_size_nb
```
Translate direction-aware signals into size, size type, and direction.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Função: prepare_fs_records_nb
```
Prepare from-signals records.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Função: from_basic_signals_nb
```
Simulate given basic signals (no limit or stop orders).

Iterates in the column-major order. Utilizes flexible broadcasting.

!!! note
    Should be only grouped if cash sharing is enabled.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Função: from_signals_nb
```
Simulate given signals.

Iterates in the column-major order. Utilizes flexible broadcasting.

!!! note
    Should be only grouped if cash sharing is enabled.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Função: init_FSInOutputs_nb
```
Initialize `vectorbtpro.portfolio.enums.FSInOutputs`.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Função: no_signal_func_nb
```
Placeholder signal function that returns no signal.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Função: from_signal_func_nb
```
Simulate given a signal function.

Iterates in the column-major order. Utilizes flexible broadcasting.

`signal_func_nb` is a user-defined signal generation function that is called at each row and column
(= element). It must accept the context of the type `vectorbtpro.portfolio.enums.SignalContext`
and return 4 signals: long entry, long exit, short entry, and short exit.

`post_signal_func_nb` is a user-defined post-signal function that is called after an order has been processed.
It must accept the context of the type `vectorbtpro.portfolio.enums.PostSignalContext` and return nothing.

`post_segment_func_nb` is a user-defined post-segment function that is called after each row and group
(= segment). It must accept the context of the type `vectorbtpro.portfolio.enums.SignalSegmentContext`
and return nothing.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Função: dir_to_ls_signals_nb
```
Convert direction-unaware to direction-aware signals.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Função: no_adjust_func_nb
```
Placeholder adjustment function.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Função: holding_enex_signal_func_nb
```
Resolve direction-aware signals for holding.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Função: dir_signal_func_nb
```
Resolve direction-aware signals out of entries, exits, and direction.

The direction of each pair of signals is taken from `direction` argument:

* True, True, `Direction.LongOnly` -> True, True, False, False
* True, True, `Direction.ShortOnly` -> False, False, True, True
* True, True, `Direction.Both` -> True, False, True, False

Best to use when the direction doesn't change throughout time.

Prior to returning the signals, calls user-defined `adjust_func_nb`, which can be used to adjust
stop values in the context. Must accept `vectorbtpro.portfolio.enums.SignalContext` and `*adjust_args`,
and return nothing.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Função: ls_signal_func_nb
```
Get an element of direction-aware signals.

The direction is already built into the arrays. Best to use when the direction changes frequently
(for example, if you have one indicator providing long signals and one providing short signals).

Prior to returning the signals, calls user-defined `adjust_func_nb`, which can be used to adjust
stop values in the context. Must accept `vectorbtpro.portfolio.enums.SignalContext` and `*adjust_args`,
and return nothing.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: from_signals.py
#### Função: order_signal_func_nb
```
Resolve direction-aware signals out of orders.

You must ensure that `size`, `size_type`, `min_size`, and `max_size` are writeable non-flexible arrays
and accumulation is enabled.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: iter_.py
#### Docstring do Módulo
```
Numba-compiled functions for iterative portfolio simulation.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: iter_.py
#### Função: select_nb
```
Get the current element using flexible indexing.

If any of the arguments are None, will use the respective value from the context.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: iter_.py
#### Função: select_from_col_nb
```
Get the current element from a specific column using flexible indexing.

If any of the arguments are None, will use the respective value from the context.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: iter_.py
#### Função: iter_above_nb
```
Call `vectorbtpro.generic.nb.iter_.iter_above_nb` on the context.

If any of the arguments are None, will use the respective value from the context.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: iter_.py
#### Função: iter_below_nb
```
Call `vectorbtpro.generic.nb.iter_.iter_below_nb` on the context.

If any of the arguments are None, will use the respective value from the context.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: iter_.py
#### Função: iter_crossed_above_nb
```
Call `vectorbtpro.generic.nb.iter_.iter_crossed_above_nb` on the context.

If any of the arguments are None, will use the respective value from the context.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: iter_.py
#### Função: iter_crossed_below_nb
```
Call `vectorbtpro.generic.nb.iter_.iter_crossed_below_nb` on the context.

If any of the arguments are None, will use the respective value from the context.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Docstring do Módulo
```
Numba-compiled functions for portfolio records.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: records_within_sim_range_nb
```
Return records within simulation range.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: apply_weights_to_orders_nb
```
Apply weights to order records.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: weighted_price_reduce_meta_nb
```
Size-weighted price average.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: fill_trade_record_nb
```
Fill a trade record.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: fill_entry_trades_in_position_nb
```
Fill entry trades located within a single position.

Returns the next trade id.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: get_entry_trades_nb
```
Fill entry trade records by aggregating order records.

Entry trade records are buy orders in a long position and sell orders in a short position.

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> close = order_price = np.array([
    ...     [1, 6],
    ...     [2, 5],
    ...     [3, 4],
    ...     [4, 3],
    ...     [5, 2],
    ...     [6, 1]
    ... ])
    >>> size = np.array([
    ...     [1, -1],
    ...     [0.1, -0.1],
    ...     [-1, 1],
    ...     [-0.1, 0.1],
    ...     [1, -1],
    ...     [-2, 2]
    ... ])
    >>> target_shape = close.shape
    >>> group_lens = np.full(target_shape[1], 1)
    >>> init_cash = np.full(target_shape[1], 100)

    >>> sim_out = vbt.pf_nb.from_orders_nb(
    ...     target_shape,
    ...     group_lens,
    ...     init_cash=init_cash,
    ...     size=size,
    ...     price=close,
    ...     fees=np.asarray([[0.01]]),
    ...     slippage=np.asarray([[0.01]])
    ... )

    >>> col_map = vbt.rec_nb.col_map_nb(sim_out.order_records['col'], target_shape[1])
    >>> entry_trade_records = vbt.pf_nb.get_entry_trades_nb(sim_out.order_records, close, col_map)
    >>> pd.DataFrame.from_records(entry_trade_records)
       id  col  size  entry_order_id  entry_idx  entry_price  entry_fees  \
    0   0    0   1.0               0          0         1.01     0.01010
    1   1    0   0.1               1          1         2.02     0.00202
    2   2    0   1.0               4          4         5.05     0.05050
    3   3    0   1.0               5          5         5.94     0.05940
    4   0    1   1.0               0          0         5.94     0.05940
    5   1    1   0.1               1          1         4.95     0.00495
    6   2    1   1.0               4          4         1.98     0.01980
    7   3    1   1.0               5          5         1.01     0.01010

       exit_order_id  exit_idx  exit_price  exit_fees       pnl    return  \
    0              3         3    3.060000   0.030600  2.009300  1.989406
    1              3         3    3.060000   0.003060  0.098920  0.489703
    2              5         5    5.940000   0.059400  0.780100  0.154475
    3             -1         5    6.000000   0.000000 -0.119400 -0.020101
    4              3         3    3.948182   0.039482  1.892936  0.318676
    5              3         3    3.948182   0.003948  0.091284  0.184411
    6              5         5    1.010000   0.010100  0.940100  0.474798
    7             -1         5    1.000000   0.000000 -0.020100 -0.019901

       direction  status  parent_id
    0          0       1          0
    1          0       1          0
    2          0       1          1
    3          1       0          2
    4          1       1          0
    5          1       1          0
    6          1       1          1
    7          0       0          2
    ```
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: get_exit_trades_nb
```
Fill exit trade records by aggregating order records.

Exit trade records are sell orders in a long position and buy orders in a short position.

Usage:
    * Building upon the example in `get_exit_trades_nb`:

    ```pycon
    >>> exit_trade_records = vbt.pf_nb.get_exit_trades_nb(sim_out.order_records, close, col_map)
    >>> pd.DataFrame.from_records(exit_trade_records)
       id  col  size  entry_order_id  entry_idx  entry_price  entry_fees  \
    0   0    0   1.0               0          0     1.101818    0.011018
    1   1    0   0.1               0          0     1.101818    0.001102
    2   2    0   1.0               4          4     5.050000    0.050500
    3   3    0   1.0               5          5     5.940000    0.059400
    4   0    1   1.0               0          0     5.850000    0.058500
    5   1    1   0.1               0          0     5.850000    0.005850
    6   2    1   1.0               4          4     1.980000    0.019800
    7   3    1   1.0               5          5     1.010000    0.010100

       exit_order_id  exit_idx  exit_price  exit_fees       pnl    return  \
    0              2         2        2.97    0.02970  1.827464  1.658589
    1              3         3        3.96    0.00396  0.280756  2.548119
    2              5         5        5.94    0.05940  0.780100  0.154475
    3             -1         5        6.00    0.00000 -0.119400 -0.020101
    4              2         2        4.04    0.04040  1.711100  0.292496
    5              3         3        3.03    0.00303  0.273120  0.466872
    6              5         5        1.01    0.01010  0.940100  0.474798
    7             -1         5        1.00    0.00000 -0.020100 -0.019901

       direction  status  parent_id
    0          0       1          0
    1          0       1          0
    2          0       1          1
    3          1       0          2
    4          1       1          0
    5          1       1          0
    6          1       1          1
    7          0       0          2
    ```
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: fill_position_record_nb
```
Fill a position record by aggregating trade records.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: copy_trade_record_nb
```
Copy a trade record.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: get_positions_nb
```
Fill position records by aggregating trade records.

Trades can be entry trades, exit trades, and even positions themselves - all will produce the same results.

Usage:
    * Building upon the example in `get_exit_trades_nb`:

    ```pycon
    >>> col_map = vbt.rec_nb.col_map_nb(exit_trade_records['col'], target_shape[1])
    >>> position_records = vbt.pf_nb.get_positions_nb(exit_trade_records, col_map)
    >>> pd.DataFrame.from_records(position_records)
       id  col  size  entry_order_id  entry_idx  entry_price  entry_fees  \
    0   0    0   1.1               0          0     1.101818     0.01212
    1   1    0   1.0               4          4     5.050000     0.05050
    2   2    0   1.0               5          5     5.940000     0.05940
    3   0    1   1.1               0          0     5.850000     0.06435
    4   1    1   1.0               4          4     1.980000     0.01980
    5   2    1   1.0               5          5     1.010000     0.01010

       exit_order_id  exit_idx  exit_price  exit_fees      pnl    return  \
    0              3         3    3.060000    0.03366  2.10822  1.739455
    1              5         5    5.940000    0.05940  0.78010  0.154475
    2             -1         5    6.000000    0.00000 -0.11940 -0.020101
    3              3         3    3.948182    0.04343  1.98422  0.308348
    4              5         5    1.010000    0.01010  0.94010  0.474798
    5             -1         5    1.000000    0.00000 -0.02010 -0.019901

       direction  status  parent_id
    0          0       1          0
    1          0       1          1
    2          1       0          2
    3          1       1          0
    4          1       1          1
    5          0       0          2
    ```
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: get_long_view_orders_nb
```
Get view of orders in long positions only.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: get_short_view_orders_nb
```
Get view of orders in short positions only.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: get_position_feature_nb
```
Get the position's feature at each time step.

For the list of supported features see `vectorbtpro.portfolio.enums.PositionFeature`.

If `fill_exit_price` is True and a part of the position is not closed yet, will fill
the exit price as if the part was closed using the current close.

If `fill_closed_position` is True, will forward-fill missing values with the prices of the
previously closed position.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: price_status_nb
```
Return the status of the order's price related to high and low.

See `vectorbtpro.portfolio.enums.OrderPriceStatus`.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: trade_winning_streak_nb
```
Return the current winning streak of each trade.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: trade_losing_streak_nb
```
Return the current losing streak of each trade.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: win_rate_reduce_nb
```
Win rate of a PnL array.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: profit_factor_reduce_nb
```
Profit factor of a PnL array.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: expectancy_reduce_nb
```
Expectancy of a PnL array.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: sqn_reduce_nb
```
SQN of a PnL array.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: trade_best_worst_price_nb
```
Best price, worst price, and their indices during a trade.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: best_price_nb
```
Get best price by applying `trade_best_worst_price_nb` on each trade.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: worst_price_nb
```
Get worst price by applying `trade_best_worst_price_nb` on each trade.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: best_price_idx_nb
```
Get index of best price by applying `trade_best_worst_price_nb` on each trade.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: worst_price_idx_nb
```
Get worst price by applying `trade_best_worst_price_nb` on each trade.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: expanding_best_price_nb
```
Get expanding best price of each trade.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: expanding_worst_price_nb
```
Get expanding worst price of each trade.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: trade_mfe_nb
```
Compute Maximum Favorable Excursion (MFE).
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: mfe_nb
```
Apply `trade_mfe_nb` on each trade.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: trade_mae_nb
```
Compute Maximum Adverse Excursion (MAE).
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: mae_nb
```
Apply `trade_mae_nb` on each trade.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: expanding_mfe_nb
```
Get expanding MFE of each trade.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: expanding_mae_nb
```
Get expanding MAE of each trade.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: edge_ratio_nb
```
Get edge ratio of each column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: records.py
#### Função: running_edge_ratio_nb
```
Get running edge ratio of each column.
```

---

# Pasta: portfolio
## Subpasta: nb
### Arquivo: __init__.py
#### Docstring do Módulo
```
Numba-compiled functions for working with portfolio.

Provides an arsenal of Numba-compiled functions that are used for portfolio
simulation, such as generating and filling orders. These only accept NumPy arrays and
other Numba-compatible types.

!!! note
    vectorbt treats matrices as first-class citizens and expects input arrays to be
    2-dim, unless function has suffix `_1d` or is meant to be input to another function.

    All functions passed as argument must be Numba-compiled.

    Records must retain the order they were created in.

!!! warning
    Accumulation of roundoff error possible.
    See [here](https://en.wikipedia.org/wiki/Round-off_error#Accumulation_of_roundoff_error) for explanation.

    Rounding errors can cause trades and positions to not close properly:

    ```pycon
    >>> print('%.50f' % 0.1)  # has positive error
    0.10000000000000000555111512312578270211815834045410

    >>> # many buy transactions with positive error -> cannot close position
    >>> sum([0.1 for _ in range(1000000)]) - 100000
    1.3328826753422618e-06

    >>> print('%.50f' % 0.3)  # has negative error
    0.29999999999999998889776975374843459576368331909180

    >>> # many sell transactions with negative error -> cannot close position
    >>> 300000 - sum([0.3 for _ in range(1000000)])
    5.657668225467205e-06
    ```

    While vectorbt has implemented tolerance checks when comparing floats for equality,
    adding/subtracting small amounts large number of times may still introduce a noticable
    error that cannot be corrected post factum.

    To mitigate this issue, avoid repeating lots of micro-transactions of the same sign.
    For example, reduce by `np.inf` or `position_now` to close a long/short position.

    See `vectorbtpro.utils.math_` for current tolerance values.

!!! warning
    Make sure to use `parallel=True` only if your columns are independent.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Docstring do Módulo
```
Base functions and classes for portfolio optimization.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: pfopt_func_dict
```
Dict that contains optimization functions as keys.

Keys can be functions themselves, their names, or `_def` for the default value.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Função: select_pfopt_func_kwargs
```
Select keyword arguments belonging to `pypfopt_func`.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Função: resolve_pypfopt_func_kwargs
```
Resolve keyword arguments passed to any optimization function with the layout of PyPortfolioOpt.

Parses the signature of `pypfopt_func`, and for each accepted argument, looks for an argument
with the same name in `kwargs`. If not found, tries to resolve that argument using other arguments
or by calling other optimization functions.

Argument `frequency` gets resolved with (global) `freq` and `year_freq` using
`vectorbtpro.returns.accessors.ReturnsAccessor.get_ann_factor`.

Any argument in `kwargs` can be wrapped using `pfopt_func_dict` to define the argument
per function rather than globally.

!!! note
    When providing custom functions, make sure that the arguments they accept are visible
    in the signature (that is, no variable arguments) and have the same naming as in PyPortfolioOpt.

    Functions `market_implied_prior_returns` and `BlackLittermanModel.bl_weights` take `risk_aversion`,
    which is different from arguments with the same name in other functions. To set it, pass `delta`.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Função: resolve_pypfopt_func_call
```
Resolve arguments using `resolve_pypfopt_func_kwargs` and call the function with that arguments.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Função: resolve_pypfopt_expected_returns
```
Resolve the expected returns.

`expected_returns` can be an array, an attribute of `pypfopt.expected_returns`, a function,
or one of the following options:

* 'mean_historical_return': `pypfopt.expected_returns.mean_historical_return`
* 'ema_historical_return': `pypfopt.expected_returns.ema_historical_return`
* 'capm_return': `pypfopt.expected_returns.capm_return`
* 'bl_returns': `pypfopt.black_litterman.BlackLittermanModel.bl_returns`

Any function is resolved using `resolve_pypfopt_func_call`.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Função: resolve_pypfopt_cov_matrix
```
Resolve the covariance matrix.

`cov_matrix` can be an array, an attribute of `pypfopt.risk_models`, a function,
or one of the following options:

* 'sample_cov': `pypfopt.risk_models.sample_cov`
* 'semicovariance' or 'semivariance': `pypfopt.risk_models.semicovariance`
* 'exp_cov': `pypfopt.risk_models.exp_cov`
* 'ledoit_wolf' or 'ledoit_wolf_constant_variance': `pypfopt.risk_models.CovarianceShrinkage.ledoit_wolf`
    with 'constant_variance' as shrinkage factor
* 'ledoit_wolf_single_factor': `pypfopt.risk_models.CovarianceShrinkage.ledoit_wolf`
    with 'single_factor' as shrinkage factor
* 'ledoit_wolf_constant_correlation': `pypfopt.risk_models.CovarianceShrinkage.ledoit_wolf`
    with 'constant_correlation' as shrinkage factor
* 'oracle_approximating': `pypfopt.risk_models.CovarianceShrinkage.ledoit_wolf`
    with 'oracle_approximating' as shrinkage factor

Any function is resolved using `resolve_pypfopt_func_call`.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Função: resolve_pypfopt_optimizer
```
Resolve the optimizer.

`optimizer` can be an instance of `pypfopt.base_optimizer.BaseOptimizer`, an attribute of `pypfopt`,
a subclass of  `pypfopt.base_optimizer.BaseOptimizer`, or one of the following options:

* 'efficient_frontier': `pypfopt.efficient_frontier.EfficientFrontier`
* 'efficient_cdar': `pypfopt.efficient_frontier.EfficientCDaR`
* 'efficient_cvar': `pypfopt.efficient_frontier.EfficientCVaR`
* 'efficient_semivariance': `pypfopt.efficient_frontier.EfficientSemivariance`
* 'black_litterman' or 'bl': `pypfopt.black_litterman.BlackLittermanModel`
* 'hierarchical_portfolio', 'hrpopt', or 'hrp': `pypfopt.hierarchical_portfolio.HRPOpt`
* 'cla': `pypfopt.cla.CLA`

Any function is resolved using `resolve_pypfopt_func_call`.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Função: pypfopt_optimize
```
Get allocation using PyPortfolioOpt.

First, it resolves the optimizer using `resolve_pypfopt_optimizer`. Depending upon which arguments it takes,
it may further resolve expected returns, covariance matrix, etc. Then, it adds objectives and constraints
to the optimizer instance, calls the target metric, extracts the weights, and finally, converts
the weights to an integer allocation (if requested).

To specify the optimizer, use `optimizer` (see `resolve_pypfopt_optimizer`).
To specify the expected returns, use `expected_returns` (see `resolve_pypfopt_expected_returns`).
To specify the covariance matrix, use `cov_matrix` (see `resolve_pypfopt_cov_matrix`).
All other keyword arguments in `**kwargs` are used by `resolve_pypfopt_func_call`.

Each objective can be a function, an attribute of `pypfopt.objective_functions`, or an iterable of such.

Each constraint can be a function or an interable of such.

The target can be an attribute of the optimizer, or a stand-alone function.
If `target_is_convex` is True, the function is added as a convex function.
Otherwise, the function is added as a non-convex function. The keyword arguments
`weights_sum_to_one` and those starting with `target` are passed
`pypfopt.base_optimizer.BaseConvexOptimizer.convex_objective`
and `pypfopt.base_optimizer.BaseConvexOptimizer.nonconvex_objective` respectively.

Set `ignore_opt_errors` to True to ignore any target optimization errors.
Set `ignore_errors` to True to ignore any errors, even those caused by the user.

If `discrete_allocation` is True, resolves `pypfopt.discrete_allocation.DiscreteAllocation`
and calls `allocation_method` as an attribute of the allocation object.

Any function is resolved using `resolve_pypfopt_func_call`.

For defaults, see `pypfopt` under `vectorbtpro._settings.pfopt`.

Usage:
    * Using mean historical returns, Ledoit-Wolf covariance matrix with constant variance,
    and efficient frontier:

    ```pycon
    >>> from vectorbtpro import *

    >>> data = vbt.YFData.pull(["MSFT", "AMZN", "KO", "MA"])
    ```

    [=100% "100%"]{: .candystripe .candystripe-animate }

    ```pycon
    >>> vbt.pypfopt_optimize(prices=data.get("Close"))
    {'MSFT': 0.13324, 'AMZN': 0.10016, 'KO': 0.03229, 'MA': 0.73431}
    ```

    * EMA historical returns and sample covariance:

    ```pycon
    >>> vbt.pypfopt_optimize(
    ...     prices=data.get("Close"),
    ...     expected_returns="ema_historical_return",
    ...     cov_matrix="sample_cov"
    ... )
    {'MSFT': 0.08984, 'AMZN': 0.0, 'KO': 0.91016, 'MA': 0.0}
    ```

    * EMA historical returns, efficient Conditional Value at Risk, and other parameters automatically
    passed to their respective functions. Optimized towards lowest CVaR:

    ```pycon
    >>> vbt.pypfopt_optimize(
    ...     prices=data.get("Close"),
    ...     expected_returns="ema_historical_return",
    ...     optimizer="efficient_cvar",
    ...     beta=0.9,
    ...     weight_bounds=(-1, 1),
    ...     target="min_cvar"
    ... )
    {'MSFT': 0.14779, 'AMZN': 0.07224, 'KO': 0.77552, 'MA': 0.00445}
    ```

    * Adding custom objectives:

    ```pycon
    >>> vbt.pypfopt_optimize(
    ...     prices=data.get("Close"),
    ...     objectives=["L2_reg"],
    ...     gamma=0.1,
    ...     target="min_volatility"
    ... )
    {'MSFT': 0.22228, 'AMZN': 0.15685, 'KO': 0.28712, 'MA': 0.33375}
    ```

    * Adding custom constraints:

    ```pycon
    >>> vbt.pypfopt_optimize(
    ...     prices=data.get("Close"),
    ...     constraints=[lambda w: w[data.symbols.index("MSFT")] <= 0.1]
    ... )
    {'MSFT': 0.1, 'AMZN': 0.10676, 'KO': 0.04341, 'MA': 0.74982}
    ```

    * Optimizing towards a custom convex objective (to add a non-convex objective,
    set `target_is_convex` to False):

    ```pycon
    >>> import cvxpy as cp

    >>> def logarithmic_barrier_objective(w, cov_matrix, k=0.1):
    ...     log_sum = cp.sum(cp.log(w))
    ...     var = cp.quad_form(w, cov_matrix)
    ...     return var - k * log_sum

    >>> pypfopt_optimize(
    ...     prices=data.get("Close"),
    ...     target=logarithmic_barrier_objective
    ... )
    {'MSFT': 0.24595, 'AMZN': 0.23047, 'KO': 0.25862, 'MA': 0.26496}
    ```
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Função: prepare_returns
```
Prepare returns.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Função: resolve_riskfolio_func_kwargs
```
Select keyword arguments belonging to `riskfolio_func`.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Função: resolve_asset_classes
```
Resolve asset classes for Riskfolio-Lib.

Supports the following formats:

* None: Takes columns where the bottom-most level is assumed to be assets
* Index: Each level in the index must be a different asset class set
* Nested dict: Each sub-dict must be a different asset class set
* Sequence of strings or ints: Matches them against level names in the columns.
If the columns have a single level, or some level names were not found, uses the sequence
directly as one class asset set named 'Class'.
* Sequence of dicts: Each dict becomes a row in the new DataFrame
* DataFrame where the first column is the asset list and the next columns are the
different asset’s classes sets (this is the target format accepted by Riskfolio-Lib).
See an example [here](https://riskfolio-lib.readthedocs.io/en/latest/constraints.html#ConstraintsFunctions.assets_constraints).

!!! note
    If `asset_classes` is neither None nor a DataFrame, the bottom-most level in `columns`
    gets renamed to 'Assets' and becomes the first column of the new DataFrame.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Função: resolve_assets_constraints
```
Resolve asset constraints for Riskfolio-Lib.

Apart from the [target format](https://riskfolio-lib.readthedocs.io/en/latest/constraints.html#ConstraintsFunctions.assets_constraints),
also accepts a sequence of dicts such that each dict becomes a row in a new DataFrame.
Dicts don't have to specify all column names, the function will autofill any missing elements/columns.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Função: resolve_factors_constraints
```
Resolve factors constraints for Riskfolio-Lib.

Apart from the [target format](https://riskfolio-lib.readthedocs.io/en/latest/constraints.html#ConstraintsFunctions.factors_constraints),
also accepts a sequence of dicts such that each dict becomes a row in a new DataFrame.
Dicts don't have to specify all column names, the function will autofill any missing elements/columns.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Função: resolve_assets_views
```
Resolve asset views for Riskfolio-Lib.

Apart from the [target format](https://riskfolio-lib.readthedocs.io/en/latest/constraints.html#ConstraintsFunctions.assets_views),
also accepts a sequence of dicts such that each dict becomes a row in a new DataFrame.
Dicts don't have to specify all column names, the function will autofill any missing elements/columns.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Função: resolve_factors_views
```
Resolve factors views for Riskfolio-Lib.

Apart from the [target format](https://riskfolio-lib.readthedocs.io/en/latest/constraints.html#ConstraintsFunctions.factors_views),
also accepts a sequence of dicts such that each dict becomes a row in a new DataFrame.
Dicts don't have to specify all column names, the function will autofill any missing elements/columns.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Função: resolve_hrp_constraints
```
Resolve HRP constraints for Riskfolio-Lib.

Apart from the [target format](https://riskfolio-lib.readthedocs.io/en/latest/constraints.html#ConstraintsFunctions.hrp_constraints),
also accepts a sequence of dicts such that each dict becomes a row in a new DataFrame.
Dicts don't have to specify all column names, the function will autofill any missing elements/columns.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Função: riskfolio_optimize
```
Get allocation using Riskfolio-Lib.

Args:
    returns (array_like): A dataframe that contains the returns of the assets.
    nan_to_zero (bool): Whether to convert NaN values to zero.
    dropna_rows (bool): Whether to drop rows with all NaN/zero values.

        Gets applied only if `nan_to_zero` is True or `dropna_any` is False.
    dropna_cols (bool): Whether to drop columns with all NaN/zero values.
    dropna_any (bool): Whether to drop any NaN values.

        Gets applied only if `nan_to_zero` is False.
    factors (array_like): A dataframe that contains the factors.
    port (Portfolio or HCPortfolio): Already initialized portfolio.
    port_cls (str or type): Portfolio class.

        Supports the following values:

        * None: Uses `Portfolio`
        * 'hc' or 'hcportfolio' (case-insensitive): Uses `HCPortfolio`
        * Other string: Uses attribute of `riskfolio`
        * Class: Uses a custom class
    opt_method (str or callable): Optimization method.

        Supports the following values:

        * None or 'optimization': Uses `port.optimization` (where `port` is a portfolio instance)
        * 'wc' or 'wc_optimization': Uses `port.wc_optimization`
        * 'rp' or 'rp_optimization': Uses `port.rp_optimization`
        * 'rrp' or 'rrp_optimization': Uses `port.rrp_optimization`
        * 'owa' or 'owa_optimization': Uses `port.owa_optimization`
        * String: Uses attribute of `port`
        * Callable: Uses a custom optimization function
    stats_methods (str or sequence of str): Sequence of stats methods to call before optimization.

        If None, tries to automatically populate the sequence using `opt_method` and `model`.
        For example, calls `port.assets_stats` if `model="Classic"` is used.
        Also, if `func_kwargs` is not empty, adds all functions whose name ends with '_stats'.
    model (str): The model used to optimize the portfolio.
    asset_classes (any): Asset classes matrix.

        See `resolve_asset_classes` for possible formats.
    constraints_method (str): Constraints method.

        Supports the following values:

        * 'assets' or 'assets_constraints': [assets constraints](https://riskfolio-lib.readthedocs.io/en/latest/constraints.html#ConstraintsFunctions.assets_constraints)
        * 'factors' or 'factors_constraints': [factors constraints](https://riskfolio-lib.readthedocs.io/en/latest/constraints.html#ConstraintsFunctions.factors_constraints)
        * 'hrp' or 'hrp_constraints': [HRP constraints](https://riskfolio-lib.readthedocs.io/en/latest/constraints.html#ConstraintsFunctions.hrp_constraints)

        If None and the class `Portfolio` is used, will use factors constraints if `factors_stats` is used,
        otherwise assets constraints. If the class `HCPortfolio` is used, will use HRP constraints.
    constraints (any): Constraints matrix.

        See `resolve_assets_constraints` for possible formats of assets constraints,
        `resolve_factors_constraints` for possible formats of factors constraints, and
        `resolve_hrp_constraints` for possible formats of HRP constraints.
    views_method (str): Views method.

        Supports the following values:

        * 'assets' or 'assets_views': [assets views](https://riskfolio-lib.readthedocs.io/en/latest/constraints.html#ConstraintsFunctions.assets_views)
        * 'factors' or 'factors_views': [factors views](https://riskfolio-lib.readthedocs.io/en/latest/constraints.html#ConstraintsFunctions.factors_views)

        If None, will use factors views if `blfactors_stats` is used, otherwise assets views.
    views (any): Views matrix.

        See `resolve_assets_views` for possible formats of assets views and
        `resolve_factors_views` for possible formats of factors views.
    solvers (list of str): Solvers.
    sol_params (dict): Solver parameters.
    freq (frequency_like): Frequency to be used to compute the annualization factor.

        Make sure to provide it when using views.
    year_freq (frequency_like): Year frequency to be used to compute the annualization factor.

        Make sure to provide it when using views.
    pre_opt (bool): Whether to pre-optimize the portfolio with `pre_opt_kwargs`.
    pre_opt_kwargs (dict): Call `riskfolio_optimize` with these keyword arguments
        and use the returned portfolio for further optimization.
    pre_opt_as_w (bool): Whether to use the weights as `w` from the pre-optimization step.
    func_kwargs (dict): Further keyword arguments by function.

        Can be used to override any arguments from `kwargs` matched with the function,
        or to add more arguments. Will be wrapped with `pfopt_func_dict` and passed to
        `select_pfopt_func_kwargs` when calling each Riskfolio-Lib's function.
    silence_warnings (bool): Whether to silence all warnings.
    return_port (bool): Whether to also return the portfolio.
    ignore_errors (bool): Whether to ignore any errors, even those caused by the user.
    **kwargs: Keyword arguments that will be passed to any Riskfolio-Lib's function
        that needs them (i.e., lists any of them in its signature).

For defaults, see `riskfolio` under `vectorbtpro._settings.pfopt`.

Usage:
    * Classic Mean Risk Optimization:

    ```pycon
    >>> from vectorbtpro import *

    >>> data = vbt.YFData.pull(["MSFT", "AMZN", "KO", "MA"])
    >>> returns = data.close.vbt.to_returns()
    ```

    [=100% "100%"]{: .candystripe .candystripe-animate }

    ```pycon
    >>> vbt.riskfolio_optimize(
    ...     returns,
    ...     method_mu='hist', method_cov='hist', d=0.94,  # assets_stats
    ...     model='Classic', rm='MV', obj='Sharpe', hist=True, rf=0, l=0  # optimization
    ... )
    {'MSFT': 0.26297126323056036,
     'AMZN': 0.13984467450137006,
     'KO': 0.35870315943426767,
     'MA': 0.238480902833802}
    ```

    * The same by splitting arguments:

    ```pycon
    >>> vbt.riskfolio_optimize(
    ...     returns,
    ...     func_kwargs=dict(
    ...         assets_stats=dict(method_mu='hist', method_cov='hist', d=0.94),
    ...         optimization=dict(model='Classic', rm='MV', obj='Sharpe', hist=True, rf=0, l=0)
    ...     )
    ... )
    {'MSFT': 0.26297126323056036,
     'AMZN': 0.13984467450137006,
     'KO': 0.35870315943426767,
     'MA': 0.238480902833802}
    ```

    * Asset constraints:

    ```pycon
    >>> vbt.riskfolio_optimize(
    ...     returns,
    ...     constraints=[
    ...         {
    ...             "Type": "Assets",
    ...             "Position": "MSFT",
    ...             "Sign": "<=",
    ...             "Weight": 0.01
    ...         }
    ...     ]
    ... )
    {'MSFT': 0.009999990814976588,
     'AMZN': 0.19788481506569947,
     'KO': 0.4553600308839969,
     'MA': 0.336755163235327}
    ```

    * Asset class constraints:

    ```pycon
    >>> vbt.riskfolio_optimize(
    ...     returns,
    ...     asset_classes=["C1", "C1", "C2", "C2"],
    ...     constraints=[
    ...         {
    ...             "Type": "Classes",
    ...             "Set": "Class",
    ...             "Position": "C1",
    ...             "Sign": "<=",
    ...             "Weight": 0.1
    ...         }
    ...     ]
    ... )
    {'MSFT': 0.03501297245802569,
     'AMZN': 0.06498702655063979,
     'KO': 0.4756624658301967,
     'MA': 0.4243375351611379}
    ```

    * Hierarchical Risk Parity (HRP) Portfolio Optimization:

    ```pycon
    >>> vbt.riskfolio_optimize(
    ...     returns,
    ...     port_cls="HCPortfolio",
    ...     model='HRP',
    ...     codependence='pearson',
    ...     rm='MV',
    ...     rf=0,
    ...     linkage='single',
    ...     max_k=10,
    ...     leaf_order=True
    ... )
    {'MSFT': 0.19091632057853536,
     'AMZN': 0.11069893826556164,
     'KO': 0.28589872132122485,
     'MA': 0.41248601983467814}
    ```
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
```
Class that exposes methods for generating allocations.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: row_stack
```
Stack multiple `PortfolioOptimizer` instances along rows.

Uses `vectorbtpro.base.wrapping.ArrayWrapper.row_stack` to stack the wrappers.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: column_stack
```
Stack multiple `PortfolioOptimizer` instances along columns.

Uses `vectorbtpro.base.wrapping.ArrayWrapper.column_stack` to stack the wrappers.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: indexing_func
```
Perform indexing on `PortfolioOptimizer`.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: resample
```
Perform resampling on `PortfolioOptimizer`.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: run_allocation_group
```
Run an allocation group.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: from_allocate_func
```
Generate allocations from an allocation function.

Generates date points and allocates at those points.

Similar to `PortfolioOptimizer.from_optimize_func`, but generates points using
`vectorbtpro.base.wrapping.ArrayWrapper.get_index_points` and makes each point available
as `index_point` in the context.

If `jitted_loop` is True, see `vectorbtpro.portfolio.pfopt.nb.allocate_meta_nb`.

Also, in contrast to `PortfolioOptimizer.from_optimize_func`, creates records of type
`vectorbtpro.portfolio.pfopt.records.AllocPoints`.

Usage:
    * Allocate uniformly every day:

    ```pycon
    >>> from vectorbtpro import *

    >>> data = vbt.YFData.pull(
    ...     ["MSFT", "AMZN", "AAPL"],
    ...     start="2010-01-01",
    ...     end="2020-01-01"
    ... )
    >>> close = data.get("Close")

    >>> def uniform_allocate_func(n_cols):
    ...     return np.full(n_cols, 1 / n_cols)

    >>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
    ...     close.vbt.wrapper,
    ...     uniform_allocate_func,
    ...     close.shape[1]
    ... )
    >>> pfo.allocations
    symbol                         MSFT      AMZN      AAPL
    Date
    2010-01-04 00:00:00-05:00  0.333333  0.333333  0.333333
    2010-01-05 00:00:00-05:00  0.333333  0.333333  0.333333
    2010-01-06 00:00:00-05:00  0.333333  0.333333  0.333333
    2010-01-07 00:00:00-05:00  0.333333  0.333333  0.333333
    2010-01-08 00:00:00-05:00  0.333333  0.333333  0.333333
    ...                             ...       ...       ...
    2019-12-24 00:00:00-05:00  0.333333  0.333333  0.333333
    2019-12-26 00:00:00-05:00  0.333333  0.333333  0.333333
    2019-12-27 00:00:00-05:00  0.333333  0.333333  0.333333
    2019-12-30 00:00:00-05:00  0.333333  0.333333  0.333333
    2019-12-31 00:00:00-05:00  0.333333  0.333333  0.333333

    [2516 rows x 3 columns]
    ```

    * Allocate randomly every first date of the year:

    ```pycon
    >>> def random_allocate_func(n_cols):
    ...     weights = np.random.uniform(size=n_cols)
    ...     return weights / weights.sum()

    >>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
    ...     close.vbt.wrapper,
    ...     random_allocate_func,
    ...     close.shape[1],
    ...     every="AS-JAN"
    ... )
    >>> pfo.allocations
    symbol                         MSFT      AMZN      AAPL
    Date
    2011-01-03 00:00:00+00:00  0.160335  0.122434  0.717231
    2012-01-03 00:00:00+00:00  0.071386  0.469564  0.459051
    2013-01-02 00:00:00+00:00  0.125853  0.168480  0.705668
    2014-01-02 00:00:00+00:00  0.391565  0.169205  0.439231
    2015-01-02 00:00:00+00:00  0.115075  0.602844  0.282081
    2016-01-04 00:00:00+00:00  0.244070  0.046547  0.709383
    2017-01-03 00:00:00+00:00  0.316065  0.335000  0.348935
    2018-01-02 00:00:00+00:00  0.422142  0.252154  0.325704
    2019-01-02 00:00:00+00:00  0.368748  0.195147  0.436106
    ```

    * Specify index points manually:

    ```pycon
    >>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
    ...     close.vbt.wrapper,
    ...     random_allocate_func,
    ...     close.shape[1],
    ...     index_points=[0, 30, 60]
    ... )
    >>> pfo.allocations
    symbol                         MSFT      AMZN      AAPL
    Date
    2010-01-04 00:00:00+00:00  0.257878  0.308287  0.433835
    2010-02-17 00:00:00+00:00  0.090927  0.471980  0.437094
    2010-03-31 00:00:00+00:00  0.395855  0.148516  0.455629
    ```

    * Specify allocations manually:

    ```pycon
    >>> def manual_allocate_func(weights):
    ...     return weights

    >>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
    ...     close.vbt.wrapper,
    ...     manual_allocate_func,
    ...     vbt.RepEval("weights[i]", context=dict(weights=[
    ...         [1, 0, 0],
    ...         [0, 1, 0],
    ...         [0, 0, 1]
    ...     ])),
    ...     index_points=[0, 30, 60]
    ... )
    >>> pfo.allocations
    symbol                     MSFT  AMZN  AAPL
    Date
    2010-01-04 00:00:00+00:00     1     0     0
    2010-02-17 00:00:00+00:00     0     1     0
    2010-03-31 00:00:00+00:00     0     0     1
    ```

    * Use Numba-compiled loop:

    ```pycon
    >>> @njit
    ... def random_allocate_func_nb(i, idx, n_cols):
    ...     weights = np.random.uniform(0, 1, n_cols)
    ...     return weights / weights.sum()

    >>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
    ...     close.vbt.wrapper,
    ...     random_allocate_func_nb,
    ...     close.shape[1],
    ...     index_points=[0, 30, 60],
    ...     jitted_loop=True
    ... )
    >>> pfo.allocations
    symbol                         MSFT      AMZN      AAPL
    Date
    2010-01-04 00:00:00+00:00  0.231925  0.351085  0.416990
    2010-02-17 00:00:00+00:00  0.163050  0.070292  0.766658
    2010-03-31 00:00:00+00:00  0.497465  0.500215  0.002319
    ```

    !!! hint
        There is no big reason of using the Numba-compiled loop, apart from when having
        to rebalance many thousands of times. Usually, using a regular Python loop
        and a Numba-compiled allocation function should suffice.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: from_allocations
```
Pick allocations from a (flexible) array.

Uses `PortfolioOptimizer.from_allocate_func`.

If `allocations` is a DataFrame, uses its index as labels. If it's a Series or dict, uses it
as a single allocation without index, which by default gets assigned to each index. If it's neither
one of the above nor a NumPy array, tries to convert it into a NumPy array.

If `allocations` is a NumPy array, uses `vectorbtpro.portfolio.pfopt.nb.pick_idx_allocate_func_nb`
and a Numba-compiled loop. Otherwise, uses a regular Python function to pick each allocation
(which can be a dict, Series, etc.). Selection of elements is done in a flexible manner,
meaning a single element will be applied to all rows, while one-dimensional arrays will be
also applied to all rows but also broadcast across columns (as opposed to rows).
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: from_initial
```
Allocate once at the first index.

Uses `PortfolioOptimizer.from_allocations` with `on=0`.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: from_filled_allocations
```
Pick allocations from an already filled array.

Uses `PortfolioOptimizer.from_allocate_func`.

Uses `vectorbtpro.portfolio.pfopt.nb.pick_point_allocate_func_nb` and a Numba-compiled loop.

Extracts allocation points using `vectorbtpro.portfolio.pfopt.nb.get_alloc_points_nb`.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: from_uniform
```
Generate uniform allocations.

Uses `PortfolioOptimizer.from_allocate_func`.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: from_random
```
Generate random allocations.

Uses `PortfolioOptimizer.from_allocate_func`.

Uses `vectorbtpro.portfolio.pfopt.nb.random_allocate_func_nb` and a Numba-compiled loop.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: from_universal_algo
```
Generate allocations using [Universal Portfolios](https://github.com/Marigold/universal-portfolios).

`S` can be any price, while `algo` must be either an attribute of the package, subclass of
`universal.algo.Algo`, instance of `universal.algo.Algo`, or instance of `universal.result.AlgoResult`.

Extracts allocation points using `vectorbtpro.portfolio.pfopt.nb.get_alloc_points_nb`.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: run_optimization_group
```
Run an optimization group.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: from_optimize_func
```
Generate allocations from an optimization function.

Generates date ranges, performs optimization on the subset of data that belongs to each date range,
and allocates at the end of each range.

This is a parameterized method that allows testing multiple combinations on most arguments.
First, it checks whether any of the arguments is wrapped with `vectorbtpro.utils.params.Param`
and combines their values. It then combines them over `group_configs`, if provided.
Before execution, it additionally processes the group config using `pre_group_func`.

It then resolves the date ranges, either using the ready-to-use `index_ranges` or
by passing all the arguments ranging from `every` to `jitted` to
`vectorbtpro.base.wrapping.ArrayWrapper.get_index_ranges`. The optimization
function `optimize_func` is then called on each date range by first substituting
any templates found in `*args` and `**kwargs`. To forward any reserved arguments
such as `jitted` to the optimization function, specify their names in `forward_args`
and `forward_kwargs`.

!!! note
    Make sure to use vectorbt's own templates to select the current date range
    (available as `index_slice` in the context mapping) from each array.

If `jitted_loop` is True, see `vectorbtpro.portfolio.pfopt.nb.optimize_meta_nb`.
Otherwise, must take template-substituted `*args` and `**kwargs`, and return an array or
dictionary with asset allocations (also empty).

!!! note
    When `jitted_loop` is True and in case of multiple groups, use templates
    to substitute by the current group index (available as `group_idx` in the context mapping).

All allocations of all groups are stacked into one big 2-dim array where columns are assets
and rows are allocations. Furthermore, date ranges are used to fill a record array of type
`vectorbtpro.portfolio.pfopt.records.AllocRanges` that acts as an indexer for allocations.
For example, the field `col` stores the group index corresponding to each allocation. Since
this record array does not hold any information on assets themselves, it has its own wrapper
that holds groups instead of columns, while the wrapper of the `PortfolioOptimizer` instance
contains regular columns grouped by groups.

Usage:
    * Allocate once:

    ```pycon
    >>> from vectorbtpro import *

    >>> data = vbt.YFData.pull(
    ...     ["MSFT", "AMZN", "AAPL"],
    ...     start="2010-01-01",
    ...     end="2020-01-01"
    ... )
    >>> close = data.get("Close")

    >>> def optimize_func(df):
    ...     sharpe = df.mean() / df.std()
    ...     return sharpe / sharpe.sum()

    >>> df_arg = vbt.RepEval("close.iloc[index_slice]", context=dict(close=close))
    >>> pfo = vbt.PortfolioOptimizer.from_optimize_func(
    ...     close.vbt.wrapper,
    ...     optimize_func,
    ...     df_arg,
    ...     end="2015-01-01"
    ... )
    >>> pfo.allocations
    symbol                                     MSFT      AMZN      AAPL
    alloc_group Date
    group       2015-01-02 00:00:00+00:00  0.402459  0.309351  0.288191
    ```

    * Allocate every first date of the year:

    ```pycon
    >>> pfo = vbt.PortfolioOptimizer.from_optimize_func(
    ...     close.vbt.wrapper,
    ...     optimize_func,
    ...     df_arg,
    ...     every="AS-JAN"
    ... )
    >>> pfo.allocations
    symbol                                     MSFT      AMZN      AAPL
    alloc_group Date
    group       2011-01-03 00:00:00+00:00  0.480693  0.257317  0.261990
                2012-01-03 00:00:00+00:00  0.489893  0.215381  0.294727
                2013-01-02 00:00:00+00:00  0.540165  0.228755  0.231080
                2014-01-02 00:00:00+00:00  0.339649  0.273996  0.386354
                2015-01-02 00:00:00+00:00  0.350406  0.418638  0.230956
                2016-01-04 00:00:00+00:00  0.332212  0.141090  0.526698
                2017-01-03 00:00:00+00:00  0.390852  0.225379  0.383769
                2018-01-02 00:00:00+00:00  0.337711  0.317683  0.344606
                2019-01-02 00:00:00+00:00  0.411852  0.282680  0.305468
    ```

    * Specify index ranges manually:

    ```pycon
    >>> pfo = vbt.PortfolioOptimizer.from_optimize_func(
    ...     close.vbt.wrapper,
    ...     optimize_func,
    ...     df_arg,
    ...     index_ranges=[
    ...         (0, 30),
    ...         (30, 60),
    ...         (60, 90)
    ...     ]
    ... )
    >>> pfo.allocations
    symbol                                     MSFT      AMZN      AAPL
    alloc_group Date
    group       2010-02-16 00:00:00+00:00  0.340641  0.285897  0.373462
                2010-03-30 00:00:00+00:00  0.596392  0.206317  0.197291
                2010-05-12 00:00:00+00:00  0.437481  0.283160  0.279358
    ```

    * Test multiple combinations of one argument:

    ```pycon
    >>> pfo = vbt.PortfolioOptimizer.from_optimize_func(
    ...     close.vbt.wrapper,
    ...     optimize_func,
    ...     df_arg,
    ...     every="AS-JAN",
    ...     start="2015-01-01",
    ...     lookback_period=vbt.Param(["3MS", "6MS"])
    ... )
    >>> pfo.allocations
    symbol                                         MSFT      AMZN      AAPL
    lookback_period Date
    3MS             2016-01-04 00:00:00+00:00  0.282725  0.234970  0.482305
                    2017-01-03 00:00:00+00:00  0.318100  0.269355  0.412545
                    2018-01-02 00:00:00+00:00  0.387499  0.236432  0.376068
                    2019-01-02 00:00:00+00:00  0.575464  0.254808  0.169728
    6MS             2016-01-04 00:00:00+00:00  0.265035  0.198619  0.536346
                    2017-01-03 00:00:00+00:00  0.314144  0.409020  0.276836
                    2018-01-02 00:00:00+00:00  0.322741  0.282639  0.394621
                    2019-01-02 00:00:00+00:00  0.565691  0.234760  0.199549
    ```

    * Test multiple cross-argument combinations:

    ```pycon
    >>> pfo = vbt.PortfolioOptimizer.from_optimize_func(
    ...     close.vbt.wrapper,
    ...     optimize_func,
    ...     df_arg,
    ...     every="AS-JAN",
    ...     group_configs=[
    ...         dict(start="2015-01-01"),
    ...         dict(start="2019-06-01", every="MS"),
    ...         dict(end="2014-01-01")
    ...     ]
    ... )
    >>> pfo.allocations
    symbol                                      MSFT      AMZN      AAPL
    group_config Date
    0            2016-01-04 00:00:00+00:00  0.332212  0.141090  0.526698
                 2017-01-03 00:00:00+00:00  0.390852  0.225379  0.383769
                 2018-01-02 00:00:00+00:00  0.337711  0.317683  0.344606
                 2019-01-02 00:00:00+00:00  0.411852  0.282680  0.305468
    1            2019-07-01 00:00:00+00:00  0.351461  0.327334  0.321205
                 2019-08-01 00:00:00+00:00  0.418411  0.249799  0.331790
                 2019-09-03 00:00:00+00:00  0.400439  0.374044  0.225517
                 2019-10-01 00:00:00+00:00  0.509387  0.250497  0.240117
                 2019-11-01 00:00:00+00:00  0.349983  0.469181  0.180835
                 2019-12-02 00:00:00+00:00  0.260437  0.380563  0.359000
    2            2012-01-03 00:00:00+00:00  0.489892  0.215381  0.294727
                 2013-01-02 00:00:00+00:00  0.540165  0.228755  0.231080
                 2014-01-02 00:00:00+00:00  0.339649  0.273997  0.386354
    ```

    * Use Numba-compiled loop:

    ```pycon
    >>> @njit
    ... def optimize_func_nb(i, from_idx, to_idx, close):
    ...     mean = vbt.nb.nanmean_nb(close[from_idx:to_idx])
    ...     std = vbt.nb.nanstd_nb(close[from_idx:to_idx])
    ...     sharpe = mean / std
    ...     return sharpe / np.sum(sharpe)

    >>> pfo = vbt.PortfolioOptimizer.from_optimize_func(
    ...     close.vbt.wrapper,
    ...     optimize_func_nb,
    ...     np.asarray(close),
    ...     index_ranges=[
    ...         (0, 30),
    ...         (30, 60),
    ...         (60, 90)
    ...     ],
    ...     jitted_loop=True
    ... )
    >>> pfo.allocations
    symbol                         MSFT      AMZN      AAPL
    Date
    2010-02-17 00:00:00+00:00  0.336384  0.289598  0.374017
    2010-03-31 00:00:00+00:00  0.599417  0.207158  0.193425
    2010-05-13 00:00:00+00:00  0.434084  0.281246  0.284670
    ```

    !!! hint
        There is no big reason of using the Numba-compiled loop, apart from when having
        to rebalance many thousands of times. Usually, using a regular Python loop
        and a Numba-compiled optimization function suffice.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: from_pypfopt
```
`PortfolioOptimizer.from_optimize_func` applied on `pypfopt_optimize`.

If a wrapper is not provided, parses the wrapper from `prices` or `returns`, if provided.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: from_riskfolio
```
`PortfolioOptimizer.from_optimize_func` applied on Riskfolio-Lib.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: alloc_records
```
Allocation ranges of type `vectorbtpro.portfolio.pfopt.records.AllocRanges`
or points of type `vectorbtpro.portfolio.pfopt.records.AllocPoints`.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: get_allocations
```
Get a DataFrame with allocation groups concatenated along the index axis.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: allocations
```
Calls `PortfolioOptimizer.get_allocations` with default arguments.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: mean_allocation
```
Get the mean allocation per column.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: fill_allocations
```
Fill an empty DataFrame with allocations.

Set `dropna` to 'all' to remove all NaN rows, or to 'head' to remove any rows coming before
the first allocation.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: filled_allocations
```
Calls `PortfolioOptimizer.fill_allocations` with default arguments.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: simulate
```
Run `vectorbtpro.portfolio.base.Portfolio.from_optimizer` on this instance.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: stats_defaults
```
Defaults for `PortfolioOptimizer.stats`.

Merges `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats_defaults` and
`stats` from `vectorbtpro._settings.pfopt`.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: plot
```
Plot allocations.

Args:
    column (str): Name of the allocation group to plot.
    dropna (int): See `PortfolioOptimizer.fill_allocations`.
    line_shape (str): Line shape.
    plot_rb_dates (bool): Whether to plot rebalancing dates.

        Defaults to True if there are no more than 20 rebalancing dates.
    trace_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Scatter`.
    add_shape_kwargs (dict): Keyword arguments passed to `fig.add_shape` for rebalancing dates.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    * Continuing with the examples under `PortfolioOptimizer.from_optimize_func`:

    ```pycon
    >>> from vectorbtpro import *

    >>> pfo = vbt.PortfolioOptimizer.from_random(
    ...     vbt.ArrayWrapper(
    ...         index=pd.date_range("2020-01-01", "2021-01-01"),
    ...         columns=["MSFT", "AMZN", "AAPL"],
    ...         ndim=2
    ...     ),
    ...     every="MS",
    ...     seed=40
    ... )
    >>> pfo.plot().show()
    ```

    ![](/assets/images/api/pfopt_plot.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/pfopt_plot.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: base.py
#### Classe: PortfolioOptimizer
#### Função: plots_defaults
```
Defaults for `PortfolioOptimizer.plots`.

Merges `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots_defaults` and
`plots` from `vectorbtpro._settings.pfopt`.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: nb.py
#### Docstring do Módulo
```
Numba-compiled functions for portfolio optimization.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: nb.py
#### Função: get_alloc_points_nb
```
Get allocation points from filled allocations.

If `valid_only` is True, does not register a new allocation when all points are NaN.v
If `nonzero_only` is True, does not register a new allocation when all points are zero.
If `unique_only` is True, does not register a new allocation when it's the same as the last one.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: nb.py
#### Função: optimize_meta_nb
```
Optimize by reducing each index range.

`reduce_func_nb` must take the range index, the range start, the range end, and `*args`.
Must return a 1-dim array with the same size as `n_cols`.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: nb.py
#### Função: allocate_meta_nb
```
Allocate by mapping each index point.

`map_func_nb` must take the point index, the index point, and `*args`.
Must return a 1-dim array with the same size as `n_cols`.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: nb.py
#### Função: pick_idx_allocate_func_nb
```
Pick the allocation at an absolute position in an array.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: nb.py
#### Função: pick_point_allocate_func_nb
```
Pick the allocation at an index point in an array.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: nb.py
#### Função: random_allocate_func_nb
```
Generate a random allocation.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: nb.py
#### Função: prepare_alloc_points_nb
```
Prepare allocation points.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: nb.py
#### Função: prepare_alloc_ranges_nb
```
Prepare allocation ranges.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: nb.py
#### Função: rescale_allocations_nb
```
Rescale allocations to a new scale.

Positive and negative weights are rescaled separately from each other.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: records.py
#### Docstring do Módulo
```
Classes for working with allocation records.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: records.py
#### Classe: AllocRanges
```
Extends `vectorbtpro.records.base.Records` for working with allocation point records.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: records.py
#### Classe: AllocPoints
```
Extends `vectorbtpro.generic.ranges.Ranges` for working with allocation range records.
```

---

# Pasta: portfolio
## Subpasta: pfopt
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules with classes and utilities for portfolio optimization.
```

---

# Pasta: px
### Arquivo: accessors.py
#### Docstring do Módulo
```
Pandas accessors for Plotly Express.

!!! note
    Accessors do not utilize caching.
```

---

# Pasta: px
### Arquivo: accessors.py
#### Função: attach_px_methods
```
Class decorator to attach Plotly Express methods.
```

---

# Pasta: px
### Arquivo: accessors.py
#### Classe: PXAccessor
```
Accessor for running Plotly Express functions.

Accessible via `pd.Series.vbt.px` and `pd.DataFrame.vbt.px`.

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> pd.Series([1, 2, 3]).vbt.px.bar().show()
    ```

    ![](/assets/images/api/px_bar.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/px_bar.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: px
### Arquivo: accessors.py
#### Classe: PXSRAccessor
```
Accessor for running Plotly Express functions. For Series only.

Accessible via `pd.Series.vbt.px`.
```

---

# Pasta: px
### Arquivo: accessors.py
#### Classe: PXDFAccessor
```
Accessor for running Plotly Express functions. For DataFrames only.

Accessible via `pd.DataFrame.vbt.px`.
```

---

# Pasta: px
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules for plotting with Plotly Express.
```

---

# Pasta: records
### Arquivo: base.py
#### Docstring do Módulo
```
Base class for working with records.

vectorbt works with two different representations of data: matrices and records.

A matrix, in this context, is just an array of one-dimensional arrays, each corresponding
to a separate feature. The matrix itself holds only one kind of information (one attribute).
For example, one can create a matrix for entry signals, with columns being different strategy
configurations. But what if the matrix is huge and sparse? What if there is more
information we would like to represent by each element? Creating multiple matrices would be
a waste of memory.

Records make possible representing complex, sparse information in a dense format. They are just
an array of one-dimensional arrays of a fixed schema, where each element holds a different
kind of information. You can imagine records being a DataFrame, where each row represents a record
and each column represents a specific attribute. Read more on structured arrays
[here](https://numpy.org/doc/stable/user/basics.rec.html).

For example, let's represent two DataFrames as a single record array:

```plaintext
               a     b
         0   1.0   5.0
attr1 =  1   2.0   NaN
         2   NaN   7.0
         3   4.0   8.0
               a     b
         0   9.0  13.0
attr2 =  1  10.0   NaN
         2   NaN  15.0
         3  12.0  16.0
            |
            v
      id  col  idx  attr1  attr2
0      0    0    0      1      9
1      1    0    1      2     10
2      2    0    3      4     12
3      0    1    0      5     13
4      1    1    2      7     15
5      2    1    3      8     16
```

Another advantage of records is that they are not constrained by size. Multiple records can map
to a single element in a matrix. For example, one can define multiple orders at the same timestamp,
which is impossible to represent in a matrix form without duplicating index entries or using complex data types.

Consider the following example:

```pycon
>>> from vectorbtpro import *

>>> example_dt = np.dtype([
...     ('id', np.int_),
...     ('col', np.int_),
...     ('idx', np.int_),
...     ('some_field', np.float_)
... ])
>>> records_arr = np.array([
...     (0, 0, 0, 10.),
...     (1, 0, 1, 11.),
...     (2, 0, 2, 12.),
...     (0, 1, 0, 13.),
...     (1, 1, 1, 14.),
...     (2, 1, 2, 15.),
...     (0, 2, 0, 16.),
...     (1, 2, 1, 17.),
...     (2, 2, 2, 18.)
... ], dtype=example_dt)
>>> wrapper = vbt.ArrayWrapper(index=['x', 'y', 'z'],
...     columns=['a', 'b', 'c'], ndim=2, freq='1 day')
>>> records = vbt.Records(wrapper, records_arr)
```

## Printing

There are two ways to print records:

* Raw dataframe that preserves field names and data types:

```pycon
>>> records.records
   id  col  idx  some_field
0   0    0    0        10.0
1   1    0    1        11.0
2   2    0    2        12.0
3   0    1    0        13.0
4   1    1    1        14.0
5   2    1    2        15.0
6   0    2    0        16.0
7   1    2    1        17.0
8   2    2    2        18.0
```

* Readable dataframe that takes into consideration `Records.field_config`:

```pycon
>>> records.readable
   Id Column Timestamp  some_field
0   0      a         x        10.0
1   1      a         y        11.0
2   2      a         z        12.0
3   0      b         x        13.0
4   1      b         y        14.0
5   2      b         z        15.0
6   0      c         x        16.0
7   1      c         y        17.0
8   2      c         z        18.0
```

## Mapping

`Records` are just [structured arrays](https://numpy.org/doc/stable/user/basics.rec.html) with a bunch
of methods and properties for processing them. Their main feature is to map the records array and
to reduce it by column (similar to the MapReduce paradigm). The main advantage is that it all happens
without conversion to the matrix form and wasting memory resources.

`Records` can be mapped to `vectorbtpro.records.mapped_array.MappedArray` in several ways:

* Use `Records.map_field` to map a record field:

```pycon
>>> records.map_field('some_field')
<vectorbtpro.records.mapped_array.MappedArray at 0x7ff49bd31a58>

>>> records.map_field('some_field').values
array([10., 11., 12., 13., 14., 15., 16., 17., 18.])
```

* Use `Records.map` to map records using a custom function.

```pycon
>>> @njit
... def power_map_nb(record, pow):
...     return record.some_field ** pow

>>> records.map(power_map_nb, 2)
<vectorbtpro.records.mapped_array.MappedArray at 0x7ff49c990cf8>

>>> records.map(power_map_nb, 2).values
array([100., 121., 144., 169., 196., 225., 256., 289., 324.])

>>> # Map using a meta function

>>> @njit
... def power_map_meta_nb(ridx, records, pow):
...     return records[ridx].some_field ** pow

>>> vbt.Records.map(power_map_meta_nb, records.values, 2, col_mapper=records.col_mapper).values
array([100., 121., 144., 169., 196., 225., 256., 289., 324.])
```

* Use `Records.map_array` to convert an array to `vectorbtpro.records.mapped_array.MappedArray`.

```pycon
>>> records.map_array(records_arr['some_field'] ** 2)
<vectorbtpro.records.mapped_array.MappedArray object at 0x7fe9bccf2978>

>>> records.map_array(records_arr['some_field'] ** 2).values
array([100., 121., 144., 169., 196., 225., 256., 289., 324.])
```

* Use `Records.apply` to apply a function on each column/group:

```pycon
>>> @njit
... def cumsum_apply_nb(records):
...     return np.cumsum(records.some_field)

>>> records.apply(cumsum_apply_nb)
<vectorbtpro.records.mapped_array.MappedArray at 0x7ff49c990cf8>

>>> records.apply(cumsum_apply_nb).values
array([10., 21., 33., 13., 27., 42., 16., 33., 51.])

>>> group_by = np.array(['first', 'first', 'second'])
>>> records.apply(cumsum_apply_nb, group_by=group_by, apply_per_group=True).values
array([10., 21., 33., 46., 60., 75., 16., 33., 51.])

>>> # Apply using a meta function

>>> @njit
... def cumsum_apply_meta_nb(idxs, col, records):
...     return np.cumsum(records[idxs].some_field)

>>> vbt.Records.apply(cumsum_apply_meta_nb, records.values, col_mapper=records.col_mapper).values
array([10., 21., 33., 13., 27., 42., 16., 33., 51.])
```

Notice how cumsum resets at each column in the first example and at each group in the second example.

## Filtering

Use `Records.apply_mask` to filter elements per column/group:

```pycon
>>> mask = [True, False, True, False, True, False, True, False, True]
>>> filtered_records = records.apply_mask(mask)
>>> filtered_records.records
   id  col  idx  some_field
0   0    0    0        10.0
1   2    0    2        12.0
2   1    1    1        14.0
3   0    2    0        16.0
4   2    2    2        18.0
```

## Grouping

One of the key features of `Records` is that you can perform reducing operations on a group
of columns as if they were a single column. Groups can be specified by `group_by`, which
can be anything from positions or names of column levels, to a NumPy array with actual groups.

There are multiple ways of define grouping:

* When creating `Records`, pass `group_by` to `vectorbtpro.base.wrapping.ArrayWrapper`:

```pycon
>>> group_by = np.array(['first', 'first', 'second'])
>>> grouped_wrapper = wrapper.replace(group_by=group_by)
>>> grouped_records = vbt.Records(grouped_wrapper, records_arr)

>>> grouped_records.map_field('some_field').mean()
first     12.5
second    17.0
dtype: float64
```

* Regroup an existing `Records`:

```pycon
>>> records.regroup(group_by).map_field('some_field').mean()
first     12.5
second    17.0
dtype: float64
```

* Pass `group_by` directly to the mapping method:

```pycon
>>> records.map_field('some_field', group_by=group_by).mean()
first     12.5
second    17.0
dtype: float64
```

* Pass `group_by` directly to the reducing method:

```pycon
>>> records.map_field('some_field').mean(group_by=group_by)
a    11.0
b    14.0
c    17.0
dtype: float64
```

!!! note
    Grouping applies only to reducing operations, there is no change to the arrays.

## Indexing

Like any other class subclassing `vectorbtpro.base.wrapping.Wrapping`, we can do pandas indexing
on a `Records` instance, which forwards indexing operation to each object with columns:

```pycon
>>> records['a'].records
   id  col  idx  some_field
0   0    0    0        10.0
1   1    0    1        11.0
2   2    0    2        12.0

>>> grouped_records['first'].records
   id  col  idx  some_field
0   0    0    0        10.0
1   1    0    1        11.0
2   2    0    2        12.0
3   0    1    0        13.0
4   1    1    1        14.0
5   2    1    2        15.0
```

!!! note
    Changing index (time axis) is not supported. The object should be treated as a Series
    rather than a DataFrame; for example, use `some_field.iloc[0]` instead of `some_field.iloc[:, 0]`
    to get the first column.

    Indexing behavior depends solely upon `vectorbtpro.base.wrapping.ArrayWrapper`.
    For example, if `group_select` is enabled indexing will be performed on groups when grouped,
    otherwise on single columns.

## Caching

`Records` supports caching. If a method or a property requires heavy computation, it's wrapped
with `vectorbtpro.utils.decorators.cached_method` and `vectorbtpro.utils.decorators.cached_property`
respectively. Caching can be disabled globally via `vectorbtpro._settings.caching`.

!!! note
    Because of caching, class is meant to be immutable and all properties are read-only.
    To change any attribute, use the `Records.replace` method and pass changes as keyword arguments.

## Saving and loading

Like any other class subclassing `vectorbtpro.utils.pickling.Pickleable`, we can save a `Records`
instance to the disk with `Records.save` and load it with `Records.load`.

## Stats

!!! hint
    See `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats` and `Records.metrics`.

```pycon
>>> records.stats(column='a')
Start                          x
End                            z
Period           3 days 00:00:00
Total Records                  3
Name: a, dtype: object
```

`Records.stats` also supports (re-)grouping:

```pycon
>>> grouped_records.stats(column='first')
Start                          x
End                            z
Period           3 days 00:00:00
Total Records                  6
Name: first, dtype: object
```

## Plots

!!! hint
    See `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots` and `Records.subplots`.

This class is too generic to have any subplots, but feel free to add custom subplots to your subclass.

## Extending

`Records` class can be extended by subclassing.

In case some of our fields have the same meaning but different naming (such as the base field `idx`)
or other properties, we can override `field_config` using `vectorbtpro.records.decorators.override_field_config`.
It will look for configs of all base classes and merge our config on top of them. This preserves
any base class property that is not explicitly listed in our config.

```pycon
>>> from vectorbtpro.records.decorators import override_field_config

>>> my_dt = np.dtype([
...     ('my_id', np.int_),
...     ('my_col', np.int_),
...     ('my_idx', np.int_)
... ])

>>> my_fields_config = dict(
...     dtype=my_dt,
...     settings=dict(
...         id=dict(name='my_id'),
...         col=dict(name='my_col'),
...         idx=dict(name='my_idx')
...     )
... )
>>> @override_field_config(my_fields_config)
... class MyRecords(vbt.Records):
...     pass

>>> records_arr = np.array([
...     (0, 0, 0),
...     (1, 0, 1),
...     (0, 1, 0),
...     (1, 1, 1)
... ], dtype=my_dt)
>>> wrapper = vbt.ArrayWrapper(index=['x', 'y'],
...     columns=['a', 'b'], ndim=2, freq='1 day')
>>> my_records = MyRecords(wrapper, records_arr)

>>> my_records.id_arr
array([0, 1, 0, 1])

>>> my_records.col_arr
array([0, 0, 1, 1])

>>> my_records.idx_arr
array([0, 1, 0, 1])
```

Alternatively, we can override the `_field_config` class attribute.

```pycon
>>> @override_field_config
... class MyRecords(vbt.Records):
...     _field_config = dict(
...         dtype=my_dt,
...         settings=dict(
...             id=dict(name='my_id'),
...             idx=dict(name='my_idx'),
...             col=dict(name='my_col')
...         )
...     )
```

!!! note
    Don't forget to decorate the class with `@override_field_config` to inherit configs from base classes.

    You can stop inheritance by not decorating or passing `merge_configs=False` to the decorator.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: MetaFields
```
Meta class that exposes a read-only class property `MetaFields.field_config`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: RecordsWithFields
```
Class exposes a read-only class property `RecordsWithFields.field_config`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
```
Wraps the actual records array (such as trades) and exposes methods for mapping
it to some array of values (such as PnL of each trade).

Args:
    wrapper (ArrayWrapper): Array wrapper.

        See `vectorbtpro.base.wrapping.ArrayWrapper`.
    records_arr (array_like): A structured NumPy array of records.

        Must have the fields `id` (record index) and `col` (column index).
    col_mapper (ColumnMapper): Column mapper if already known.

        !!! note
            It depends on `records_arr`, so make sure to invalidate `col_mapper` upon creating
            a `Records` instance with a modified `records_arr`.

            `Records.replace` does it automatically.
    **kwargs: Custom keyword arguments passed to the config.

        Useful if any subclass wants to extend the config.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: MetaFields
#### Função: field_config
```
Field config.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: RecordsWithFields
#### Função: field_config
```
Field config of `${cls_name}`.

```python
${field_config}
```
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: field_config
```
Field config of `${cls_name}`.

```python
${field_config}
```

Returns `${cls_name}._field_config`, which gets (hybrid-) copied upon creation of each instance.
Thus, changing this config won't affect the class.

To change fields, you can either change the config in-place, override this property,
or overwrite the instance variable `${cls_name}._field_config`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: row_stack_records_arrs
```
Stack multiple record arrays along rows.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: get_row_stack_record_indices
```
Get the indices that map concatenated record arrays into the row-stacked record array.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: row_stack
```
Stack multiple `Records` instances along rows.

Uses `vectorbtpro.base.wrapping.ArrayWrapper.row_stack` to stack the wrappers
and `Records.row_stack_records_arrs` to stack the record arrays.

!!! note
    Will produce a column-sorted array.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: column_stack_records_arrs
```
Stack multiple record arrays along columns.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: get_column_stack_record_indices
```
Get the indices that map concatenated record arrays into the column-stacked record array.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: column_stack
```
Stack multiple `Records` instances along columns.

Uses `vectorbtpro.base.wrapping.ArrayWrapper.column_stack` to stack the wrappers
and `Records.column_stack_records_arrs` to stack the record arrays.

`get_indexer_kwargs` are passed to
[pandas.Index.get_indexer](https://pandas.pydata.org/docs/reference/api/pandas.Index.get_indexer.html)
to translate old indices to new ones after the reindexing operation.

!!! note
    Will produce a column-sorted array.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: replace
```
See `vectorbtpro.utils.config.Configured.replace`.

Also, makes sure that `Records.col_mapper` is not passed to the new instance.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: select_cols
```
Select columns.

Returns indices and new record array. Automatically decides whether to use column lengths or column map.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: indexing_func_meta
```
Perform indexing on `Records` and return metadata.

By default, all fields that are mapped to index are indexed.
To avoid indexing on some fields, set their setting `noindex` to True.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: indexing_func
```
Perform indexing on `Records`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: resample_records_arr
```
Perform resampling on the record array.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: resample_meta
```
Perform resampling on `Records` and return metadata.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: resample
```
Perform resampling on `Records`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: records_arr
```
Records array.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: values
```
Records array.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: records
```
Records.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: col_mapper
```
Column mapper.

See `vectorbtpro.records.col_mapper.ColumnMapper`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: field_names
```
Field names.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: to_readable
```
Get records in a human-readable format.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: records_readable
```
`Records.to_readable` with default arguments.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: get_field_setting
```
Get any setting of the field. Uses `Records.field_config`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: get_field_name
```
Get the name of the field. Uses `Records.field_config`..
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: get_field_title
```
Get the title of the field. Uses `Records.field_config`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: get_field_mapping
```
Get the mapping of the field. Uses `Records.field_config`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: get_field_arr
```
Get the array of the field. Uses `Records.field_config`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: get_map_field
```
Get the mapped array of the field. Uses `Records.field_config`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: get_map_field_to_index
```
Get the mapped array on the field, with index applied. Uses `Records.field_config`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: get_map_field_to_columns
```
Get the mapped array on the field, with columns applied. Uses `Records.field_config`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: get_apply_mapping_arr
```
Get the mapped array on the field, with mapping applied. Uses `Records.field_config`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: get_apply_mapping_str_arr
```
Get the mapped array on the field, with mapping applied and stringified. Uses `Records.field_config`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: id_arr
```
Get id array.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: col_arr
```
Get column array.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: idx_arr
```
Get index array.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: is_sorted
```
Check whether records are sorted.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: sort
```
Sort records by columns (primary) and ids (secondary, optional).

!!! note
    Sorting is expensive. A better approach is to append records already in the correct order.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: apply_mask
```
Return a new class instance, filtered by mask.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: first_n
```
Return the first N records in each column.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: last_n
```
Return the last N records in each column.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: random_n
```
Return random N records in each column.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: map_array
```
Convert array to mapped array.

The length of the array must match that of the records.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: map_field
```
Convert field to mapped array.

`**kwargs` are passed to `Records.map_array`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: map
```
Map each record to a scalar value. Returns mapped array.

See `vectorbtpro.records.nb.map_records_nb`.

For details on the meta version, see `vectorbtpro.records.nb.map_records_meta_nb`.

`**kwargs` are passed to `Records.map_array`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: apply
```
Apply function on records per column/group. Returns mapped array.

Applies per group if `apply_per_group` is True.

See `vectorbtpro.records.nb.apply_nb`.

For details on the meta version, see `vectorbtpro.records.nb.apply_meta_nb`.

`**kwargs` are passed to `Records.map_array`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: get_pd_mask
```
Get mask in form of a Series/DataFrame from row and column indices.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: pd_mask
```
`MappedArray.get_pd_mask` with default arguments.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: count
```
Get count by column.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: has_conflicts
```
See `vectorbtpro.records.mapped_array.MappedArray.has_conflicts`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: coverage_map
```
See `vectorbtpro.records.mapped_array.MappedArray.coverage_map`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: stats_defaults
```
Defaults for `Records.stats`.

Merges `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats_defaults` and
`stats` from `vectorbtpro._settings.records`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: prepare_customdata
```
Prepare customdata and hoverinfo for Plotly.

Will display all fields in the data type or only those in `incl_fields`, unless any of them has
the field config setting `as_customdata` disabled, or it's listed in `excl_fields`.
Additionally, you can define `hovertemplate` in the field config such as by using
`vectorbtpro.utils.template.Sub` where `title` is substituted by the title and `index` is
substituted by (final) index in the customdata. If provided as a string, will be wrapped with
`vectorbtpro.utils.template.Sub`. Defaults to "$title: %{{customdata[$index]}}". Mapped fields
will be stringified automatically.

To append one or more custom arrays, provide `append_info` as a list of tuples, each consisting
of a 1-dim NumPy array, title, and optionally hoverinfo. If the array's data type is `object`,
will treat it as strings, otherwise as numbers.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: plots_defaults
```
Defaults for `Records.plots`.

Merges `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots_defaults` and
`plots` from `vectorbtpro._settings.records`.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: build_field_config_doc
```
Build field config documentation.
```

---

# Pasta: records
### Arquivo: base.py
#### Classe: Records
#### Função: override_field_config_doc
```
Call this method on each subclass that overrides `Records.field_config`.
```

---

# Pasta: records
### Arquivo: chunking.py
#### Docstring do Módulo
```
Extensions for chunking records and mapped arrays.
```

---

# Pasta: records
### Arquivo: chunking.py
#### Função: fix_field_in_records
```
Fix a field of the record array in each chunk.
```

---

# Pasta: records
### Arquivo: chunking.py
#### Função: merge_records
```
Merge chunks of record arrays.

Mapper is only applied on the column field.
```

---

# Pasta: records
### Arquivo: col_mapper.py
#### Docstring do Módulo
```
Class for mapping column arrays.
```

---

# Pasta: records
### Arquivo: col_mapper.py
#### Classe: ColumnMapper
```
Used by `vectorbtpro.records.base.Records` and `vectorbtpro.records.mapped_array.MappedArray`
classes to make use of column and group metadata.
```

---

# Pasta: records
### Arquivo: col_mapper.py
#### Classe: ColumnMapper
#### Função: row_stack
```
Stack multiple `ColumnMapper` instances along rows.

Uses `vectorbtpro.base.wrapping.ArrayWrapper.row_stack` to stack the wrappers.

!!! note
    Will produce a column-sorted array.
```

---

# Pasta: records
### Arquivo: col_mapper.py
#### Classe: ColumnMapper
#### Função: column_stack
```
Stack multiple `ColumnMapper` instances along columns.

Uses `vectorbtpro.base.wrapping.ArrayWrapper.column_stack` to stack the wrappers.

!!! note
    Will produce a column-sorted array.
```

---

# Pasta: records
### Arquivo: col_mapper.py
#### Classe: ColumnMapper
#### Função: select_cols
```
Select columns.

Returns indices and new column array. Automatically decides whether to use column lengths or column map.
```

---

# Pasta: records
### Arquivo: col_mapper.py
#### Classe: ColumnMapper
#### Função: indexing_func_meta
```
Perform indexing on `ColumnMapper` and return metadata.
```

---

# Pasta: records
### Arquivo: col_mapper.py
#### Classe: ColumnMapper
#### Função: indexing_func
```
Perform indexing on `ColumnMapper`.
```

---

# Pasta: records
### Arquivo: col_mapper.py
#### Classe: ColumnMapper
#### Função: col_arr
```
Column array.
```

---

# Pasta: records
### Arquivo: col_mapper.py
#### Classe: ColumnMapper
#### Função: get_col_arr
```
Get group-aware column array.
```

---

# Pasta: records
### Arquivo: col_mapper.py
#### Classe: ColumnMapper
#### Função: col_lens
```
Column lengths.

Faster than `ColumnMapper.col_map` but only compatible with sorted columns.
```

---

# Pasta: records
### Arquivo: col_mapper.py
#### Classe: ColumnMapper
#### Função: get_col_lens
```
Get group-aware column lengths.
```

---

# Pasta: records
### Arquivo: col_mapper.py
#### Classe: ColumnMapper
#### Função: col_map
```
Column map.

More flexible than `ColumnMapper.col_lens`.
More suited for mapped arrays.
```

---

# Pasta: records
### Arquivo: col_mapper.py
#### Classe: ColumnMapper
#### Função: get_col_map
```
Get group-aware column map.
```

---

# Pasta: records
### Arquivo: col_mapper.py
#### Classe: ColumnMapper
#### Função: is_sorted
```
Check whether column array is sorted.
```

---

# Pasta: records
### Arquivo: col_mapper.py
#### Classe: ColumnMapper
#### Função: new_id_arr
```
Generate a new id array.
```

---

# Pasta: records
### Arquivo: col_mapper.py
#### Classe: ColumnMapper
#### Função: get_new_id_arr
```
Generate a new group-aware id array.
```

---

# Pasta: records
### Arquivo: decorators.py
#### Docstring do Módulo
```
Class decorators for records.
```

---

# Pasta: records
### Arquivo: decorators.py
#### Função: override_field_config
```
Class decorator to override the field config of a class subclassing
`vectorbtpro.records.base.Records`.

Instead of overriding `_field_config` class attribute, you can pass `config` directly to this decorator.

Disable `merge_configs` to not merge, which will effectively disable field inheritance.
```

---

# Pasta: records
### Arquivo: decorators.py
#### Função: attach_fields
```
Class decorator to attach field properties in a `vectorbtpro.records.base.Records` class.

Will extract `dtype` and other relevant information from `vectorbtpro.records.base.Records.field_config`
and map its fields as properties. This behavior can be changed by using `config`.

!!! note
    Make sure to run `attach_fields` after `override_field_config`.

`config` must contain fields (keys) and dictionaries (values) with the following keys:

* `attach`: Whether to attach the field property. Can be provided as a string to be used
    as a target attribute name. Defaults to True.
* `defaults`: Dictionary with default keyword arguments for `vectorbtpro.records.base.Records.map_field`.
    Defaults to an empty dict.
* `attach_filters`: Whether to attach filters based on the field's values. Can be provided as a dict
    to be used instead of the mapping (filter value -> target filter name). Defaults to False.
    If True, defaults to `mapping` in `vectorbtpro.records.base.Records.field_config`.
* `filter_defaults`: Dictionary with default keyword arguments for `vectorbtpro.records.base.Records.apply_mask`.
    Can be provided by target filter name. Defaults to an empty dict.
* `on_conflict`: Overrides global `on_conflict` for both field and filter properties.

Any potential attribute name is prepared by placing underscores between capital letters and
converting to the lower case.

If an attribute with the same name already exists in the class but the name is not listed in the field config:

* it will be overridden if `on_conflict` is 'override'
* it will be ignored if `on_conflict` is 'ignore'
* an error will be raised if `on_conflict` is 'raise'
```

---

# Pasta: records
### Arquivo: decorators.py
#### Função: attach_shortcut_properties
```
Class decorator to attach shortcut properties.

`config` must contain target property names (keys) and settings (values) with the following keys:

* `method_name`: Name of the source method. Defaults to the target name prepended with the prefix `get_`.
* `obj_type`: Type of the returned object. Can be 'array' for 2-dim arrays, 'red_array' for 1-dim arrays,
    'records' for record arrays, and 'mapped_array' for mapped arrays. Defaults to 'records'.
* `group_aware`: Whether the returned object is aligned based on the current grouping.
    Defaults to True.
* `method_kwargs`: Keyword arguments passed to the source method. Defaults to None.
* `decorator`: Defaults to `vectorbtpro.utils.decorators.cached_property` for object types
    'records' and 'red_array'. Otherwise, to `vectorbtpro.utils.decorators.cacheable_property`.
* `decorator_kwargs`: Keyword arguments passed to the decorator. By default,
    includes options `obj_type` and `group_aware`.
* `docstring`: Method docstring.

The class must be a subclass of `vectorbtpro.records.base.Records`.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Docstring do Módulo
```
Base class for working with mapped arrays.

This class takes the mapped array and the corresponding column and (optionally) index arrays,
and offers features to directly process the mapped array without converting it to pandas;
for example, to compute various statistics by column, such as standard deviation.

Consider the following example:

```pycon
>>> from vectorbtpro import *

>>> a = np.array([10., 11., 12., 13., 14., 15., 16., 17., 18.])
>>> col_arr = np.array([0, 0, 0, 1, 1, 1, 2, 2, 2])
>>> idx_arr = np.array([0, 1, 2, 0, 1, 2, 0, 1, 2])
>>> wrapper = vbt.ArrayWrapper(index=['x', 'y', 'z'],
...     columns=['a', 'b', 'c'], ndim=2, freq='1 day')
>>> ma = vbt.MappedArray(wrapper, a, col_arr, idx_arr=idx_arr)
```

## Reducing

Using `MappedArray`, we can then reduce by column as follows:

* Use already provided reducers such as `MappedArray.mean`:

```pycon
>>> ma.mean()
a    11.0
b    14.0
c    17.0
dtype: float64
```

* Use `MappedArray.to_pd` to map to pandas and then reduce manually (expensive):

```pycon
>>> ma.to_pd().mean()
a    11.0
b    14.0
c    17.0
dtype: float64
```

* Use `MappedArray.reduce` to reduce using a custom function:

```pycon
>>> # Reduce to a scalar

>>> @njit
... def pow_mean_reduce_nb(a, pow):
...     return np.mean(a ** pow)

>>> ma.reduce(pow_mean_reduce_nb, 2)
a    121.666667
b    196.666667
c    289.666667
dtype: float64

>>> # Reduce to an array

>>> @njit
... def min_max_reduce_nb(a):
...     return np.array([np.min(a), np.max(a)])

>>> ma.reduce(min_max_reduce_nb, returns_array=True,
...     wrap_kwargs=dict(name_or_index=['min', 'max']))
        a     b     c
min  10.0  13.0  16.0
max  12.0  15.0  18.0

>>> # Reduce to an array of indices

>>> @njit
... def idxmin_idxmax_reduce_nb(a):
...     return np.array([np.argmin(a), np.argmax(a)])

>>> ma.reduce(idxmin_idxmax_reduce_nb, returns_array=True,
...     returns_idx=True, wrap_kwargs=dict(name_or_index=['idxmin', 'idxmax']))
        a  b  c
idxmin  x  x  x
idxmax  z  z  z

>>> # Reduce using a meta function to combine multiple mapped arrays

>>> @njit
... def mean_ratio_reduce_meta_nb(idxs, col, a, b):
...     return np.mean(a[idxs]) / np.mean(b[idxs])

>>> vbt.MappedArray.reduce(mean_ratio_reduce_meta_nb,
...     ma.values - 1, ma.values + 1, col_mapper=ma.col_mapper)
a    0.833333
b    0.866667
c    0.888889
Name: reduce, dtype: float64
```

## Mapping

Use `MappedArray.apply` to apply a function on each column/group:

```pycon
>>> @njit
... def cumsum_apply_nb(a):
...     return np.cumsum(a)

>>> ma.apply(cumsum_apply_nb)
<vectorbtpro.records.mapped_array.MappedArray at 0x7ff061382198>

>>> ma.apply(cumsum_apply_nb).values
array([10., 21., 33., 13., 27., 42., 16., 33., 51.])

>>> group_by = np.array(['first', 'first', 'second'])
>>> ma.apply(cumsum_apply_nb, group_by=group_by, apply_per_group=True).values
array([10., 21., 33., 46., 60., 75., 16., 33., 51.])

>>> # Apply using a meta function

>>> @njit
... def cumsum_apply_meta_nb(ridxs, col, a):
...     return np.cumsum(a[ridxs])

>>> vbt.MappedArray.apply(cumsum_apply_meta_nb, ma.values, col_mapper=ma.col_mapper).values
array([10., 21., 33., 13., 27., 42., 16., 33., 51.])
```

Notice how cumsum resets at each column in the first example and at each group in the second example.

## Conversion

We can unstack any `MappedArray` instance to pandas:

* Given `idx_arr` was provided:

```pycon
>>> ma.to_pd()
      a     b     c
x  10.0  13.0  16.0
y  11.0  14.0  17.0
z  12.0  15.0  18.0
```

!!! note
    Will throw a warning if there are multiple values pointing to the same position.

* In case `group_by` was provided, index can be ignored, or there are position conflicts:

```pycon
>>> ma.to_pd(group_by=np.array(['first', 'first', 'second']), ignore_index=True)
   first  second
0   10.0    16.0
1   11.0    17.0
2   12.0    18.0
3   13.0     NaN
4   14.0     NaN
5   15.0     NaN
```

## Resolving conflicts

Sometimes, we may encounter multiple values for each index and column combination.
In such case, we can use `MappedArray.reduce_segments` to aggregate "duplicate" elements.
For example, let's sum up duplicate values per each index and column combination:

```pycon
>>> ma_conf = ma.replace(idx_arr=np.array([0, 0, 0, 1, 1, 1, 2, 2, 2]))
>>> ma_conf.to_pd()
UserWarning: Multiple values are pointing to the same position. Only the latest value is used.
      a     b     c
x  12.0   NaN   NaN
y   NaN  15.0   NaN
z   NaN   NaN  18.0

>>> @njit
... def sum_reduce_nb(a):
...     return np.sum(a)

>>> ma_no_conf = ma_conf.reduce_segments(
...     (ma_conf.idx_arr, ma_conf.col_arr),
...     sum_reduce_nb
... )
>>> ma_no_conf.to_pd()
      a     b     c
x  33.0   NaN   NaN
y   NaN  42.0   NaN
z   NaN   NaN  51.0
```

## Filtering

Use `MappedArray.apply_mask` to filter elements per column/group:

```pycon
>>> mask = [True, False, True, False, True, False, True, False, True]
>>> filtered_ma = ma.apply_mask(mask)
>>> filtered_ma.count()
a    2
b    1
c    2
dtype: int64

>>> filtered_ma.id_arr
array([0, 2, 4, 6, 8])
```

## Grouping

One of the key features of `MappedArray` is that we can perform reducing operations on a group
of columns as if they were a single column. Groups can be specified by `group_by`, which
can be anything from positions or names of column levels, to a NumPy array with actual groups.

There are multiple ways of define grouping:

* When creating `MappedArray`, pass `group_by` to `vectorbtpro.base.wrapping.ArrayWrapper`:

```pycon
>>> group_by = np.array(['first', 'first', 'second'])
>>> grouped_wrapper = wrapper.replace(group_by=group_by)
>>> grouped_ma = vbt.MappedArray(grouped_wrapper, a, col_arr, idx_arr=idx_arr)

>>> grouped_ma.mean()
first     12.5
second    17.0
dtype: float64
```

* Regroup an existing `MappedArray`:

```pycon
>>> ma.regroup(group_by).mean()
first     12.5
second    17.0
dtype: float64
```

* Pass `group_by` directly to the reducing method:

```pycon
>>> ma.mean(group_by=group_by)
first     12.5
second    17.0
dtype: float64
```

By the same way we can disable or modify any existing grouping:

```pycon
>>> grouped_ma.mean(group_by=False)
a    11.0
b    14.0
c    17.0
dtype: float64
```

!!! note
    Grouping applies only to reducing operations, there is no change to the arrays.

## Operators

`MappedArray` implements arithmetic, comparison, and logical operators. We can perform basic
operations (such as addition) on mapped arrays as if they were NumPy arrays.

```pycon
>>> ma ** 2
<vectorbtpro.records.mapped_array.MappedArray at 0x7f97bfc49358>

>>> ma * np.array([1, 2, 3, 4, 5, 6])
<vectorbtpro.records.mapped_array.MappedArray at 0x7f97bfc65e80>

>>> ma + ma
<vectorbtpro.records.mapped_array.MappedArray at 0x7fd638004d30>
```

!!! note
    Ensure that your `MappedArray` operand is on the left if the other operand is an array.

    If two `MappedArray` operands have different metadata, will copy metadata from the first one,
    but at least their `id_arr` and `col_arr` must match.

## Indexing

Like any other class subclassing `vectorbtpro.base.wrapping.Wrapping`, we can do pandas indexing
on a `MappedArray` instance, which forwards indexing operation to each object with columns:

```pycon
>>> ma['a'].values
array([10., 11., 12.])

>>> grouped_ma['first'].values
array([10., 11., 12., 13., 14., 15.])
```

!!! note
    Changing index (time axis) is not supported. The object should be treated as a Series
    rather than a DataFrame; for example, use `some_field.iloc[0]` instead of `some_field.iloc[:, 0]`
    to get the first column.

    Indexing behavior depends solely upon `vectorbtpro.base.wrapping.ArrayWrapper`.
    For example, if `group_select` is enabled indexing will be performed on groups,
    otherwise on single columns.

## Caching

`MappedArray` supports caching. If a method or a property requires heavy computation, it's wrapped
with `vectorbtpro.utils.decorators.cached_method` and `vectorbtpro.utils.decorators.cached_property`
respectively. Caching can be disabled globally in `vectorbtpro._settings.caching`.

!!! note
    Because of caching, class is meant to be immutable and all properties are read-only.
    To change any attribute, use the `MappedArray.replace` method and pass changes as keyword arguments.

## Saving and loading

Like any other class subclassing `vectorbtpro.utils.pickling.Pickleable`, we can save a `MappedArray`
instance to the disk with `MappedArray.save` and load it with `MappedArray.load`.

## Stats

!!! hint
    See `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats` and `MappedArray.metrics`.

Metric for mapped arrays are similar to that for `vectorbtpro.generic.accessors.GenericAccessor`.

```pycon
>>> ma.stats(column='a')
Start                      x
End                        z
Period       3 days 00:00:00
Count                      3
Mean                    11.0
Std                      1.0
Min                     10.0
Median                  11.0
Max                     12.0
Min Index                  x
Max Index                  z
Name: a, dtype: object
```

The main difference unfolds once the mapped array has a mapping:
values are then considered as categorical and usual statistics are meaningless to compute.
For this case, `MappedArray.stats` returns the value counts:

```pycon
>>> mapping = {v: "test_" + str(v) for v in np.unique(ma.values)}
>>> ma.stats(column='a', settings=dict(mapping=mapping))
Start                                    x
End                                      z
Period                     3 days 00:00:00
Count                                    3
Value Counts: test_10.0                  1
Value Counts: test_11.0                  1
Value Counts: test_12.0                  1
Value Counts: test_13.0                  0
Value Counts: test_14.0                  0
Value Counts: test_15.0                  0
Value Counts: test_16.0                  0
Value Counts: test_17.0                  0
Value Counts: test_18.0                  0
Name: a, dtype: object

`MappedArray.stats` also supports (re-)grouping:

```pycon
>>> grouped_ma.stats(column='first')
Start                      x
End                        z
Period       3 days 00:00:00
Count                      6
Mean                    12.5
Std                 1.870829
Min                     10.0
Median                  12.5
Max                     15.0
Min Index                  x
Max Index                  z
Name: first, dtype: object
```

## Plots

We can build histograms and boxplots of `MappedArray` directly:

```pycon
>>> ma.boxplot().show()
```

![](/assets/images/api/mapped_boxplot.light.svg#only-light){: .iimg loading=lazy }
![](/assets/images/api/mapped_boxplot.dark.svg#only-dark){: .iimg loading=lazy }

To use scatterplots or any other plots that require index, convert to pandas first:

```pycon
>>> ma.to_pd().vbt.plot().show()
```

![](/assets/images/api/mapped_to_pd_plot.light.svg#only-light){: .iimg loading=lazy }
![](/assets/images/api/mapped_to_pd_plot.dark.svg#only-dark){: .iimg loading=lazy }

!!! hint
    See `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots` and `MappedArray.subplots`.

`MappedArray` class has a single subplot based on `MappedArray.to_pd` and
`vectorbtpro.generic.accessors.GenericAccessor.plot`.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Função: combine_mapped_with_other
```
Combine `MappedArray` with other compatible object.

If other object is also `MappedArray`, their `id_arr` and `col_arr` must match.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
```
Exposes methods for reducing, converting, and plotting arrays mapped by
`vectorbtpro.records.base.Records` class.

Args:
    wrapper (ArrayWrapper): Array wrapper.

        See `vectorbtpro.base.wrapping.ArrayWrapper`.
    mapped_arr (array_like): A one-dimensional array of mapped record values.
    col_arr (array_like): A one-dimensional column array.

        Must be of the same size as `mapped_arr`.
    id_arr (array_like): A one-dimensional id array. Defaults to simple range.

        Must be of the same size as `mapped_arr`.
    idx_arr (array_like): A one-dimensional index array. Optional.

        Must be of the same size as `mapped_arr`.
    mapping (namedtuple, dict or callable): Mapping.
    col_mapper (ColumnMapper): Column mapper if already known.

        !!! note
            It depends upon `wrapper` and `col_arr`, so make sure to invalidate `col_mapper` upon creating
            a `MappedArray` instance with a modified `wrapper` or `col_arr.

            `MappedArray.replace` does it automatically.
    **kwargs: Custom keyword arguments passed to the config.

        Useful if any subclass wants to extend the config.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: row_stack
```
Stack multiple `MappedArray` instances along rows.

Uses `vectorbtpro.base.wrapping.ArrayWrapper.row_stack` to stack the wrappers.

!!! note
    Will produce a column-sorted array.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: column_stack
```
Stack multiple `MappedArray` instances along columns.

Uses `vectorbtpro.base.wrapping.ArrayWrapper.column_stack` to stack the wrappers.

`get_indexer_kwargs` are passed to
[pandas.Index.get_indexer](https://pandas.pydata.org/docs/reference/api/pandas.Index.get_indexer.html)
to translate old indices to new ones after the reindexing operation.

!!! note
    Will produce a column-sorted array.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: replace
```
See `vectorbtpro.utils.config.Configured.replace`.

Also, makes sure that `MappedArray.col_mapper` is not passed to the new instance.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: indexing_func_meta
```
Perform indexing on `MappedArray` and return metadata.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: indexing_func
```
Perform indexing on `MappedArray`.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: resample_meta
```
Perform resampling on `MappedArray` and return metadata.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: resample
```
Perform resampling on `MappedArray`.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: mapped_arr
```
Mapped array.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: values
```
Mapped array.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: to_readable
```
Get values in a human-readable format.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: mapped_readable
```
`MappedArray.to_readable` with default arguments.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: col_arr
```
Column array.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: col_mapper
```
Column mapper.

See `vectorbtpro.records.col_mapper.ColumnMapper`.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: idx_arr
```
Index array.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: id_arr
```
Id array.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: mapping
```
Mapping.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: is_sorted
```
Check whether mapped array is sorted.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: sort
```
Sort mapped array by column array (primary) and id array (secondary, optional).

`**kwargs` are passed to `MappedArray.replace`.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: apply_mask
```
Return a new class instance, filtered by mask.

`**kwargs` are passed to `MappedArray.replace`.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: top_n_mask
```
Return mask of top N elements in each column/group.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: bottom_n_mask
```
Return mask of bottom N elements in each column/group.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: top_n
```
Filter top N elements from each column/group.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: bottom_n
```
Filter bottom N elements from each column/group.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: resolve_mapping
```
Resolve mapping.

Set `mapping` to False to disable mapping completely.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: apply_mapping
```
Apply mapping on each element.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: to_index
```
Convert to index.

If `minus_one_to_zero` is True, index -1 will automatically become 0.
Otherwise, will throw an error.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: to_columns
```
Convert to columns.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: apply
```
Apply function on mapped array per column/group. Returns a new mapped array.

Applies per group of columns if `apply_per_group` is True.

See `vectorbtpro.records.nb.apply_nb`.

For details on the meta version, see `vectorbtpro.records.nb.apply_meta_nb`.

`**kwargs` are passed to `MappedArray.replace`.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: reduce_segments
```
Reduce each segment of values in mapped array. Returns a new mapped array.

`segment_arr` must be an array of integers increasing per column, each indicating a segment.
It must have the same length as the mapped array. You can also pass a list of such arrays.
In this case, each unique combination of values will be considered a single segment.
Can also pass the string "idx" to use the index array.

`reduce_func_nb` can be a string denoting the suffix of a reducing function
from `vectorbtpro.generic.nb`. For example, "sum" will refer to "sum_reduce_nb".

!!! warning
    Each segment or combination of segments in `segment_arr` is assumed to be coherent and non-repeating.
    That is, `np.array([0, 1, 0])` for a single column annotates three different segments, not two.
    See `vectorbtpro.utils.array_.index_repeating_rows_nb`.

!!! hint
    Use `MappedArray.sort` to bring the mapped array to the desired order, if required.

Applies per group of columns if `apply_per_group` is True.

See `vectorbtpro.records.nb.reduce_mapped_segments_nb`.

`**kwargs` are passed to `MappedArray.replace`.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: reduce
```
Reduce mapped array by column/group.

Set `returns_array` to True if `reduce_func_nb` returns an array.

Set `returns_idx` to True if `reduce_func_nb` returns row index/position. Must pass `idx_arr`.

Set `to_index` to True to return labels instead of positions.

Use `fill_value` to set the default value.

For implementation details, see

* `vectorbtpro.records.nb.reduce_mapped_nb` if `returns_array` is False and `returns_idx` is False
* `vectorbtpro.records.nb.reduce_mapped_to_idx_nb` if `returns_array` is False and `returns_idx` is True
* `vectorbtpro.records.nb.reduce_mapped_to_array_nb` if `returns_array` is True and `returns_idx` is False
* `vectorbtpro.records.nb.reduce_mapped_to_idx_array_nb` if `returns_array` is True and `returns_idx` is True

For implementation details on the meta versions, see

* `vectorbtpro.records.nb.reduce_mapped_meta_nb` if `returns_array` is False and `returns_idx` is False
* `vectorbtpro.records.nb.reduce_mapped_to_idx_meta_nb` if `returns_array` is False and `returns_idx` is True
* `vectorbtpro.records.nb.reduce_mapped_to_array_meta_nb` if `returns_array` is True and `returns_idx` is False
* `vectorbtpro.records.nb.reduce_mapped_to_idx_array_meta_nb` if `returns_array` is True and `returns_idx` is True
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: nth
```
Return n-th element of each column/group.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: nth_index
```
Return index of n-th element of each column/group.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: min
```
Return min by column/group.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: max
```
Return max by column/group.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: mean
```
Return mean by column/group.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: median
```
Return median by column/group.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: std
```
Return std by column/group.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: sum
```
Return sum by column/group.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: idxmin
```
Return index of min by column/group.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: idxmax
```
Return index of max by column/group.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: describe
```
Return statistics by column/group.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: count
```
Return number of values by column/group.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: value_counts
```
See `vectorbtpro.generic.accessors.GenericAccessor.value_counts`.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: has_conflicts
```
See `vectorbtpro.records.nb.mapped_has_conflicts_nb`.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: coverage_map
```
See `vectorbtpro.records.nb.mapped_coverage_map_nb`.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: to_pd
```
Unstack mapped array to a Series/DataFrame.

If `reduce_func_nb` is not None, will use it to reduce conflicting index segments
using `MappedArray.reduce_segments`.

* If `ignore_index`, will ignore the index and place values on top of each other in every column/group.
    See `vectorbtpro.records.nb.ignore_unstack_mapped_nb`.
* If `repeat_index`, will repeat any index pointed from multiple values.
    Otherwise, in case of positional conflicts, will throw a warning and use the latest value.
    See `vectorbtpro.records.nb.repeat_unstack_mapped_nb`.
* Otherwise, see `vectorbtpro.records.nb.unstack_mapped_nb`.

!!! note
    Will raise an error if there are multiple values pointing to the same position.
    Set `ignore_index` to True in this case.

!!! warning
    Mapped arrays represent information in the most memory-friendly format.
    Mapping back to pandas may occupy lots of memory if records are sparse.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: get_pd_mask
```
Get mask in form of a Series/DataFrame from row and column indices.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: pd_mask
```
`MappedArray.get_pd_mask` with default arguments.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: stats_defaults
```
Defaults for `MappedArray.stats`.

Merges `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats_defaults` and
`stats` from `vectorbtpro._settings.mapped_array`.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: histplot
```
Plot histogram by column/group.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: boxplot
```
Plot box plot by column/group.
```

---

# Pasta: records
### Arquivo: mapped_array.py
#### Classe: MappedArray
#### Função: plots_defaults
```
Defaults for `MappedArray.plots`.

Merges `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots_defaults` and
`plots` from `vectorbtpro._settings.mapped_array`.
```

---

# Pasta: records
### Arquivo: nb.py
#### Docstring do Módulo
```
Numba-compiled functions for records and mapped arrays.

Provides an arsenal of Numba-compiled functions for records and mapped arrays.
These only accept NumPy arrays and other Numba-compatible types.

!!! note
    All functions passed as argument must be Numba-compiled.

    Records must retain the order they were created in.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: generate_ids_nb
```
Generate the monotonically increasing id array based on the column index array.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: col_lens_nb
```
Get column lengths from sorted column array.

!!! note
    Requires `col_arr` to be in ascending order. This can be done by sorting.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: record_col_lens_select_nb
```
Perform indexing on sorted records using column lengths.

Returns new records.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: col_map_nb
```
Build a map between columns and value indices.

Returns an array with indices segmented by column and an array with column lengths.

Works well for unsorted column arrays.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: record_col_map_select_nb
```
Same as `record_col_lens_select_nb` but using column map `col_map`.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: is_col_sorted_nb
```
Check whether the column array is sorted.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: is_col_id_sorted_nb
```
Check whether the column and id arrays are sorted.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: first_n_nb
```
Returns the mask of the first N elements.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: last_n_nb
```
Returns the mask of the last N elements.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: random_n_nb
```
Returns the mask of random N elements.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: top_n_mapped_nb
```
Returns the mask of the top N mapped elements.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: bottom_n_mapped_nb
```
Returns the mask of the bottom N mapped elements.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: map_records_nb
```
Map each record to a single value.

`map_func_nb` must accept a single record and `*args`. Must return a single value.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: map_records_meta_nb
```
Meta version of `map_records_nb`.

`map_func_nb` must accept the record index and `*args`. Must return a single value.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: apply_nb
```
Apply function on mapped array or records per column.

Returns the same shape as `arr`.

`apply_func_nb` must accept the values of the column and `*args`. Must return an array.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: apply_meta_nb
```
Meta version of `apply_nb`.

`apply_func_nb` must accept the indices, the column index, and `*args`. Must return an array.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: reduce_mapped_segments_nb
```
Reduce each segment of values in mapped array.

Uses the last column, index, and id of each segment for the new value.

`reduce_func_nb` must accept the values in the segment and `*args`. Must return a single value.

!!! note
    Groups must come in ascending order per column, and `idx_arr` and `id_arr`
    must come in ascending order per segment of values.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: reduce_mapped_nb
```
Reduce mapped array by column to a single value.

Faster than `unstack_mapped_nb` and `vbt.*` used together, and also
requires less memory. But does not take advantage of caching.

`reduce_func_nb` must accept the mapped array and `*args`.
Must return a single value.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: reduce_mapped_meta_nb
```
Meta version of `reduce_mapped_nb`.

`reduce_func_nb` must accept the mapped indices, the column index, and `*args`.
Must return a single value.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: reduce_mapped_to_idx_nb
```
Reduce mapped array by column to an index.

Same as `reduce_mapped_nb` except `idx_arr` must be passed.

!!! note
    Must return integers or raise an exception.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: reduce_mapped_to_idx_meta_nb
```
Meta version of `reduce_mapped_to_idx_nb`.

`reduce_func_nb` is the same as in `reduce_mapped_meta_nb`.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: reduce_mapped_to_array_nb
```
Reduce mapped array by column to an array.

`reduce_func_nb` same as for `reduce_mapped_nb` but must return an array.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: reduce_mapped_to_array_meta_nb
```
Meta version of `reduce_mapped_to_array_nb`.

`reduce_func_nb` is the same as in `reduce_mapped_meta_nb`.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: reduce_mapped_to_idx_array_nb
```
Reduce mapped array by column to an index array.

Same as `reduce_mapped_to_array_nb` except `idx_arr` must be passed.

!!! note
    Must return integers or raise an exception.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: reduce_mapped_to_idx_array_meta_nb
```
Meta version of `reduce_mapped_to_idx_array_nb`.

`reduce_func_nb` is the same as in `reduce_mapped_meta_nb`.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: mapped_value_counts_per_col_nb
```
Get value counts per column/group of an already factorized mapped array.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: mapped_value_counts_per_row_nb
```
Get value counts per row of an already factorized mapped array.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: mapped_value_counts_nb
```
Get value counts globally of an already factorized mapped array.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: mapped_has_conflicts_nb
```
Check whether mapped array has positional conflicts.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: mapped_coverage_map_nb
```
Get the coverage map of a mapped array.

Each element corresponds to the number of times it was referenced (= duplicates of `col_arr` and `idx_arr`).
More than one depicts a positional conflict.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: unstack_mapped_nb
```
Unstack mapped array using index data.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: ignore_unstack_mapped_nb
```
Unstack mapped array by ignoring index data.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: unstack_index_nb
```
Unstack index using the number of times each element must repeat.

`repeat_cnt_arr` can be created from the coverage map.
```

---

# Pasta: records
### Arquivo: nb.py
#### Função: repeat_unstack_mapped_nb
```
Unstack mapped array using repeated index data.
```

---

# Pasta: records
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules for working with records.

Records are the second form of data representation in vectorbtpro. They allow storing sparse event data
such as drawdowns, orders, trades, and positions, without converting them back to the matrix form and
occupying the user's memory.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Docstring do Módulo
```
Global registry for cacheables.

Caching in vectorbt is achieved through a combination of decorators and the registry.
Cacheable decorators such as `vectorbtpro.utils.decorators.cacheable` take a function and wrap
it with another function that behaves like the wrapped function but also takes care of all
caching modalities.

But unlike other implementations such as that of `functools.lru_cache`, the actual caching procedure
doesn't happen nor are the results stored inside the decorators themselves: decorators just register a
so-called "setup" for the wrapped function at the registry (see `CARunSetup`).

## Runnable setups

The actual magic happens within a runnable setup: it takes the function that should be called
and the arguments that should be passed to this function, looks whether the result should be cached,
runs the function, stores the result in the cache, updates the metrics, etc. It then returns the
resulting object to the wrapping function, which in turn returns it to the user. Each setup is stateful
- it stores the cache, the number of hits and misses, and other metadata. Thus, there can be only one
registered setup per each cacheable function globally at a time. To avoid creating new setups for the same
function over and over again, each setup can be uniquely identified by its function through hashing:

```pycon
>>> from vectorbtpro import *

>>> my_func = lambda: np.random.uniform(size=1000000)

>>> # Decorator returns a wrapper
>>> my_ca_func = vbt.cached(my_func)

>>> # Wrapper registers a new setup
>>> my_ca_func.get_ca_setup()
CARunSetup(registry=<vectorbtpro.registries.ca_registry.CacheableRegistry object at 0x7fe14c27df60>, use_cache=True, whitelist=False, cacheable=<function <lambda> at 0x7fe14e94cae8>, instance=None, max_size=None, ignore_args=None, cache={})

>>> # Another call won't register a new setup but return the existing one
>>> my_ca_func.get_ca_setup()
CARunSetup(registry=<vectorbtpro.registries.ca_registry.CacheableRegistry object at 0x7fe14c27df60>, use_cache=True, whitelist=False, cacheable=<function <lambda> at 0x7fe14e94cae8>, instance=None, max_size=None, ignore_args=None, cache={})

>>> # Only one CARunSetup object per wrapper and optionally the instance the wrapper is bound to
>>> hash(my_ca_func.get_ca_setup()) == hash((my_ca_func, None))
True
```

When we call `my_ca_func`, it takes the setup from the registry and calls `CARunSetup.run`.
The caching happens by the setup itself and isn't in any way visible to `my_ca_func`.
To access the cache or any metric of interest, we can ask the setup:

```pycon
>>> my_setup = my_ca_func.get_ca_setup()

>>> # Cache is empty
>>> my_setup.get_stats()
{
    'hash': 4792160544297109364,
    'string': '<bound func __main__.<lambda>>',
    'use_cache': True,
    'whitelist': False,
    'caching_enabled': True,
    'hits': 0,
    'misses': 0,
    'total_size': '0 Bytes',
    'total_elapsed': None,
    'total_saved': None,
    'first_run_time': None,
    'last_run_time': None,
    'first_hit_time': None,
    'last_hit_time': None,
    'creation_time': 'now',
    'last_update_time': None
}

>>> # The result is cached
>>> my_ca_func()
>>> my_setup.get_stats()
{
    'hash': 4792160544297109364,
    'string': '<bound func __main__.<lambda>>',
    'use_cache': True,
    'whitelist': False,
    'caching_enabled': True,
    'hits': 0,
    'misses': 1,
    'total_size': '8.0 MB',
    'total_elapsed': '11.33 milliseconds',
    'total_saved': '0 milliseconds',
    'first_run_time': 'now',
    'last_run_time': 'now',
    'first_hit_time': None,
    'last_hit_time': None,
    'creation_time': 'now',
    'last_update_time': None
}

>>> # The cached result is retrieved
>>> my_ca_func()
>>> my_setup.get_stats()
{
    'hash': 4792160544297109364,
    'string': '<bound func __main__.<lambda>>',
    'use_cache': True,
    'whitelist': False,
    'caching_enabled': True,
    'hits': 1,
    'misses': 1,
    'total_size': '8.0 MB',
    'total_elapsed': '11.33 milliseconds',
    'total_saved': '11.33 milliseconds',
    'first_run_time': 'now',
    'last_run_time': 'now',
    'first_hit_time': 'now',
    'last_hit_time': 'now',
    'creation_time': 'now',
    'last_update_time': None
}
```

## Enabling/disabling caching

To enable or disable caching, we can invoke `CARunSetup.enable_caching` and `CARunSetup.disable_caching`
respectively. This will set `CARunSetup.use_cache` flag to True or False. Even though we expressed
our disire to change caching rules, the final decision also depends on the global settings and whether
the setup is whitelisted in case caching is disabled globally. This decision is available via
`CARunSetup.caching_enabled`:

```pycon
>>> my_setup.disable_caching()
>>> my_setup.caching_enabled
False

>>> my_setup.enable_caching()
>>> my_setup.caching_enabled
True

>>> vbt.settings.caching['disable'] = True
>>> my_setup.caching_enabled
False

>>> my_setup.enable_caching()
UserWarning: This operation has no effect: caching is disabled globally and this setup is not whitelisted

>>> my_setup.enable_caching(force=True)
>>> my_setup.caching_enabled
True

>>> vbt.settings.caching['disable_whitelist'] = True
>>> my_setup.caching_enabled
False

>>> my_setup.enable_caching(force=True)
UserWarning: This operation has no effect: caching and whitelisting are disabled globally
```

To disable registration of new setups completely, use `disable_machinery`:

```pycon
>>> vbt.settings.caching['disable_machinery'] = True
```

## Setup hierarchy

But what if we wanted to change caching rules for an entire instance or class at once?
Even if we changed the setup of every cacheable function declared in the class, how do we
make sure that each future subclass or instance inherits the changes that we applied?
To account for this, vectorbt provides us with a set of setups that both are stateful
and can delegate various operations to their child setups, all the way down to `CARunSetup`.
The setup hierarchy follows the inheritance hierarchy in OOP:

![](/assets/images/api/setup_hierarchy.svg){: loading=lazy style="width:700px;" }

For example, calling `B.get_ca_setup().disable_caching()` would disable caching for each current
and future subclass and instance of `B`, but it won't disable caching for `A` or any other superclass of `B`.
In turn, each instance of `B` would then disable caching for each cacheable property and method in
that instance. As we see, the propagation of this operation is happening from top to bottom.

The reason why unbound setups are stretching outside of their classes in the diagram is
because there is no easy way to derive the class when calling a cacheable decorator,
thus their functions are considered to be living on their own. When calling
`B.f.get_ca_setup().disable_caching()`, we are disabling caching for the function `B.f`
for each current and future subclass and instance of `B`, while all other functions remain untouched.

But what happens when we enable caching for the class `B` and disable caching for the unbound
function `B.f`? Would the future method `b2.f` be cached or not? Quite easy: it would then
inherit the state from the setup that has been updated more recently.

Here is another illustration of how operations are propagated from parents to children:

![](/assets/images/api/setup_propagation.svg){: loading=lazy style="width:800px;" }

The diagram above depicts the following setup hierarchy:

```pycon
>>> # Populate setups at init
>>> vbt.settings.caching.reset()
>>> vbt.settings.caching['register_lazily'] = False

>>> class A(vbt.Cacheable):
...     @vbt.cached_property
...     def f1(self): pass

>>> class B(A):
...     def f2(self): pass

>>> class C(A):
...     @vbt.cached_method
...     def f2(self): pass

>>> b1 = B()
>>> c1 = C()
>>> c2 = C()

>>> print(vbt.prettify(A.get_ca_setup().get_setup_hierarchy()))
[
    {
        "parent": "<class __main__.B>",
        "children": [
            {
                "parent": "<instance of __main__.B>",
                "children": [
                    "<instance property __main__.B.f1>"
                ]
            }
        ]
    },
    {
        "parent": "<class __main__.C>",
        "children": [
            {
                "parent": "<instance of __main__.C>",
                "children": [
                    "<instance method __main__.C.f2>",
                    "<instance property __main__.C.f1>"
                ]
            },
            {
                "parent": "<instance of __main__.C>",
                "children": [
                    "<instance method __main__.C.f2>",
                    "<instance property __main__.C.f1>"
                ]
            }
        ]
    }
]

>>> print(vbt.prettify(A.f1.get_ca_setup().get_setup_hierarchy()))
[
    "<instance property __main__.C.f1>",
    "<instance property __main__.C.f1>",
    "<instance property __main__.B.f1>"
]

>>> print(vbt.prettify(C.f2.get_ca_setup().get_setup_hierarchy()))
[
    "<instance method __main__.C.f2>",
    "<instance method __main__.C.f2>"
]
```

Let's disable caching for the entire `A` class:

```pycon
>>> A.get_ca_setup().disable_caching()
>>> A.get_ca_setup().use_cache
False
>>> B.get_ca_setup().use_cache
False
>>> C.get_ca_setup().use_cache
False
```

This disabled caching for `A`, subclasses `B` and `C`, their instances, and any instance function.
But it didn't touch unbound functions such as `C.f1` and `C.f2`:

```pycon
>>> C.f1.get_ca_setup().use_cache
True
>>> C.f2.get_ca_setup().use_cache
True
```

This is because unbound functions are not children of the classes they are declared in!
Still, any future instance method of `C` won't be cached because it looks which parent
has been updated more recently: the class or the unbound function. In our case,
the class had a more recent update.

```pycon
>>> c3 = C()
>>> C.f2.get_ca_setup(c3).use_cache
False
```

In fact, if we want to disable an entire class but leave one function untouched,
we need to perform two operations in a particular order: 1) disable caching on the class
and 2) enable caching on the unbound function.

```pycon
>>> A.get_ca_setup().disable_caching()
>>> C.f2.get_ca_setup().enable_caching()

>>> c4 = C()
>>> C.f2.get_ca_setup(c4).use_cache
True
```

## Getting statistics

The main advantage of having a central registry of setups is that we can easily find any setup
registered in any part of vectorbt that matches some condition using `CacheableRegistry.match_setups`.

!!! note
    By default, all setups are registered lazily - no setup is registered until it's run
    or explicitly called. To change this behavior, set `register_lazily` in the global
    settings to False.

For example, let's look which setups have been registered so far:

```pycon
>>> vbt.ca_reg.match_setups(kind=None)
{
    CAClassSetup(registry=<vectorbtpro.registries.ca_registry.CacheableRegistry object at 0x7fe14c27df60>, use_cache=None, whitelist=None, cls=<class '__main__.B'>),
    CAClassSetup(registry=<vectorbtpro.registries.ca_registry.CacheableRegistry object at 0x7fe14c27df60>, use_cache=None, whitelist=None, cls=<class '__main__.C'>),
    CAInstanceSetup(registry=<vectorbtpro.registries.ca_registry.CacheableRegistry object at 0x7fe14c27df60>, use_cache=None, whitelist=None, instance=<weakref at 0x7fe14e9d83b8; to 'B' at 0x7fe14e944978>),
    CAInstanceSetup(registry=<vectorbtpro.registries.ca_registry.CacheableRegistry object at 0x7fe14c27df60>, use_cache=None, whitelist=None, instance=<weakref at 0x7fe14e9d84f8; to 'C' at 0x7fe14e9448d0>),
    CAInstanceSetup(registry=<vectorbtpro.registries.ca_registry.CacheableRegistry object at 0x7fe14c27df60>, use_cache=None, whitelist=None, instance=<weakref at 0x7fe14e9d8688; to 'C' at 0x7fe1495111d0>),
    CARunSetup(registry=<vectorbtpro.registries.ca_registry.CacheableRegistry object at 0x7fe14c27df60>, use_cache=True, whitelist=False, cacheable=<function <lambda> at 0x7fe14e94cae8>, instance=None, max_size=None, ignore_args=None, cache={}),
    CARunSetup(registry=<vectorbtpro.registries.ca_registry.CacheableRegistry object at 0x7fe14c27df60>, use_cache=True, whitelist=False, cacheable=<function C.f2 at 0x7fe13959ee18>, instance=<weakref at 0x7fe14e9d85e8; to 'C' at 0x7fe14e9448d0>, max_size=None, ignore_args=None, cache={}),
    CARunSetup(registry=<vectorbtpro.registries.ca_registry.CacheableRegistry object at 0x7fe14c27df60>, use_cache=True, whitelist=False, cacheable=<function C.f2 at 0x7fe13959ee18>, instance=<weakref at 0x7fe14e9d8728; to 'C' at 0x7fe1495111d0>, max_size=None, ignore_args=None, cache={}),
    CARunSetup(registry=<vectorbtpro.registries.ca_registry.CacheableRegistry object at 0x7fe14c27df60>, use_cache=True, whitelist=False, cacheable=<vectorbtpro.utils.decorators.cached_property object at 0x7fe118045408>, instance=<weakref at 0x7fe14e9d8458; to 'B' at 0x7fe14e944978>, max_size=None, ignore_args=None, cache={}),
    CARunSetup(registry=<vectorbtpro.registries.ca_registry.CacheableRegistry object at 0x7fe14c27df60>, use_cache=True, whitelist=False, cacheable=<vectorbtpro.utils.decorators.cached_property object at 0x7fe118045408>, instance=<weakref at 0x7fe14e9d8598; to 'C' at 0x7fe14e9448d0>, max_size=None, ignore_args=None, cache={}),
    CARunSetup(registry=<vectorbtpro.registries.ca_registry.CacheableRegistry object at 0x7fe14c27df60>, use_cache=True, whitelist=False, cacheable=<vectorbtpro.utils.decorators.cached_property object at 0x7fe118045408>, instance=<weakref at 0x7fe14e9d86d8; to 'C' at 0x7fe1495111d0>, max_size=None, ignore_args=None, cache={}),
    CAUnboundSetup(registry=<vectorbtpro.registries.ca_registry.CacheableRegistry object at 0x7fe14c27df60>, use_cache=True, whitelist=False, cacheable=<function C.f2 at 0x7fe13959ee18>),
    CAUnboundSetup(registry=<vectorbtpro.registries.ca_registry.CacheableRegistry object at 0x7fe14c27df60>, use_cache=True, whitelist=False, cacheable=<vectorbtpro.utils.decorators.cached_property object at 0x7fe118045408>)
}
```

Let's get the runnable setup of any property and method called `f2`:

```pycon
>>> vbt.ca_reg.match_setups('f2', kind='runnable')
{
    CARunSetup(registry=<vectorbtpro.registries.ca_registry.CacheableRegistry object at 0x7fe14c27df60>, use_cache=True, whitelist=False, cacheable=<function C.f2 at 0x7fe13959ee18>, instance=<weakref at 0x7fe14e9d85e8; to 'C' at 0x7fe14e9448d0>, max_size=None, ignore_args=None, cache={}),
    CARunSetup(registry=<vectorbtpro.registries.ca_registry.CacheableRegistry object at 0x7fe14c27df60>, use_cache=True, whitelist=False, cacheable=<function C.f2 at 0x7fe13959ee18>, instance=<weakref at 0x7fe14e9d8728; to 'C' at 0x7fe1495111d0>, max_size=None, ignore_args=None, cache={})
}
```

But there is a better way to get the stats: `CAQueryDelegator.get_stats`.
It returns a DataFrame with setup stats as rows:

```pycon
>>> vbt.CAQueryDelegator('f2', kind='runnable').get_stats()
                                               string  use_cache  whitelist  \
hash
 3506416602224216137  <instance method __main__.C.f2>       True      False
-4747092115268118855  <instance method __main__.C.f2>       True      False
-4748466030718995055  <instance method __main__.C.f2>       True      False

                      caching_enabled  hits  misses total_size total_elapsed  \
hash
 3506416602224216137             True     0       0    0 Bytes          None
-4747092115268118855             True     0       0    0 Bytes          None
-4748466030718995055             True     0       0    0 Bytes          None

                     total_saved first_run_time last_run_time first_hit_time  \
hash
 3506416602224216137        None           None          None           None
-4747092115268118855        None           None          None           None
-4748466030718995055        None           None          None           None

                     last_hit_time  creation_time last_update_time
hash
 3506416602224216137          None  9 minutes ago    9 minutes ago
-4747092115268118855          None  9 minutes ago    9 minutes ago
-4748466030718995055          None  9 minutes ago    9 minutes ago
```

## Clearing up

Instance and runnable setups hold only weak references to their instances such that
deleting those instances won't keep them in memory and will automatically remove the setups.

To clear all caches:

```pycon
>>> vbt.CAQueryDelegator().clear_cache()
```

## Resetting

To reset global caching flags:

```pycon
>>> vbt.settings.caching.reset()
```

To remove all setups:

```pycon
>>> vbt.CAQueryDelegator(kind=None).deregister()
```
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Função: is_cacheable_function
```
Check if `cacheable` is a cacheable function.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Função: is_cacheable_property
```
Check if `cacheable` is a cacheable property.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Função: is_cacheable_method
```
Check if `cacheable` is a cacheable method.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Função: is_bindable_cacheable
```
Check if `cacheable` is a cacheable that can be bound to an instance.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Função: is_cacheable
```
Check if `cacheable` is a cacheable.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Função: get_obj_id
```
Get id of an instance.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Função: _instance_converter
```
Make the reference to the instance weak.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAQuery
```
Data class that represents a query for matching and ranking setups.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARule
```
Data class that represents a rule that should be enforced on setups that match a query.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CacheableRegistry
```
Class for registering setups of cacheables.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAMetrics
```
Abstract class that exposes various metrics related to caching.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
```
Base class that exposes properties and methods for cache management.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CASetupDelegatorMixin
```
Mixin class that delegates cache management to child setups.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseDelegatorSetup
```
Base class acting as a stateful setup that delegates cache management to child setups.

First delegates the work and only then changes its own state.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Função: _assert_value_not_none
```
Assert that value is not None.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAClassSetup
```
Class that represents a setup of a cacheable class.

The provided class must subclass `vectorbtpro.utils.caching.Cacheable`.

Delegates cache management to its child subclass setups of type `CAClassSetup` and
child instance setups of type `CAInstanceSetup`.

If `use_cash` or `whitelist` are None, inherits a non-empty value from its superclass setups
using the method resolution order (MRO).

!!! note
    Unbound setups are not children of class setups. See notes on `CAUnboundSetup`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAInstanceSetup
```
Class that represents a setup of an instance that has cacheables bound to it.

The provided instance must be of `vectorbtpro.utils.caching.Cacheable`.

Delegates cache management to its child setups of type `CARunSetup`.

If `use_cash` or `whitelist` are None, inherits a non-empty value from its parent class setup.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAUnboundSetup
```
Class that represents a setup of an unbound cacheable property or method.

An unbound callable is a callable that was declared in a class but is not bound
to any instance (just yet).

!!! note
    Unbound callables are just regular functions - they have no parent setups. Even though they
    are formally declared in a class, there is no easy way to get a reference to the class
    from the decorator itself. Thus, searching for child setups of a specific class won't return
    unbound setups.

Delegates cache management to its child setups of type `CARunSetup`.
One unbound cacheable property or method can be bound to multiple instances, thus there is
one-to-many relationship between `CAUnboundSetup` and `CARunSetup` instances.

!!! hint
    Use class attributes instead of instance attributes to access unbound callables.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARunResult
```
Class that represents a cached result of a run.

!!! note
    Hashed solely by the hash of the arguments `args_hash`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARunSetup
```
Class that represents a runnable cacheable setup.

Takes care of running functions and caching the results using `CARunSetup.run`.

Accepts as `cacheable` either `vectorbtpro.utils.decorators.cacheable_property`,
`vectorbtpro.utils.decorators.cacheable_method`, or `vectorbtpro.utils.decorators.cacheable`.

Hashed by the callable and optionally the id of the instance its bound to.
This way, it can be uniquely identified among all setups.

!!! note
    Cacheable properties and methods must provide an instance.

    Only one instance per each unique combination of `cacheable` and `instance` can exist at a time.

If `use_cash` or `whitelist` are None, inherits a non-empty value either from its parent instance setup
or its parent unbound setup. If both setups have non-empty values, takes the one that has been
updated more recently.

!!! note
    Use `CARunSetup.get` class method instead of `CARunSetup.__init__` to create a setup. The class method
    first checks whether a setup with the same hash has already been registered, and if so, returns it.
    Otherwise, creates and registers a new one. Using `CARunSetup.__init__` will throw an error if there
    is a setup with the same hash.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAQueryDelegator
```
Class that delegates any setups that match a query.

`*args`, `collapse`, and `**kwargs` are passed to `CacheableRegistry.match_setups`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Função: get_cache_stats
```
Get cache stats globally or of an object.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Função: print_cache_stats
```
Print cache stats globally or of an object.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Função: clear_cache
```
Clear cache globally or of an object.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Função: collect_garbage
```
Collect garbage.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Função: flush
```
Clear cache and collect garbage.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Função: disable_caching
```
Disable caching globally.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Função: enable_caching
```
Enable caching globally.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingDisabled
```
Context manager to disable caching.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Função: with_caching_disabled
```
Decorator to run a function with `CachingDisabled`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingEnabled
```
Context manager to enable caching.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Função: with_caching_enabled
```
Decorator to run a function with `CachingEnabled`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAQuery
#### Função: parse
```
Parse a query-like object.

!!! note
    Not all attribute combinations can be safely parsed by this function.
    For example, you cannot combine cacheable together with options.

Usage:
    ```pycon
    >>> vbt.CAQuery.parse(lambda x: x)
    CAQuery(cacheable=<function <lambda> at 0x7fd4766c7730>, instance=None, cls=None, base_cls=None, options=None)

    >>> vbt.CAQuery.parse("a")
    CAQuery(cacheable='a', instance=None, cls=None, base_cls=None, options=None)

    >>> vbt.CAQuery.parse("A.a")
    CAQuery(cacheable='a', instance=None, cls=None, base_cls='A', options=None)

    >>> vbt.CAQuery.parse("A")
    CAQuery(cacheable=None, instance=None, cls=None, base_cls='A', options=None)

    >>> vbt.CAQuery.parse("A", use_base_cls=False)
    CAQuery(cacheable=None, instance=None, cls='A', base_cls=None, options=None)

    >>> vbt.CAQuery.parse(vbt.Regex("[A-B]"))
    CAQuery(cacheable=None, instance=None, cls=None, base_cls=Regex(pattern='[A-B]', flags=0), options=None)

    >>> vbt.CAQuery.parse(dict(my_option=100))
    CAQuery(cacheable=None, instance=None, cls=None, base_cls=None, options={'my_option': 100})
    ```
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAQuery
#### Função: instance_obj
```
Instance object.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAQuery
#### Função: matches_setup
```
Return whether the setup matches this query.

Usage:
    Let's evaluate various queries:

    ```pycon
    >>> class A(vbt.Cacheable):
    ...     @vbt.cached_method(my_option=True)
    ...     def f(self):
    ...         return None

    >>> class B(A):
    ...     pass

    >>> @vbt.cached(my_option=False)
    ... def f():
    ...     return None

    >>> a = A()
    >>> b = B()

    >>> def match_query(query):
    ...     matched = []
    ...     if query.matches_setup(A.f.get_ca_setup()):  # unbound method
    ...         matched.append('A.f')
    ...     if query.matches_setup(A.get_ca_setup()):  # class
    ...         matched.append('A')
    ...     if query.matches_setup(a.get_ca_setup()):  # instance
    ...         matched.append('a')
    ...     if query.matches_setup(A.f.get_ca_setup(a)):  # instance method
    ...         matched.append('a.f')
    ...     if query.matches_setup(B.f.get_ca_setup()):  # unbound method
    ...         matched.append('B.f')
    ...     if query.matches_setup(B.get_ca_setup()):  # class
    ...         matched.append('B')
    ...     if query.matches_setup(b.get_ca_setup()):  # instance
    ...         matched.append('b')
    ...     if query.matches_setup(B.f.get_ca_setup(b)):  # instance method
    ...         matched.append('b.f')
    ...     if query.matches_setup(f.get_ca_setup()):  # function
    ...         matched.append('f')
    ...     return matched

    >>> match_query(vbt.CAQuery())
    ['A.f', 'A', 'a', 'a.f', 'B.f', 'B', 'b', 'b.f', 'f']
    >>> match_query(vbt.CAQuery(cacheable=A.f))
    ['A.f', 'a.f', 'B.f', 'b.f']
    >>> match_query(vbt.CAQuery(cacheable=B.f))
    ['A.f', 'a.f', 'B.f', 'b.f']
    >>> match_query(vbt.CAQuery(cls=A))
    ['A', 'a', 'a.f']
    >>> match_query(vbt.CAQuery(cls=B))
    ['B', 'b', 'b.f']
    >>> match_query(vbt.CAQuery(cls=vbt.Regex('[A-B]')))
    ['A', 'a', 'a.f', 'B', 'b', 'b.f']
    >>> match_query(vbt.CAQuery(base_cls=A))
    ['A', 'a', 'a.f', 'B', 'b', 'b.f']
    >>> match_query(vbt.CAQuery(base_cls=B))
    ['B', 'b', 'b.f']
    >>> match_query(vbt.CAQuery(instance=a))
    ['a', 'a.f']
    >>> match_query(vbt.CAQuery(instance=b))
    ['b', 'b.f']
    >>> match_query(vbt.CAQuery(instance=a, cacheable='f'))
    ['a.f']
    >>> match_query(vbt.CAQuery(instance=b, cacheable='f'))
    ['b.f']
    >>> match_query(vbt.CAQuery(options=dict(my_option=True)))
    ['A.f', 'a.f', 'B.f', 'b.f']
    >>> match_query(vbt.CAQuery(options=dict(my_option=False)))
    ['f']
    ```
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARule
#### Função: matches_setup
```
Return whether the setup matches the rule.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARule
#### Função: enforce
```
Run `CARule.enforce_func` on the setup if it has been matched.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CacheableRegistry
#### Função: class_setups
```
Dict of registered `CAClassSetup` instances by their hash.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CacheableRegistry
#### Função: instance_setups
```
Dict of registered `CAInstanceSetup` instances by their hash.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CacheableRegistry
#### Função: unbound_setups
```
Dict of registered `CAUnboundSetup` instances by their hash.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CacheableRegistry
#### Função: run_setups
```
Dict of registered `CARunSetup` instances by their hash.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CacheableRegistry
#### Função: setups
```
Dict of registered `CABaseSetup` instances by their hash.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CacheableRegistry
#### Função: rules
```
List of registered `CARule` instances.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CacheableRegistry
#### Função: get_setup_by_hash
```
Get the setup by its hash.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CacheableRegistry
#### Função: setup_registered
```
Return whether the setup is registered.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CacheableRegistry
#### Função: register_setup
```
Register a new setup of type `CABaseSetup`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CacheableRegistry
#### Função: deregister_setup
```
Deregister a new setup of type `CABaseSetup`.

Removes the setup from its respective collection.

To also deregister its children, call the `CASetupDelegatorMixin.deregister` method.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CacheableRegistry
#### Função: register_rule
```
Register a new rule of type `CARule`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CacheableRegistry
#### Função: deregister_rule
```
Deregister a rule of type `CARule`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CacheableRegistry
#### Função: get_run_setup
```
Get a setup of type `CARunSetup` with this cacheable and instance, or return None.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CacheableRegistry
#### Função: get_unbound_setup
```
Get a setup of type `CAUnboundSetup` with this cacheable or return None.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CacheableRegistry
#### Função: get_instance_setup
```
Get a setup of type `CAInstanceSetup` with this instance or return None.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CacheableRegistry
#### Função: get_class_setup
```
Get a setup of type `CAInstanceSetup` with this class or return None.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CacheableRegistry
#### Função: match_setups
```
Match all setups registered in this registry against `query_like`.

`query_like` can be one or more query-like objects that will be parsed using `CAQuery.parse`.

Set `collapse` to True to remove child setups that belong to any matched parent setup.

`kind` can be one or multiple of the following:

* 'class' to only return class setups (instances of `CAClassSetup`)
* 'instance' to only return instance setups (instances of `CAInstanceSetup`)
* 'unbound' to only return unbound setups (instances of `CAUnboundSetup`)
* 'runnable' to only return runnable setups (instances of `CARunSetup`)

Set `exclude` to one or multiple setups to exclude. To not exclude their children,
set `exclude_children` to False.

!!! note
    `exclude_children` is applied only when `collapse` is True.

`filter_func` can be used to filter out setups. For example, `lambda setup: setup.caching_enabled`
includes only those setups that have caching enabled. It must take a setup and return a boolean
of whether to include this setup in the final results.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAMetrics
#### Função: hits
```
Number of hits.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAMetrics
#### Função: misses
```
Number of misses.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAMetrics
#### Função: total_size
```
Total size of cached objects.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAMetrics
#### Função: total_elapsed
```
Total number of seconds elapsed during running the function.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAMetrics
#### Função: total_saved
```
Total number of seconds saved by using the cache.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAMetrics
#### Função: first_run_time
```
Time of the first run.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAMetrics
#### Função: last_run_time
```
Time of the last run.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAMetrics
#### Função: first_hit_time
```
Time of the first hit.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAMetrics
#### Função: last_hit_time
```
Time of the last hit.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAMetrics
#### Função: metrics
```
Dict with all metrics.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: query
```
Query to match this setup.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: caching_enabled
```
Whether caching is enabled in this setup.

Caching is disabled when any of the following apply:

* `CARunSetup.use_cache` is False
* Caching is disabled globally and `CARunSetup.whitelist` is False
* Caching and whitelisting are disabled globally

Returns None if `CABaseSetup.use_cache` or `CABaseSetup.whitelist` is None.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: register
```
Register setup using `CacheableRegistry.register_setup`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: deregister
```
Register setup using `CacheableRegistry.deregister_setup`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: registered
```
Return whether setup is registered.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: enforce_rules
```
Enforce registry rules.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: activate
```
Activate.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: deactivate
```
Deactivate.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: enable_whitelist
```
Enable whitelisting.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: disable_whitelist
```
Disable whitelisting.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: enable_caching
```
Enable caching.

Set `force` to True to whitelist this setup.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: disable_caching
```
Disable caching.

Set `clear_cache` to True to also clear the cache.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: creation_time
```
Time when this setup was created.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: use_cache_lut
```
Last time `CABaseSetup.use_cache` was updated.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: whitelist_lut
```
Last time `CABaseSetup.whitelist` was updated.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: last_update_time
```
Last time any of `CABaseSetup.use_cache` and `CABaseSetup.whitelist` were updated.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: clear_cache
```
Clear the cache.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: short_str
```
Convert this setup into a short string.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: readable_name
```
Get a readable name of the object the setup is bound to.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: position_among_similar
```
Get position among all similar setups.

Ordered by creation time.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: readable_str
```
Convert this setup into a readable string.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseSetup
#### Função: get_stats
```
Get stats of the setup as a dict with metrics.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CASetupDelegatorMixin
#### Função: child_setups
```
Child setups.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CASetupDelegatorMixin
#### Função: get_setup_hierarchy
```
Get the setup hierarchy by recursively traversing the child setups.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CASetupDelegatorMixin
#### Função: delegate
```
Delegate a function to all child setups.

`func` must take the setup and return nothing. If the setup is an instance of
`CASetupDelegatorMixin`, it must additionally accept `exclude`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CASetupDelegatorMixin
#### Função: deregister
```
Calls `CABaseSetup.deregister` on each child setup.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CASetupDelegatorMixin
#### Função: enable_whitelist
```
Calls `CABaseSetup.enable_whitelist` on each child setup.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CASetupDelegatorMixin
#### Função: disable_whitelist
```
Calls `CABaseSetup.disable_whitelist` on each child setup.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CASetupDelegatorMixin
#### Função: enable_caching
```
Calls `CABaseSetup.enable_caching` on each child setup.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CASetupDelegatorMixin
#### Função: disable_caching
```
Calls `CABaseSetup.disable_caching` on each child setup.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CASetupDelegatorMixin
#### Função: clear_cache
```
Calls `CABaseSetup.clear_cache` on each child setup.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CASetupDelegatorMixin
#### Função: get_stats
```
Get a DataFrame out of stats dicts of child setups.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CABaseDelegatorSetup
#### Função: child_setups
```
Get child setups that match `CABaseDelegatorSetup.query`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAClassSetup
#### Função: get_cacheable_superclasses
```
Get an ordered list of the cacheable superclasses of a class.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAClassSetup
#### Função: get_superclass_setups
```
Setups of type `CAClassSetup` of each in `CAClassSetup.get_cacheable_superclasses`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAClassSetup
#### Função: get_cacheable_subclasses
```
Get an ordered list of the cacheable subclasses of a class.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAClassSetup
#### Função: get_subclass_setups
```
Setups of type `CAClassSetup` of each in `CAClassSetup.get_cacheable_subclasses`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAClassSetup
#### Função: get_unbound_cacheables
```
Get a set of the unbound cacheables of a class.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAClassSetup
#### Função: get_unbound_setups
```
Setups of type `CAUnboundSetup` of each in `CAClassSetup.get_unbound_cacheables`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAClassSetup
#### Função: get
```
Get setup from `CacheableRegistry` or register a new one.

`**kwargs` are passed to `CAClassSetup.__init__`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAClassSetup
#### Função: superclass_setups
```
See `CAClassSetup.get_superclass_setups`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAClassSetup
#### Função: subclass_setups
```
See `CAClassSetup.get_subclass_setups`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAClassSetup
#### Função: unbound_setups
```
See `CAClassSetup.get_unbound_setups`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAClassSetup
#### Função: instance_setups
```
Setups of type `CAInstanceSetup` of instances of the class.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAClassSetup
#### Função: any_use_cache_lut
```
Last time `CABaseSetup.use_cache` was updated in this class or any of its superclasses.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAClassSetup
#### Função: any_whitelist_lut
```
Last time `CABaseSetup.whitelist` was updated in this class or any of its superclasses.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAInstanceSetup
#### Função: get
```
Get setup from `CacheableRegistry` or register a new one.

`**kwargs` are passed to `CAInstanceSetup.__init__`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAInstanceSetup
#### Função: instance_obj
```
Instance object.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAInstanceSetup
#### Função: contains_garbage
```
Whether instance was destroyed.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAInstanceSetup
#### Função: class_setup
```
Setup of type `CAClassSetup` of the cacheable class of the instance.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAInstanceSetup
#### Função: unbound_setups
```
Setups of type `CAUnboundSetup` of unbound cacheables declared in the class of the instance.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAInstanceSetup
#### Função: run_setups
```
Setups of type `CARunSetup` of cacheables bound to the instance.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAUnboundSetup
#### Função: get
```
Get setup from `CacheableRegistry` or register a new one.

`**kwargs` are passed to `CAUnboundSetup.__init__`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAUnboundSetup
#### Função: run_setups
```
Setups of type `CARunSetup` of bound cacheables.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARunResult
#### Função: result_size
```
Get size of the result in memory.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARunResult
#### Função: run_time
```
Time of the run.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARunResult
#### Função: first_hit_time
```
Time of the first hit.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARunResult
#### Função: last_hit_time
```
Time of the last hit.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARunResult
#### Função: hit
```
Hit the result.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARunSetup
#### Função: get
```
Get setup from `CacheableRegistry` or register a new one.

`**kwargs` are passed to `CARunSetup.__init__`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARunSetup
#### Função: instance_obj
```
Instance object.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARunSetup
#### Função: contains_garbage
```
Whether instance was destroyed.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARunSetup
#### Função: instance_setup
```
Setup of type `CAInstanceSetup` of the instance this cacheable is bound to.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARunSetup
#### Função: unbound_setup
```
Setup of type `CAUnboundSetup` of the unbound cacheable.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARunSetup
#### Função: run_func
```
Run the setup's function without caching.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARunSetup
#### Função: get_args_hash
```
Get the hash of the passed arguments.

`CARunSetup.ignore_args` gets extended with `ignore_args` under `vectorbtpro._settings.caching`.

If no arguments were passed, hashes None.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARunSetup
#### Função: run_func_and_cache
```
Run the setup's function and cache the result.

Hashes the arguments using `CARunSetup.get_args_hash`, runs the function using
`CARunSetup.run_func`, wraps the result using `CARunResult`, and uses the hash
as a key to store the instance of `CARunResult` into `CARunSetup.cache` for later retrieval.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARunSetup
#### Função: run
```
Run the setup and cache it depending on a range of conditions.

Runs `CARunSetup.run_func` if caching is disabled or arguments are not hashable,
and `CARunSetup.run_func_and_cache` otherwise.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CARunSetup
#### Função: clear_cache
```
Clear the cache.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAQueryDelegator
#### Função: args
```
Arguments.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAQueryDelegator
#### Função: kwargs
```
Keyword arguments.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAQueryDelegator
#### Função: registry
```
Registry of type `CacheableRegistry`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CAQueryDelegator
#### Função: child_setups
```
Get child setups by matching them using `CacheableRegistry.match_setups`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingDisabled
#### Função: query_like
```
See `CAQuery.parse`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingDisabled
#### Função: use_base_cls
```
See `CAQuery.parse`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingDisabled
#### Função: kind
```
See `CARule.kind`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingDisabled
#### Função: exclude
```
See `CARule.exclude`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingDisabled
#### Função: filter_func
```
See `CARule.filter_func`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingDisabled
#### Função: registry
```
Registry of type `CacheableRegistry`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingDisabled
#### Função: disable_whitelist
```
Whether to disable whitelist.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingDisabled
#### Função: disable_machinery
```
Whether to disable machinery.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingDisabled
#### Função: clear_cache
```
Whether to clear global cache when entering or local cache when disabling caching.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingDisabled
#### Função: silence_warnings
```
Whether to silence warnings.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingDisabled
#### Função: rule
```
Rule.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingDisabled
#### Função: init_settings
```
Initial caching settings.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingDisabled
#### Função: init_setup_settings
```
Initial setup settings.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingEnabled
#### Função: query_like
```
See `CAQuery.parse`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingEnabled
#### Função: use_base_cls
```
See `CAQuery.parse`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingEnabled
#### Função: kind
```
See `CARule.kind`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingEnabled
#### Função: exclude
```
See `CARule.exclude`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingEnabled
#### Função: filter_func
```
See `CARule.filter_func`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingEnabled
#### Função: registry
```
Registry of type `CacheableRegistry`.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingEnabled
#### Função: enable_whitelist
```
Whether to enable whitelist.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingEnabled
#### Função: enable_machinery
```
Whether to enable machinery.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingEnabled
#### Função: clear_cache
```
Whether to clear global cache when exiting or local cache when disabling caching.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingEnabled
#### Função: silence_warnings
```
Whether to silence warnings.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingEnabled
#### Função: rule
```
Rule.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingEnabled
#### Função: init_settings
```
Initial caching settings.
```

---

# Pasta: registries
### Arquivo: ca_registry.py
#### Classe: CachingEnabled
#### Função: init_setup_settings
```
Initial setup settings.
```

---

# Pasta: registries
### Arquivo: ch_registry.py
#### Docstring do Módulo
```
Global registry for chunkables.
```

---

# Pasta: registries
### Arquivo: ch_registry.py
#### Classe: ChunkedSetup
```
Class that represents a chunkable setup.

!!! note
    Hashed solely by `setup_id`.
```

---

# Pasta: registries
### Arquivo: ch_registry.py
#### Classe: ChunkableRegistry
```
Class for registering chunkable functions.
```

---

# Pasta: registries
### Arquivo: ch_registry.py
#### Função: register_chunkable
```
Register a new chunkable function.

If `return_wrapped` is True, wraps with the `vectorbtpro.utils.chunking.chunked` decorator.
Otherwise, leaves the function as-is (preferred).

Options are merged in the following order:

* `options` in `vectorbtpro._settings.chunking`
* `setup_options.{setup_id}` in `vectorbtpro._settings.chunking`
* `options`
* `override_options` in `vectorbtpro._settings.chunking`
* `override_setup_options.{setup_id}` in `vectorbtpro._settings.chunking`

!!! note
    Calling the `register_chunkable` decorator before (or below) the `vectorbtpro.registries.jit_registry.register_jitted`
    decorator with `return_wrapped` set to True won't work. Doing the same after (or above)
    `vectorbtpro.registries.jit_registry.register_jitted` will work for calling the function from Python but not from Numba.
    Generally, avoid wrapping right away and use `ChunkableRegistry.decorate` to perform decoration.
```

---

# Pasta: registries
### Arquivo: ch_registry.py
#### Classe: ChunkableRegistry
#### Função: setups
```
Dict of registered `ChunkedSetup` instances by `ChunkedSetup.setup_id`.
```

---

# Pasta: registries
### Arquivo: ch_registry.py
#### Classe: ChunkableRegistry
#### Função: register
```
Register a new setup.
```

---

# Pasta: registries
### Arquivo: ch_registry.py
#### Classe: ChunkableRegistry
#### Função: match_setups
```
Match setups against an expression with each setup being a context.
```

---

# Pasta: registries
### Arquivo: ch_registry.py
#### Classe: ChunkableRegistry
#### Função: get_setup
```
Get setup by its id or function.

`setup_id_or_func` can be an identifier or a function.
If it's a function, will build the identifier using its module and name.
```

---

# Pasta: registries
### Arquivo: ch_registry.py
#### Classe: ChunkableRegistry
#### Função: decorate
```
Decorate the setup's function using the `vectorbtpro.utils.chunking.chunked` decorator.

Finds setup using `ChunkableRegistry.get_setup`.

Merges setup's options with `options`.

Specify `target_func` to apply the found setup on another function.
```

---

# Pasta: registries
### Arquivo: ch_registry.py
#### Classe: ChunkableRegistry
#### Função: resolve_option
```
Same as `ChunkableRegistry.decorate` but using `vectorbtpro.utils.chunking.resolve_chunked`.
```

---

# Pasta: registries
### Arquivo: jit_registry.py
#### Docstring do Módulo
```
Global registry for jittables.

Jitting is a process of just-in-time compiling functions to make their execution faster.
A jitter is a decorator that wraps a regular Python function and returns the decorated function.
Depending upon a jitter, this decorated function has the same or at least a similar signature
to the function that has been decorated. Jitters take various jitter-specific options
to change the behavior of execution; that is, a single regular Python function can be
decorated by multiple jitter instances (for example, one jitter for decorating a function
with `numba.jit` and another jitter for doing the same with `parallel=True` flag).

In addition to jitters, vectorbt introduces the concept of tasks. One task can be
executed by multiple jitter types (such as NumPy, Numba, and JAX). For example, one
can create a task that converts price into returns and implements it using NumPy and Numba.
Those implementations are registered by `JITRegistry` as `JitableSetup` instances, are stored
in `JITRegistry.jitable_setups`, and can be uniquely identified by the task id and jitter type.
Note that `JitableSetup` instances contain only information on how to decorate a function.

The decorated function itself and the jitter that has been used are registered as a `JittedSetup`
instance and stored in `JITRegistry.jitted_setups`. It acts as a cache to quickly retrieve an
already decorated function and to avoid recompilation.

Let's implement a task that takes a sum over an array using both NumPy and Numba:

```pycon
>>> from vectorbtpro import *

>>> @vbt.register_jitted(task_id_or_func='sum')
... def sum_np(a):
...     return a.sum()

>>> @vbt.register_jitted(task_id_or_func='sum')
... def sum_nb(a):
...     out = 0.
...     for i in range(a.shape[0]):
...         out += a[i]
...     return out
```

We can see that two new jitable setups were registered:

```pycon
>>> vbt.jit_reg.jitable_setups['sum']
{'np': JitableSetup(task_id='sum', jitter_id='np', py_func=<function sum_np at 0x7fea215b1e18>, jitter_kwargs={}, tags=None),
 'nb': JitableSetup(task_id='sum', jitter_id='nb', py_func=<function sum_nb at 0x7fea273d41e0>, jitter_kwargs={}, tags=None)}
```

Moreover, two jitted setups were registered for our decorated functions:

```pycon
>>> from vectorbtpro.registries.jit_registry import JitableSetup

>>> hash_np = JitableSetup.get_hash('sum', 'np')
>>> vbt.jit_reg.jitted_setups[hash_np]
{3527539: JittedSetup(jitter=<vectorbtpro.utils.jitting.NumPyJitter object at 0x7fea21506080>, jitted_func=<function sum_np at 0x7fea215b1e18>)}

>>> hash_nb = JitableSetup.get_hash('sum', 'nb')
>>> vbt.jit_reg.jitted_setups[hash_nb]
{6326224984503844995: JittedSetup(jitter=<vectorbtpro.utils.jitting.NumbaJitter object at 0x7fea214d0ba8>, jitted_func=CPUDispatcher(<function sum_nb at 0x7fea273d41e0>))}
```

These setups contain decorated functions with the options passed during the registration.
When we call `JITRegistry.resolve` without any additional keyword arguments,
`JITRegistry` returns exactly these functions:

```pycon
>>> jitted_func = vbt.jit_reg.resolve('sum', jitter='nb')
>>> jitted_func
CPUDispatcher(<function sum_nb at 0x7fea273d41e0>)

>>> jitted_func.targetoptions
{'nopython': True, 'nogil': True, 'parallel': False, 'boundscheck': False}
```

Once we pass any other option, the Python function will be redecorated, and another `JittedOption`
instance will be registered:

```pycon
>>> jitted_func = vbt.jit_reg.resolve('sum', jitter='nb', nopython=False)
>>> jitted_func
CPUDispatcher(<function sum_nb at 0x7fea273d41e0>)

>>> jitted_func.targetoptions
{'nopython': False, 'nogil': True, 'parallel': False, 'boundscheck': False}

>>> vbt.jit_reg.jitted_setups[hash_nb]
{6326224984503844995: JittedSetup(jitter=<vectorbtpro.utils.jitting.NumbaJitter object at 0x7fea214d0ba8>, jitted_func=CPUDispatcher(<function sum_nb at 0x7fea273d41e0>)),
 -2979374923679407948: JittedSetup(jitter=<vectorbtpro.utils.jitting.NumbaJitter object at 0x7fea00bf94e0>, jitted_func=CPUDispatcher(<function sum_nb at 0x7fea273d41e0>))}
```

## Templates

Templates can be used to, based on the current context, dynamically select the jitter or
keyword arguments for jitting. For example, let's pick the NumPy jitter over any other
jitter if there are more than two of them for a given task:

```pycon
>>> vbt.jit_reg.resolve('sum', jitter=vbt.RepEval("'nb' if 'nb' in task_setups else None"))
CPUDispatcher(<function sum_nb at 0x7fea273d41e0>)
```

## Disabling

In the case we want to disable jitting, we can simply pass `disable=True` to `JITRegistry.resolve`:

```pycon
>>> py_func = vbt.jit_reg.resolve('sum', jitter='nb', disable=True)
>>> py_func
<function __main__.sum_nb(a)>
```

We can also disable jitting globally:

```pycon
>>> vbt.settings.jitting['disable'] = True

>>> vbt.jit_reg.resolve('sum', jitter='nb')
<function __main__.sum_nb(a)>
```

!!! hint
    If we don't plan to use any additional options and we have only one jitter registered per task,
    we can also disable resolution to increase performance.

!!! warning
    Disabling jitting globally only applies to functions resolved using `JITRegistry.resolve`.
    Any decorated function that is being called directly will be executed as usual.

## Jitted option

Since most functions that call other jitted functions in vectorbt have a `jitted` argument,
you can pass `jitted` as a dictionary with options, as a string denoting the jitter, or False
to disable jitting (see `vectorbtpro.utils.jitting.resolve_jitted_option`):

```pycon
>>> def sum_arr(arr, jitted=None):
...     func = vbt.jit_reg.resolve_option('sum', jitted)
...     return func(arr)

>>> arr = np.random.uniform(size=1000000)

>>> %timeit sum_arr(arr, jitted='np')
319 µs ± 3.35 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

>>> %timeit sum_arr(arr, jitted='nb')
1.09 ms ± 4.13 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)

>>> %timeit sum_arr(arr, jitted=dict(jitter='nb', disable=True))
133 ms ± 2.32 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

!!! hint
    A good rule of thumb is: whenever a caller function accepts a `jitted` argument,
    the jitted functions it calls are most probably resolved using `JITRegistry.resolve_option`.

## Changing options upon registration

Options are usually specified upon registration using `register_jitted`:

```pycon
>>> from numba import prange

>>> @vbt.register_jitted(parallel=True, tags={'can_parallel'})
... def sum_parallel_nb(a):
...     out = np.empty(a.shape[1])
...     for col in prange(a.shape[1]):
...         total = 0.
...         for i in range(a.shape[0]):
...             total += a[i, col]
...         out[col] = total
...     return out

>>> sum_parallel_nb.targetoptions
{'nopython': True, 'nogil': True, 'parallel': True, 'boundscheck': False}
```

But what if we wanted to change the registration options of vectorbt's own jitable functions,
such as `vectorbtpro.generic.nb.base.diff_nb`? For example, let's disable caching for all Numba functions.

```pycon
>>> vbt.settings.jitting.jitters['nb']['override_options'] = dict(cache=False)
```

Since all functions have already been registered, the above statement has no effect:

```pycon
>>> vbt.jit_reg.jitable_setups['vectorbtpro.generic.nb.base.diff_nb']['nb'].jitter_kwargs
{'cache': True}
```

In order for them to be applied, we need to save the settings to a file and
load them before all functions are imported:

```pycon
>>> vbt.settings.save('my_settings')
```

Let's restart the runtime and instruct vectorbt to load the file with settings before anything else:

```pycon
>>> import os
>>> os.environ['VBT_SETTINGS_PATH'] = "my_settings"

>>> from vectorbtpro import *
>>> vbt.jit_reg.jitable_setups['vectorbtpro.generic.nb.base.diff_nb']['nb'].jitter_kwargs
{'cache': False}
```

We can also change the registration options for some specific tasks, and even replace Python functions.
For example, we can change the implementation in the deepest places of the core.
Let's change the default `ddof` from 0 to 1 in `vectorbtpro.generic.nb.base.nanstd_1d_nb` and disable caching with Numba:

```pycon
>>> vbt.nb.nanstd_1d_nb(np.array([1, 2, 3]))
0.816496580927726

>>> def new_nanstd_1d_nb(arr, ddof=1):
...     return np.sqrt(vbt.nb.nanvar_1d_nb(arr, ddof=ddof))

>>> vbt.settings.jitting.jitters['nb']['tasks']['vectorbtpro.generic.nb.base.nanstd_1d_nb'] = dict(
...     replace_py_func=new_nanstd_1d_nb,
...     override_options=dict(
...         cache=False
...     )
... )

>>> vbt.settings.save('my_settings')
```

After restarting the runtime:

```pycon
>>> import os
>>> os.environ['VBT_SETTINGS_PATH'] = "my_settings"

>>> vbt.nb.nanstd_1d_nb(np.array([1, 2, 3]))
1.0
```

!!! note
    All of the above examples require saving the setting to a file, restarting the runtime,
    setting the path to the file to an environment variable, and only then importing vectorbtpro.

## Changing options upon resolution

Another approach but without the need to restart the runtime is by changing the options
upon resolution using `JITRegistry.resolve_option`:

```pycon
>>> # On specific Numba function
>>> vbt.settings.jitting.jitters['nb']['tasks']['vectorbtpro.generic.nb.base.diff_nb'] = dict(
...     resolve_kwargs=dict(
...         nogil=False
...     )
... )

>>> # disabled
>>> vbt.jit_reg.resolve('vectorbtpro.generic.nb.base.diff_nb', jitter='nb').targetoptions
{'nopython': True, 'nogil': False, 'parallel': False, 'boundscheck': False}

>>> # still enabled
>>> vbt.jit_reg.resolve('sum', jitter='nb').targetoptions
{'nopython': True, 'nogil': True, 'parallel': False, 'boundscheck': False}

>>> # On each Numba function
>>> vbt.settings.jitting.jitters['nb']['resolve_kwargs'] = dict(nogil=False)

>>> # disabled
>>> vbt.jit_reg.resolve('vectorbtpro.generic.nb.base.diff_nb', jitter='nb').targetoptions
{'nopython': True, 'nogil': False, 'parallel': False, 'boundscheck': False}

>>> # disabled
>>> vbt.jit_reg.resolve('sum', jitter='nb').targetoptions
{'nopython': True, 'nogil': False, 'parallel': False, 'boundscheck': False}
```

## Building custom jitters

Let's build a custom jitter on top of `vectorbtpro.utils.jitting.NumbaJitter` that converts
any argument that contains a Pandas object to a 2-dimensional NumPy array prior to decoration:

```pycon
>>> from functools import wraps
>>> from vectorbtpro.utils.jitting import NumbaJitter

>>> class SafeNumbaJitter(NumbaJitter):
...     def decorate(self, py_func, tags=None):
...         if self.wrapping_disabled:
...             return py_func
...
...         @wraps(py_func)
...         def wrapper(*args, **kwargs):
...             new_args = ()
...             for arg in args:
...                 if isinstance(arg, pd.Series):
...                     arg = np.expand_dims(arg.values, 1)
...                 elif isinstance(arg, pd.DataFrame):
...                     arg = arg.values
...                 new_args += (arg,)
...             new_kwargs = dict()
...             for k, v in kwargs.items():
...                 if isinstance(v, pd.Series):
...                     v = np.expand_dims(v.values, 1)
...                 elif isinstance(v, pd.DataFrame):
...                     v = v.values
...                 new_kwargs[k] = v
...             return NumbaJitter.decorate(self, py_func, tags=tags)(*new_args, **new_kwargs)
...         return wrapper
```

After we have defined our jitter class, we need to register it globally:

```pycon
>>> vbt.settings.jitting.jitters['safe_nb'] = dict(cls=SafeNumbaJitter)
```

Finally, we can execute any Numba function by specifying our new jitter:

```pycon
>>> func = vbt.jit_reg.resolve(
...     task_id_or_func=vbt.generic.nb.diff_nb,
...     jitter='safe_nb',
...     allow_new=True
... )
>>> func(pd.DataFrame([[1, 2], [3, 4]]))
array([[nan, nan],
       [ 2.,  2.]])
```

Whereas executing the same func using the vanilla Numba jitter causes an error:

```pycon
>>> func = vbt.jit_reg.resolve(task_id_or_func=vbt.generic.nb.diff_nb)
>>> func(pd.DataFrame([[1, 2], [3, 4]]))
Failed in nopython mode pipeline (step: nopython frontend)
non-precise type pyobject
```

!!! note
    Make sure to pass a function as `task_id_or_func` if the jitted function hasn't been registered yet.

    This jitter cannot be used for decorating Numba functions that should be called
    from other Numba functions since the convertion operation is done using Python.
```

---

# Pasta: registries
### Arquivo: jit_registry.py
#### Função: get_func_full_name
```
Get full name of the func to be used as task id.
```

---

# Pasta: registries
### Arquivo: jit_registry.py
#### Classe: JitableSetup
```
Class that represents a jitable setup.

!!! note
    Hashed solely by `task_id` and `jitter_id`.
```

---

# Pasta: registries
### Arquivo: jit_registry.py
#### Classe: JittedSetup
```
Class that represents a jitted setup.

!!! note
    Hashed solely by sorted config of `jitter`. That is, two jitters with the same config
    will yield the same hash and the function won't be re-decorated.
```

---

# Pasta: registries
### Arquivo: jit_registry.py
#### Classe: JITRegistry
```
Class for registering jitted functions.
```

---

# Pasta: registries
### Arquivo: jit_registry.py
#### Função: register_jitted
```
Decorate and register a jitable function using `JITRegistry.decorate_and_register`.

If `task_id_or_func` is a callable, gets replaced by the callable's module name and function name.
Additionally, the function name may contain a suffix pointing at the jitter (such as `_nb`).

Options are merged in the following order:

* `jitters.{jitter_id}.options` in `vectorbtpro._settings.jitting`
* `jitters.{jitter_id}.tasks.{task_id}.options` in `vectorbtpro._settings.jitting`
* `options`
* `jitters.{jitter_id}.override_options` in `vectorbtpro._settings.jitting`
* `jitters.{jitter_id}.tasks.{task_id}.override_options` in `vectorbtpro._settings.jitting`

`py_func` can also be overridden using `jitters.your_jitter.tasks.your_task.replace_py_func`
in `vectorbtpro._settings.jitting`.
```

---

# Pasta: registries
### Arquivo: jit_registry.py
#### Classe: JITRegistry
#### Função: jitable_setups
```
Dict of registered `JitableSetup` instances by `task_id` and `jitter_id`.
```

---

# Pasta: registries
### Arquivo: jit_registry.py
#### Classe: JITRegistry
#### Função: jitted_setups
```
Nested dict of registered `JittedSetup` instances by hash of their `JitableSetup` instance.
```

---

# Pasta: registries
### Arquivo: jit_registry.py
#### Classe: JITRegistry
#### Função: register_jitable_setup
```
Register a jitable setup.
```

---

# Pasta: registries
### Arquivo: jit_registry.py
#### Classe: JITRegistry
#### Função: register_jitted_setup
```
Register a jitted setup.
```

---

# Pasta: registries
### Arquivo: jit_registry.py
#### Classe: JITRegistry
#### Função: decorate_and_register
```
Decorate a jitable function and register both jitable and jitted setups.
```

---

# Pasta: registries
### Arquivo: jit_registry.py
#### Classe: JITRegistry
#### Função: match_jitable_setups
```
Match jitable setups against an expression with each setup being a context.
```

---

# Pasta: registries
### Arquivo: jit_registry.py
#### Classe: JITRegistry
#### Função: match_jitted_setups
```
Match jitted setups of a jitable setup against an expression with each setup a context.
```

---

# Pasta: registries
### Arquivo: jit_registry.py
#### Classe: JITRegistry
#### Função: resolve
```
Resolve jitted function for the given task id.

For details on the format of `task_id_or_func`, see `register_jitted`.

Jitter keyword arguments are merged in the following order:

* `jitable_setup.jitter_kwargs`
* `jitter.your_jitter.resolve_kwargs` in `vectorbtpro._settings.jitting`
* `jitter.your_jitter.tasks.your_task.resolve_kwargs` in `vectorbtpro._settings.jitting`
* `jitter_kwargs`

Templates are substituted in `jitter`, `disable`, and `jitter_kwargs`.

Set `disable` to True to return the Python function without decoration.
If `disable_resolution` is enabled globally, `task_id_or_func` is returned unchanged.

!!! note
    `disable` is only being used by `JITRegistry`, not `vectorbtpro.utils.jitting`.

!!! note
    If there are more than one jitted setups registered for a single task id,
    make sure to provide a jitter.

If no jitted setup of type `JittedSetup` was found and `allow_new` is True,
decorates and returns the function supplied as `task_id_or_func` (otherwise throws an error).

Set `return_missing_task` to True to return `task_id_or_func` if it cannot be found
in `JITRegistry.jitable_setups`.
```

---

# Pasta: registries
### Arquivo: jit_registry.py
#### Classe: JITRegistry
#### Função: resolve_option
```
Resolve `option` using `vectorbtpro.utils.jitting.resolve_jitted_option` and call `JITRegistry.resolve`.
```

---

# Pasta: registries
### Arquivo: pbar_registry.py
#### Docstring do Módulo
```
Global registry for progress bars.
```

---

# Pasta: registries
### Arquivo: pbar_registry.py
#### Classe: PBarRegistry
```
Class for registering `vectorbtpro.utils.pbar.ProgressBar` instances.
```

---

# Pasta: registries
### Arquivo: pbar_registry.py
#### Classe: PBarRegistry
#### Função: generate_bar_id
```
Generate a unique bar id.
```

---

# Pasta: registries
### Arquivo: pbar_registry.py
#### Classe: PBarRegistry
#### Função: instances
```
Dict of registered instances by their bar id.
```

---

# Pasta: registries
### Arquivo: pbar_registry.py
#### Classe: PBarRegistry
#### Função: register_instance
```
Register an instance.
```

---

# Pasta: registries
### Arquivo: pbar_registry.py
#### Classe: PBarRegistry
#### Função: deregister_instance
```
Deregister an instance.
```

---

# Pasta: registries
### Arquivo: pbar_registry.py
#### Classe: PBarRegistry
#### Função: has_conflict
```
Return whether there is an (active) instance with the same bar id.
```

---

# Pasta: registries
### Arquivo: pbar_registry.py
#### Classe: PBarRegistry
#### Função: get_last_active_instance
```
Get the last active instance.
```

---

# Pasta: registries
### Arquivo: pbar_registry.py
#### Classe: PBarRegistry
#### Função: get_first_pending_instance
```
Get the first pending instance.
```

---

# Pasta: registries
### Arquivo: pbar_registry.py
#### Classe: PBarRegistry
#### Função: get_pending_instance
```
Get the pending instance.

If the bar id is not None, searches for the same id in the dictionary.
```

---

# Pasta: registries
### Arquivo: pbar_registry.py
#### Classe: PBarRegistry
#### Função: get_parent_instances
```
Get the (active) parent instances of an instance.
```

---

# Pasta: registries
### Arquivo: pbar_registry.py
#### Classe: PBarRegistry
#### Função: get_parent_instance
```
Get the (active) parent instance of an instance.
```

---

# Pasta: registries
### Arquivo: pbar_registry.py
#### Classe: PBarRegistry
#### Função: get_child_instances
```
Get child (active or pending) instances of an instance.
```

---

# Pasta: registries
### Arquivo: pbar_registry.py
#### Classe: PBarRegistry
#### Função: clear_instances
```
Clear instances.
```

---

# Pasta: registries
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules that register objects across vectorbtpro.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Docstring do Módulo
```
Custom Pandas accessors for returns.

Methods can be accessed as follows:

* `ReturnsSRAccessor` -> `pd.Series.vbt.returns.*`
* `ReturnsDFAccessor` -> `pd.DataFrame.vbt.returns.*`

!!! note
    The underlying Series/DataFrame must already be a return series.
    To convert price to returns, use `ReturnsAccessor.from_value`.

    Grouping is only supported by the methods that accept the `group_by` argument.

    Accessors do not utilize caching.

There are three options to compute returns and get the accessor:

```pycon
>>> from vectorbtpro import *

>>> price = pd.Series([1.1, 1.2, 1.3, 1.2, 1.1])

>>> # 1. pd.Series.pct_change
>>> rets = price.pct_change()
>>> ret_acc = rets.vbt.returns(freq='d')

>>> # 2. vectorbtpro.generic.accessors.GenericAccessor.to_returns
>>> rets = price.vbt.to_returns()
>>> ret_acc = rets.vbt.returns(freq='d')

>>> # 3. vectorbtpro.returns.accessors.ReturnsAccessor.from_value
>>> ret_acc = pd.Series.vbt.returns.from_value(price, freq='d')

>>> # vectorbtpro.returns.accessors.ReturnsAccessor.total
>>> ret_acc.total()
0.0
```

The accessors extend `vectorbtpro.generic.accessors`.

```pycon
>>> # inherited from GenericAccessor
>>> ret_acc.max()
0.09090909090909083
```

## Defaults

`vectorbtpro.returns.accessors.ReturnsAccessor` accepts `defaults` dictionary where you can pass
defaults for arguments used throughout the accessor, such as

* `start_value`: The starting value.
* `window`: Window length.
* `minp`: Minimum number of observations in a window required to have a value.
* `ddof`: Delta Degrees of Freedom.
* `risk_free`: Constant risk-free return throughout the period.
* `levy_alpha`: Scaling relation (Levy stability exponent).
* `required_return`: Minimum acceptance return of the investor.
* `cutoff`: Decimal representing the percentage cutoff for the bottom percentile of returns.
* `period`: Number of observations for annualization. Can be an integer or "dt_period".

Defaults as well as `bm_returns` and `year_freq` can be set globally using settings:

```pycon
>>> benchmark = pd.Series([1.05, 1.1, 1.15, 1.1, 1.05])
>>> bm_returns = benchmark.vbt.to_returns()

>>> vbt.settings.returns['bm_returns'] = bm_returns
```

## Stats

!!! hint
    See `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats` and `ReturnsAccessor.metrics`.

```pycon
>>> ret_acc.stats()
Start                                      0
End                                        4
Duration                     5 days 00:00:00
Total Return [%]                           0
Benchmark Return [%]                       0
Annualized Return [%]                      0
Annualized Volatility [%]            184.643
Sharpe Ratio                        0.691185
Calmar Ratio                               0
Max Drawdown [%]                     15.3846
Omega Ratio                          1.08727
Sortino Ratio                        1.17805
Skew                              0.00151002
Kurtosis                            -5.94737
Tail Ratio                           1.08985
Common Sense Ratio                   1.08985
Value at Risk                     -0.0823718
Alpha                                0.78789
Beta                                 1.83864
dtype: object
```

!!! note
    `ReturnsAccessor.stats` does not support grouping.

## Plots

!!! hint
    See `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots` and `ReturnsAccessor.subplots`.

`ReturnsAccessor` class has a single subplot based on `ReturnsAccessor.plot_cumulative`:

```pycon
>>> ret_acc.plots().show()
```

![](/assets/images/api/returns_plots.light.svg#only-light){: .iimg loading=lazy }
![](/assets/images/api/returns_plots.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
```
Accessor on top of return series. For both, Series and DataFrames.

Accessible via `pd.Series.vbt.returns` and `pd.DataFrame.vbt.returns`.

Args:
    obj (pd.Series or pd.DataFrame): Pandas object representing returns.
    bm_returns (array_like): Pandas object representing benchmark returns.
    log_returns (bool): Whether returns and benchmark returns are provided as log returns.
    year_freq (any): Year frequency for annualization purposes.
    defaults (dict): Defaults that override `defaults` in `vectorbtpro._settings.returns`.
    sim_start (int, datetime_like, or array_like): Simulation start per column.
    sim_end (int, datetime_like, or array_like): Simulation end per column.
    **kwargs: Keyword arguments that are passed down to `vectorbtpro.generic.accessors.GenericAccessor`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsSRAccessor
```
Accessor on top of return series. For Series only.

Accessible via `pd.Series.vbt.returns`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsDFAccessor
```
Accessor on top of return series. For DataFrames only.

Accessible via `pd.DataFrame.vbt.returns`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: from_value
```
Returns a new `ReturnsAccessor` instance with returns calculated from `value`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: resolve_row_stack_kwargs
```
Resolve keyword arguments for initializing `ReturnsAccessor` after stacking along rows.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: resolve_column_stack_kwargs
```
Resolve keyword arguments for initializing `ReturnsAccessor` after stacking along columns.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: sr_accessor_cls
```
Accessor class for `pd.Series`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: df_accessor_cls
```
Accessor class for `pd.DataFrame`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: indexing_func
```
Perform indexing on `ReturnsAccessor`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: bm_returns
```
Benchmark returns.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: get_bm_returns_acc
```
Get accessor for benchmark returns.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: bm_returns_acc
```
`ReturnsAccessor.get_bm_returns_acc` with default arguments.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: log_returns
```
Whether returns and benchmark returns are provided as log returns.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: auto_detect_ann_factor
```
Auto-detect annualization factor from a datetime index.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: parse_ann_factor
```
Parse annualization factor from a datetime index.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: ann_factor_to_year_freq
```
Convert annualization factor into year frequency.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: year_freq_depends_on_index
```
Return whether frequency depends on index.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: get_year_freq
```
Resolve year frequency.

If `year_freq` is "auto", uses `ReturnsAccessor.auto_detect_ann_factor`. If `year_freq`
is "auto_[method_name]`, also applies the method `np.[method_name]` to the annualization factor,
mostly to round it. If `year_freq` is "index_[method_name]", uses `ReturnsAccessor.parse_ann_factor`
to determine the annualization factor by applying the method to `pd.DatetimeIndex.year`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: year_freq
```
Year frequency.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: get_ann_factor
```
Get the annualization factor from the year and data frequency.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: ann_factor
```
Annualization factor.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: get_period
```
Prepare period.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: period
```
Period.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: deannualize
```
Deannualize a value.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: defaults
```
Defaults for `ReturnsAccessor`.

Merges `defaults` from `vectorbtpro._settings.returns` with `defaults`
from `ReturnsAccessor.__init__`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: resample
```
Perform resampling on `ReturnsAccessor`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: resample_returns
```
Resample returns to a custom frequency, date offset, or index.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: daily
```
Daily returns.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: annual
```
Annual returns.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: cumulative
```
Cumulative returns.

See `vectorbtpro.returns.nb.cumulative_returns_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: final_value
```
Final value.

See `vectorbtpro.returns.nb.final_value_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_final_value
```
Rolling final value.

See `vectorbtpro.returns.nb.rolling_final_value_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: total
```
Total return.

See `vectorbtpro.returns.nb.total_return_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_total
```
Rolling total return.

See `vectorbtpro.returns.nb.rolling_total_return_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: annualized
```
Annualized return.

See `vectorbtpro.returns.nb.annualized_return_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_annualized
```
Rolling annualized return.

See `vectorbtpro.returns.nb.rolling_annualized_return_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: annualized_volatility
```
Annualized volatility.

See `vectorbtpro.returns.nb.annualized_volatility_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_annualized_volatility
```
Rolling annualized volatility.

See `vectorbtpro.returns.nb.rolling_annualized_volatility_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: calmar_ratio
```
Calmar ratio.

See `vectorbtpro.returns.nb.calmar_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_calmar_ratio
```
Rolling Calmar ratio.

See `vectorbtpro.returns.nb.rolling_calmar_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: omega_ratio
```
Omega ratio.

See `vectorbtpro.returns.nb.omega_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_omega_ratio
```
Rolling Omega ratio.

See `vectorbtpro.returns.nb.rolling_omega_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: sharpe_ratio
```
Sharpe ratio.

See `vectorbtpro.returns.nb.sharpe_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_sharpe_ratio
```
Rolling Sharpe ratio.

See `vectorbtpro.returns.nb.rolling_sharpe_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: sharpe_ratio_std
```
Standard deviation of the sharpe ratio estimation.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: prob_sharpe_ratio
```
Probabilistic Sharpe Ratio (PSR).
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: deflated_sharpe_ratio
```
Deflated Sharpe Ratio (DSR).

Expresses the chance that the advertised strategy has a positive Sharpe ratio.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: downside_risk
```
Downside risk.

See `vectorbtpro.returns.nb.downside_risk_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_downside_risk
```
Rolling downside risk.

See `vectorbtpro.returns.nb.rolling_downside_risk_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: sortino_ratio
```
Sortino ratio.

See `vectorbtpro.returns.nb.sortino_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_sortino_ratio
```
Rolling Sortino ratio.

See `vectorbtpro.returns.nb.rolling_sortino_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: information_ratio
```
Information ratio.

See `vectorbtpro.returns.nb.information_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_information_ratio
```
Rolling information ratio.

See `vectorbtpro.returns.nb.rolling_information_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: beta
```
Beta.

See `vectorbtpro.returns.nb.beta_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_beta
```
Rolling beta.

See `vectorbtpro.returns.nb.rolling_beta_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: alpha
```
Alpha.

See `vectorbtpro.returns.nb.alpha_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_alpha
```
Rolling alpha.

See `vectorbtpro.returns.nb.rolling_alpha_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: tail_ratio
```
Tail ratio.

See `vectorbtpro.returns.nb.tail_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_tail_ratio
```
Rolling tail ratio.

See `vectorbtpro.returns.nb.rolling_tail_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: profit_factor
```
Profit factor.

See `vectorbtpro.returns.nb.profit_factor_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_profit_factor
```
Rolling profit factor.

See `vectorbtpro.returns.nb.rolling_profit_factor_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: common_sense_ratio
```
Common Sense Ratio (CSR).

See `vectorbtpro.returns.nb.common_sense_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_common_sense_ratio
```
Rolling Common Sense Ratio (CSR).

See `vectorbtpro.returns.nb.rolling_common_sense_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: value_at_risk
```
Value at Risk (VaR).

See `vectorbtpro.returns.nb.value_at_risk_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_value_at_risk
```
Rolling Value at Risk (VaR).

See `vectorbtpro.returns.nb.rolling_value_at_risk_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: cond_value_at_risk
```
Conditional Value at Risk (CVaR).

See `vectorbtpro.returns.nb.cond_value_at_risk_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_cond_value_at_risk
```
Rolling Conditional Value at Risk (CVaR).

See `vectorbtpro.returns.nb.rolling_cond_value_at_risk_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: capture_ratio
```
Capture ratio.

See `vectorbtpro.returns.nb.capture_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_capture_ratio
```
Rolling capture ratio.

See `vectorbtpro.returns.nb.rolling_capture_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: up_capture_ratio
```
Up-market capture ratio.

See `vectorbtpro.returns.nb.up_capture_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_up_capture_ratio
```
Rolling up-market capture ratio.

See `vectorbtpro.returns.nb.rolling_up_capture_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: down_capture_ratio
```
Up-market capture ratio.

See `vectorbtpro.returns.nb.down_capture_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_down_capture_ratio
```
Rolling down-market capture ratio.

See `vectorbtpro.returns.nb.rolling_down_capture_ratio_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: drawdown
```
Relative decline from a peak.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: max_drawdown
```
Maximum Drawdown (MDD).

See `vectorbtpro.returns.nb.max_drawdown_nb`.

Yields the same out as `max_drawdown` of `ReturnsAccessor.drawdowns`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: rolling_max_drawdown
```
Rolling Maximum Drawdown (MDD).

See `vectorbtpro.returns.nb.rolling_max_drawdown_nb`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: drawdowns
```
`ReturnsAccessor.get_drawdowns` with default arguments.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: get_drawdowns
```
Generate drawdown records of cumulative returns.

See `vectorbtpro.generic.drawdowns.Drawdowns`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: qs
```
Quantstats adapter.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: resolve_self
```
Resolve self.

See `vectorbtpro.base.wrapping.Wrapping.resolve_self`.

Creates a copy of this instance `year_freq` is different in `cond_kwargs`.
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: stats_defaults
```
Defaults for `ReturnsAccessor.stats`.

Merges `vectorbtpro.generic.accessors.GenericAccessor.stats_defaults`,
defaults from `ReturnsAccessor.defaults` (acting as `settings`), and
`stats` from `vectorbtpro._settings.returns`
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: plot_cumulative
```
Plot cumulative returns.

Args:
    column (str): Name of the column to plot.
    bm_returns (array_like): Benchmark return to compare returns against.
        Will broadcast per element.
    start_value (float): The starting value.
    sim_start (int, datetime_like, or array_like): Simulation start row or index (inclusive).
    sim_end (int, datetime_like, or array_like): Simulation end row or index (exclusive).
    fit_sim_range (bool): Whether to fit figure to simulation range.
    fill_to_benchmark (bool): Whether to fill between main and benchmark, or between main and `start_value`.
    main_kwargs (dict): Keyword arguments passed to `vectorbtpro.generic.accessors.GenericSRAccessor.plot` for main.
    bm_kwargs (dict): Keyword arguments passed to `vectorbtpro.generic.accessors.GenericSRAccessor.plot` for benchmark.
    pct_scale (bool): Whether to use the percentage scale for the y-axis.
    hline_shape_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Figure.add_shape` for `start_value` line.
    add_trace_kwargs (dict): Keyword arguments passed to `add_trace`.
    xref (str): X coordinate axis.
    yref (str): Y coordinate axis.
    fig (Figure or FigureWidget): Figure to add traces to.
    **layout_kwargs: Keyword arguments for layout.

Usage:
    ```pycon
    >>> np.random.seed(0)
    >>> rets = pd.Series(np.random.uniform(-0.05, 0.05, size=100))
    >>> bm_returns = pd.Series(np.random.uniform(-0.05, 0.05, size=100))
    >>> rets.vbt.returns.plot_cumulative(bm_returns=bm_returns).show()
    ```

    ![](/assets/images/api/plot_cumulative.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/plot_cumulative.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: returns
### Arquivo: accessors.py
#### Classe: ReturnsAccessor
#### Função: plots_defaults
```
Defaults for `ReturnsAccessor.plots`.

Merges `vectorbtpro.generic.accessors.GenericAccessor.plots_defaults`,
defaults from `ReturnsAccessor.defaults` (acting as `settings`), and
`plots` from `vectorbtpro._settings.returns`
```

---

# Pasta: returns
### Arquivo: enums.py
#### Docstring do Módulo
```
Named tuples and enumerated types for returns.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Docstring do Módulo
```
Numba-compiled functions for returns.

Provides an arsenal of Numba-compiled functions that are used by accessors and for measuring
portfolio performance. These only accept NumPy arrays and other Numba-compatible types.

!!! note
    vectorbt treats matrices as first-class citizens and expects input arrays to be
    2-dim, unless function has suffix `_1d` or is meant to be input to another function.
    Data is processed along index (axis 0).

    All functions passed as argument must be Numba-compiled.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: get_return_nb
```
Calculate return from input and output value.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: returns_1d_nb
```
Calculate returns.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: returns_nb
```
2-dim version of `returns_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: cumulative_returns_1d_nb
```
Cumulative returns.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: cumulative_returns_nb
```
2-dim version of `cumulative_returns_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: final_value_1d_nb
```
Final value.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: final_value_nb
```
2-dim version of `final_value_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_final_value_nb
```
Rolling version of `final_value_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: total_return_1d_nb
```
Total return.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: total_return_nb
```
2-dim version of `total_return_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_total_return_nb
```
Rolling version of `total_return_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: annualized_return_1d_nb
```
Annualized total return.

This is equivalent to the compound annual growth rate (CAGR).
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: annualized_return_nb
```
2-dim version of `annualized_return_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_annualized_return_nb
```
Rolling version of `annualized_return_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: annualized_volatility_1d_nb
```
Annualized volatility of a strategy.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: annualized_volatility_nb
```
2-dim version of `annualized_volatility_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_annualized_volatility_nb
```
Rolling version of `annualized_volatility_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: max_drawdown_1d_nb
```
Total maximum drawdown (MDD).
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: max_drawdown_nb
```
2-dim version of `max_drawdown_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_max_drawdown_nb
```
Rolling version of `max_drawdown_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: calmar_ratio_1d_nb
```
Calmar ratio, or drawdown ratio, of a strategy.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: calmar_ratio_nb
```
2-dim version of `calmar_ratio_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_calmar_ratio_nb
```
Rolling version of `calmar_ratio_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: deannualized_return_nb
```
Deannualized return.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: omega_ratio_1d_nb
```
Omega ratio of a strategy.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: omega_ratio_nb
```
2-dim version of `omega_ratio_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_omega_ratio_nb
```
Rolling version of `omega_ratio_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: sharpe_ratio_1d_nb
```
Sharpe ratio of a strategy.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: sharpe_ratio_nb
```
2-dim version of `sharpe_ratio_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_sharpe_ratio_acc_nb
```
Accumulator of `rolling_sharpe_ratio_stream_nb`.

Takes a state of type `vectorbtpro.returns.enums.RollSharpeAIS` and returns
a state of type `vectorbtpro.returns.enums.RollSharpeAOS`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_sharpe_ratio_stream_nb
```
Rolling Sharpe ratio in a streaming fashion.

Uses `rolling_sharpe_ratio_acc_nb` at each iteration.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_sharpe_ratio_nb
```
Rolling version of `sharpe_ratio_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: downside_risk_1d_nb
```
Downside deviation below a threshold.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: downside_risk_nb
```
2-dim version of `downside_risk_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_downside_risk_nb
```
Rolling version of `downside_risk_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: sortino_ratio_1d_nb
```
Sortino ratio of a strategy.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: sortino_ratio_nb
```
2-dim version of `sortino_ratio_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_sortino_ratio_nb
```
Rolling version of `sortino_ratio_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: information_ratio_1d_nb
```
Information ratio of a strategy.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: information_ratio_nb
```
2-dim version of `information_ratio_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_information_ratio_nb
```
Rolling version of `information_ratio_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: beta_1d_nb
```
Beta.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: beta_nb
```
2-dim version of `beta_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_beta_nb
```
Rolling version of `beta_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: alpha_1d_nb
```
Annualized alpha.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: alpha_nb
```
2-dim version of `alpha_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_alpha_nb
```
Rolling version of `alpha_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: tail_ratio_1d_nb
```
Ratio between the right (95%) and left tail (5%).
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: tail_ratio_noarr_1d_nb
```
`tail_ratio_1d_nb` that does not allocate any arrays.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: tail_ratio_nb
```
2-dim version of `tail_ratio_1d_nb` and `tail_ratio_noarr_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_tail_ratio_nb
```
Rolling version of `tail_ratio_1d_nb` and `tail_ratio_noarr_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: profit_factor_1d_nb
```
Profit factor.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: profit_factor_nb
```
2-dim version of `profit_factor_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_profit_factor_nb
```
Rolling version of `profit_factor_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: common_sense_ratio_1d_nb
```
Common Sense Ratio.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: common_sense_ratio_nb
```
2-dim version of `common_sense_ratio_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_common_sense_ratio_nb
```
Rolling version of `common_sense_ratio_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: value_at_risk_1d_nb
```
Value at risk (VaR) of a returns stream.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: value_at_risk_noarr_1d_nb
```
`value_at_risk_1d_nb` that does not allocate any arrays.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: value_at_risk_nb
```
2-dim version of `value_at_risk_1d_nb` and `value_at_risk_noarr_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_value_at_risk_nb
```
Rolling version of `value_at_risk_1d_nb` and `value_at_risk_noarr_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: cond_value_at_risk_1d_nb
```
Conditional value at risk (CVaR) of a returns stream.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: cond_value_at_risk_noarr_1d_nb
```
`cond_value_at_risk_1d_nb` that does not allocate any arrays.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: cond_value_at_risk_nb
```
2-dim version of `cond_value_at_risk_1d_nb` and `cond_value_at_risk_noarr_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_cond_value_at_risk_nb
```
Rolling version of `cond_value_at_risk_1d_nb` and `cond_value_at_risk_noarr_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: capture_ratio_1d_nb
```
Capture ratio.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: capture_ratio_nb
```
2-dim version of `capture_ratio_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_capture_ratio_nb
```
Rolling version of `capture_ratio_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: up_capture_ratio_1d_nb
```
Capture ratio for periods when the benchmark return is positive.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: up_capture_ratio_nb
```
2-dim version of `up_capture_ratio_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_up_capture_ratio_nb
```
Rolling version of `up_capture_ratio_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: down_capture_ratio_1d_nb
```
Capture ratio for periods when the benchmark return is negative.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: down_capture_ratio_nb
```
2-dim version of `down_capture_ratio_1d_nb`.
```

---

# Pasta: returns
### Arquivo: nb.py
#### Função: rolling_down_capture_ratio_nb
```
Rolling version of `down_capture_ratio_1d_nb`.
```

---

# Pasta: returns
### Arquivo: qs_adapter.py
#### Docstring do Módulo
```
Adapter class for QuantStats.

!!! note
    Accessors do not utilize caching.

We can access the adapter from `ReturnsAccessor`:

```pycon
>>> from vectorbtpro import *
>>> import quantstats as qs

>>> np.random.seed(42)
>>> rets = pd.Series(np.random.uniform(-0.1, 0.1, size=(100,)))
>>> bm_returns = pd.Series(np.random.uniform(-0.1, 0.1, size=(100,)))

>>> rets.vbt.returns.qs.r_squared(benchmark=bm_returns)
0.0011582111228735541
```

Which is the same as:

```pycon
>>> qs.stats.r_squared(rets, bm_returns)
```

So why not just using `qs.stats`?

First, we can define all parameters such as benchmark returns once and avoid passing them repeatedly
to every function. Second, vectorbt automatically translates parameters passed to `ReturnsAccessor`
for the use in quantstats.

```pycon
>>> # Defaults that vectorbt understands
>>> ret_acc = rets.vbt.returns(
...     bm_returns=bm_returns,
...     freq='d',
...     year_freq='365d',
...     defaults=dict(risk_free=0.001)
... )

>>> ret_acc.qs.r_squared()
0.0011582111228735541

>>> ret_acc.qs.sharpe()
-1.9158923252075455

>>> # Defaults that only quantstats understands
>>> qs_defaults = dict(
...     benchmark=bm_returns,
...     periods=365,
...     rf=0.001
... )
>>> ret_acc_qs = rets.vbt.returns.qs(defaults=qs_defaults)

>>> ret_acc_qs.r_squared()
0.0011582111228735541

>>> ret_acc_qs.sharpe()
-1.9158923252075455
```

The adapter automatically passes the returns to the particular function.
It also merges the defaults defined in the settings, the defaults passed to `ReturnsAccessor`,
and the defaults passed to `QSAdapter` itself, and matches them with the argument names listed
in the function's signature.

For example, the `periods` argument defaults to the annualization factor
`ReturnsAccessor.ann_factor`, which itself is based on the `freq` argument. This makes the results
produced by quantstats and vectorbt at least somewhat similar.

```pycon
>>> vbt.settings.wrapping['freq'] = 'h'
>>> vbt.settings.returns['year_freq'] = '365d'

>>> rets.vbt.returns.sharpe_ratio()  # ReturnsAccessor
-9.38160953971508

>>> rets.vbt.returns.qs.sharpe()  # quantstats via QSAdapter
-9.38160953971508
```

We can still override any argument by overriding its default or by passing it directly to the function:

```pycon
>>> rets.vbt.returns.qs(defaults=dict(periods=252)).sharpe()
-1.5912029345745982

>>> rets.vbt.returns.qs.sharpe(periods=252)
-1.5912029345745982

>>> qs.stats.sharpe(rets)
-1.5912029345745982
```
```

---

# Pasta: returns
### Arquivo: qs_adapter.py
#### Função: attach_qs_methods
```
Class decorator to attach quantstats methods.
```

---

# Pasta: returns
### Arquivo: qs_adapter.py
#### Classe: QSAdapter
```
Adapter class for quantstats.
```

---

# Pasta: returns
### Arquivo: qs_adapter.py
#### Classe: QSAdapter
#### Função: __call__
```
Allows passing arguments to the initializer.
```

---

# Pasta: returns
### Arquivo: qs_adapter.py
#### Classe: QSAdapter
#### Função: returns_acc
```
Returns accessor.
```

---

# Pasta: returns
### Arquivo: qs_adapter.py
#### Classe: QSAdapter
#### Função: defaults_mapping
```
Common argument names in quantstats mapped to `ReturnsAccessor.defaults`.
```

---

# Pasta: returns
### Arquivo: qs_adapter.py
#### Classe: QSAdapter
#### Função: defaults
```
Defaults for `QSAdapter`.

Merges `defaults` from `vectorbtpro._settings.qs_adapter`, `returns_acc.defaults`
(with adapted naming), and `defaults` from `QSAdapter.__init__`.
```

---

# Pasta: returns
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules for working with returns.

Offers common financial risk and performance metrics as found in [empyrical](https://github.com/quantopian/empyrical),
an adapter for quantstats, and other features based on returns.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Docstring do Módulo
```
Custom Pandas accessors for signals.

Methods can be accessed as follows:

* `SignalsSRAccessor` -> `pd.Series.vbt.signals.*`
* `SignalsDFAccessor` -> `pd.DataFrame.vbt.signals.*`

```pycon
>>> from vectorbtpro import *

>>> # vectorbtpro.signals.accessors.SignalsAccessor.pos_rank
>>> pd.Series([False, True, True, True, False]).vbt.signals.pos_rank()
0   -1
1    0
2    1
3    2
4   -1
dtype: int64
```

The accessors extend `vectorbtpro.generic.accessors`.

!!! note
    The underlying Series/DataFrame must already be a signal series and have boolean data type.

    Grouping is only supported by the methods that accept the `group_by` argument.

    Accessors do not utilize caching.

Run for the examples below:
    
```pycon
>>> mask = pd.DataFrame({
...     'a': [True, False, False, False, False],
...     'b': [True, False, True, False, True],
...     'c': [True, True, True, False, False]
... }, index=pd.date_range("2020", periods=5))
>>> mask
                a      b      c
2020-01-01   True   True   True
2020-01-02  False  False   True
2020-01-03  False   True   True
2020-01-04  False  False  False
2020-01-05  False   True  False
```

## Stats

!!! hint
    See `vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats` and `SignalsAccessor.metrics`.

```pycon
>>> mask.vbt.signals.stats(column='a')
Start                         2020-01-01 00:00:00
End                           2020-01-05 00:00:00
Period                            5 days 00:00:00
Total                                           1
Rate [%]                                     20.0
First Index                   2020-01-01 00:00:00
Last Index                    2020-01-01 00:00:00
Norm Avg Index [-1, 1]                       -1.0
Distance: Min                                 NaT
Distance: Median                              NaT
Distance: Max                                 NaT
Total Partitions                                1
Partition Rate [%]                          100.0
Partition Length: Min             1 days 00:00:00
Partition Length: Median          1 days 00:00:00
Partition Length: Max             1 days 00:00:00
Partition Distance: Min                       NaT
Partition Distance: Median                    NaT
Partition Distance: Max                       NaT
Name: a, dtype: object
```

We can pass another signal array to compare this array with:

```pycon
>>> mask.vbt.signals.stats(column='a', settings=dict(target=mask['b']))

Start                         2020-01-01 00:00:00
End                           2020-01-05 00:00:00
Period                            5 days 00:00:00
Total                                           1
Rate [%]                                     20.0
Total Overlapping                               1
Overlapping Rate [%]                    33.333333
First Index                   2020-01-01 00:00:00
Last Index                    2020-01-01 00:00:00
Norm Avg Index [-1, 1]                       -1.0
Distance -> Target: Min           0 days 00:00:00
Distance -> Target: Median        2 days 00:00:00
Distance -> Target: Max           4 days 00:00:00
Total Partitions                                1
Partition Rate [%]                          100.0
Partition Length: Min             1 days 00:00:00
Partition Length: Median          1 days 00:00:00
Partition Length: Max             1 days 00:00:00
Partition Distance: Min                       NaT
Partition Distance: Median                    NaT
Partition Distance: Max                       NaT
Name: a, dtype: object
```

We can also return duration as a floating number rather than a timedelta:

```pycon
>>> mask.vbt.signals.stats(column='a', settings=dict(to_timedelta=False))
Start                         2020-01-01 00:00:00
End                           2020-01-05 00:00:00
Period                                          5
Total                                           1
Rate [%]                                     20.0
First Index                   2020-01-01 00:00:00
Last Index                    2020-01-01 00:00:00
Norm Avg Index [-1, 1]                       -1.0
Distance: Min                                 NaN
Distance: Median                              NaN
Distance: Max                                 NaN
Total Partitions                                1
Partition Rate [%]                          100.0
Partition Length: Min                         1.0
Partition Length: Median                      1.0
Partition Length: Max                         1.0
Partition Distance: Min                       NaN
Partition Distance: Median                    NaN
Partition Distance: Max                       NaN
Name: a, dtype: object
```

`SignalsAccessor.stats` also supports (re-)grouping:

```pycon
>>> mask.vbt.signals.stats(column=0, group_by=[0, 0, 1])
Start                         2020-01-01 00:00:00
End                           2020-01-05 00:00:00
Period                            5 days 00:00:00
Total                                           4
Rate [%]                                     40.0
First Index                   2020-01-01 00:00:00
Last Index                    2020-01-05 00:00:00
Norm Avg Index [-1, 1]                      -0.25
Distance: Min                     2 days 00:00:00
Distance: Median                  2 days 00:00:00
Distance: Max                     2 days 00:00:00
Total Partitions                                4
Partition Rate [%]                          100.0
Partition Length: Min             1 days 00:00:00
Partition Length: Median          1 days 00:00:00
Partition Length: Max             1 days 00:00:00
Partition Distance: Min           2 days 00:00:00
Partition Distance: Median        2 days 00:00:00
Partition Distance: Max           2 days 00:00:00
Name: 0, dtype: object
```

## Plots

!!! hint
    See `vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots` and `SignalsAccessor.subplots`.

This class inherits subplots from `vectorbtpro.generic.accessors.GenericAccessor`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
```
Accessor on top of signal series. For both, Series and DataFrames.

Accessible via `pd.Series.vbt.signals` and `pd.DataFrame.vbt.signals`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsSRAccessor
```
Accessor on top of signal series. For Series only.

Accessible via `pd.Series.vbt.signals`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsDFAccessor
```
Accessor on top of signal series. For DataFrames only.

Accessible via `pd.DataFrame.vbt.signals`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: sr_accessor_cls
```
Accessor class for `pd.Series`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: df_accessor_cls
```
Accessor class for `pd.DataFrame`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: empty
```
`vectorbtpro.base.accessors.BaseAccessor.empty` with `fill_value=False`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: empty_like
```
`vectorbtpro.base.accessors.BaseAccessor.empty_like` with `fill_value=False`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: generate
```
See `vectorbtpro.signals.nb.generate_nb`.

`shape` can be a shape-like tuple or an instance of `vectorbtpro.base.wrapping.ArrayWrapper`
(will be used as `wrapper`).

Arguments to `place_func_nb` can be passed either as `*args` or `place_args` (but not both!).

Usage:
    * Generate random signals manually:

    ```pycon
    >>> @njit
    ... def place_func_nb(c):
    ...     i = np.random.choice(len(c.out))
    ...     c.out[i] = True
    ...     return i

    >>> vbt.pd_acc.signals.generate(
    ...     (5, 3),
    ...     place_func_nb,
    ...     wrap_kwargs=dict(
    ...         index=mask.index,
    ...         columns=mask.columns
    ...     )
    ... )
                    a      b      c
    2020-01-01   True  False  False
    2020-01-02  False   True  False
    2020-01-03  False  False   True
    2020-01-04  False  False  False
    2020-01-05  False  False  False
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: generate_both
```
See `vectorbtpro.signals.nb.generate_enex_nb`.

`shape` can be a shape-like tuple or an instance of `vectorbtpro.base.wrapping.ArrayWrapper`
(will be used as `wrapper`).

Arguments to `entry_place_func_nb` can be passed either as `*args` or `entry_place_args` while
arguments to `exit_place_func_nb` can be passed either as `*args` or `exit_place_args` (but not both!).

Usage:
    * Generate entry and exit signals one after another:

    ```pycon
    >>> @njit
    ... def place_func_nb(c):
    ...     c.out[0] = True
    ...     return 0

    >>> en, ex = vbt.pd_acc.signals.generate_both(
    ...     (5, 3),
    ...     entry_place_func_nb=place_func_nb,
    ...     exit_place_func_nb=place_func_nb,
    ...     wrap_kwargs=dict(
    ...         index=mask.index,
    ...         columns=mask.columns
    ...     )
    ... )
    >>> en
                    a      b      c
    2020-01-01   True   True   True
    2020-01-02  False  False  False
    2020-01-03   True   True   True
    2020-01-04  False  False  False
    2020-01-05   True   True   True
    >>> ex
                    a      b      c
    2020-01-01  False  False  False
    2020-01-02   True   True   True
    2020-01-03  False  False  False
    2020-01-04   True   True   True
    2020-01-05  False  False  False
    ```

    * Generate three entries and one exit one after another:

    ```pycon
    >>> @njit
    ... def entry_place_func_nb(c, n):
    ...     c.out[:n] = True
    ...     return n - 1

    >>> @njit
    ... def exit_place_func_nb(c, n):
    ...     c.out[:n] = True
    ...     return n - 1

    >>> en, ex = vbt.pd_acc.signals.generate_both(
    ...     (5, 3),
    ...     entry_place_func_nb=entry_place_func_nb,
    ...     entry_place_args=(3,),
    ...     exit_place_func_nb=exit_place_func_nb,
    ...     exit_place_args=(1,),
    ...     wrap_kwargs=dict(
    ...         index=mask.index,
    ...         columns=mask.columns
    ...     )
    ... )
    >>> en
                    a      b      c
    2020-01-01   True   True   True
    2020-01-02   True   True   True
    2020-01-03   True   True   True
    2020-01-04  False  False  False
    2020-01-05   True   True   True
    >>> ex
                    a      b      c
    2020-01-01  False  False  False
    2020-01-02  False  False  False
    2020-01-03  False  False  False
    2020-01-04   True   True   True
    2020-01-05  False  False  False
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: generate_exits
```
See `vectorbtpro.signals.nb.generate_ex_nb`.

Usage:
    * Generate an exit just before the next entry:

    ```pycon
    >>> @njit
    ... def exit_place_func_nb(c):
    ...     c.out[-1] = True
    ...     return len(c.out) - 1

    >>> mask.vbt.signals.generate_exits(exit_place_func_nb)
                    a      b      c
    2020-01-01  False  False  False
    2020-01-02  False   True  False
    2020-01-03  False  False  False
    2020-01-04  False   True  False
    2020-01-05   True  False   True
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: clean
```
Clean signals.

If one array is passed, see `SignalsAccessor.first`. If two arrays passed,
entries and exits, see `vectorbtpro.signals.nb.clean_enex_nb`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: generate_random
```
Generate signals randomly.

`shape` can be a shape-like tuple or an instance of `vectorbtpro.base.wrapping.ArrayWrapper`
(will be used as `wrapper`).

If `n` is set, uses `vectorbtpro.signals.nb.rand_place_nb`.
If `prob` is set, uses `vectorbtpro.signals.nb.rand_by_prob_place_nb`.

For arguments, see `SignalsAccessor.generate`.

`n` must be either a scalar or an array that will broadcast to the number of columns.
`prob` must be either a single number or an array that will broadcast to match `shape`.

Specify `seed` to make output deterministic.

Usage:
    * For each column, generate a variable number of signals:

    ```pycon
    >>> vbt.pd_acc.signals.generate_random(
    ...     (5, 3),
    ...     n=[0, 1, 2],
    ...     seed=42,
    ...     wrap_kwargs=dict(
    ...         index=mask.index,
    ...         columns=mask.columns
    ...     )
    ... )
                    a      b      c
    2020-01-01  False  False  False
    2020-01-02  False  False  False
    2020-01-03  False  False   True
    2020-01-04  False   True  False
    2020-01-05  False  False   True
    ```

    * For each column and time step, pick a signal with 50% probability:

    ```pycon
    >>> vbt.pd_acc.signals.generate_random(
    ...     (5, 3),
    ...     prob=0.5,
    ...     seed=42,
    ...     wrap_kwargs=dict(
    ...         index=mask.index,
    ...         columns=mask.columns
    ...     )
    ... )
                    a      b      c
    2020-01-01   True   True   True
    2020-01-02  False   True  False
    2020-01-03  False  False  False
    2020-01-04  False  False   True
    2020-01-05   True  False   True
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: generate_random_both
```
Generate chain of entry and exit signals randomly.

`shape` can be a shape-like tuple or an instance of `vectorbtpro.base.wrapping.ArrayWrapper`
(will be used as `wrapper`).

If `n` is set, uses `vectorbtpro.signals.nb.generate_rand_enex_nb`.
If `entry_prob` and `exit_prob` are set, uses `SignalsAccessor.generate_both` with
`vectorbtpro.signals.nb.rand_by_prob_place_nb`.

Usage:
    * For each column, generate two entries and exits randomly:

    ```pycon
    >>> en, ex = vbt.pd_acc.signals.generate_random_both(
    ...     (5, 3),
    ...     n=2,
    ...     seed=42,
    ...     wrap_kwargs=dict(
    ...         index=mask.index,
    ...         columns=mask.columns
    ...     )
    ... )
    >>> en
                    a      b      c
    2020-01-01  False  False   True
    2020-01-02   True   True  False
    2020-01-03  False  False  False
    2020-01-04   True   True   True
    2020-01-05  False  False  False
    >>> ex
                    a      b      c
    2020-01-01  False  False  False
    2020-01-02  False  False   True
    2020-01-03   True   True  False
    2020-01-04  False  False  False
    2020-01-05   True   True   True
    ```

    * For each column and time step, pick entry with 50% probability and exit right after:

    ```pycon
    >>> en, ex = vbt.pd_acc.signals.generate_random_both(
    ...     (5, 3),
    ...     entry_prob=0.5,
    ...     exit_prob=1.,
    ...     seed=42,
    ...     wrap_kwargs=dict(
    ...         index=mask.index,
    ...         columns=mask.columns
    ...     )
    ... )
    >>> en
                    a      b      c
    2020-01-01   True   True   True
    2020-01-02  False  False  False
    2020-01-03  False  False  False
    2020-01-04  False  False   True
    2020-01-05   True  False  False
    >>> ex
                    a      b      c
    2020-01-01  False  False  False
    2020-01-02   True   True   True
    2020-01-03  False  False  False
    2020-01-04  False  False  False
    2020-01-05  False  False   True
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: generate_random_exits
```
Generate exit signals randomly.

If `prob` is None, uses `vectorbtpro.signals.nb.rand_place_nb`.
Otherwise, uses `vectorbtpro.signals.nb.rand_by_prob_place_nb`.

Uses `SignalsAccessor.generate_exits`.

Specify `seed` to make output deterministic.

Usage:
    * After each entry in `mask`, generate exactly one exit:

    ```pycon
    >>> mask.vbt.signals.generate_random_exits(seed=42)
                    a      b      c
    2020-01-01  False  False  False
    2020-01-02  False   True  False
    2020-01-03  False  False  False
    2020-01-04   True   True  False
    2020-01-05  False  False   True
    ```

    * After each entry in `mask` and at each time step, generate exit with 50% probability:

    ```pycon
    >>> mask.vbt.signals.generate_random_exits(prob=0.5, seed=42)
                    a      b      c
    2020-01-01  False  False  False
    2020-01-02   True  False  False
    2020-01-03  False  False  False
    2020-01-04  False  False  False
    2020-01-05  False  False   True
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: generate_stop_exits
```
Generate exits based on when `ts` hits the stop.

For arguments, see `vectorbtpro.signals.nb.stop_place_nb`.
If `chain` is True, uses `SignalsAccessor.generate_both`.
Otherwise, uses `SignalsAccessor.generate_exits`.

Use `out_dict` as a dict to pass `stop_ts` array. You can also set `out_dict` to {}
to produce this array automatically and still have access to it.

All array-like arguments including stops and `out_dict` will broadcast using
`vectorbtpro.base.reshaping.broadcast` and `broadcast_kwargs`.

!!! hint
    Default arguments will generate an exit signal strictly between two entry signals.
    If both entry signals are too close to each other, no exit will be generated.

    To ignore all entries that come between an entry and its exit,
    set `until_next` to False and `skip_until_exit` to True.

    To remove all entries that come between an entry and its exit,
    set `chain` to True. This will return two arrays: new entries and exits.

Usage:
    * Regular stop loss:

    ```pycon
    >>> ts = pd.Series([1, 2, 3, 2, 1])

    >>> mask.vbt.signals.generate_stop_exits(ts, stop=-0.1)
                    a      b      c
    2020-01-01  False  False  False
    2020-01-02  False  False  False
    2020-01-03  False  False  False
    2020-01-04  False   True   True
    2020-01-05  False  False  False
    ```

    * Trailing stop loss:

    ```pycon
    >>> mask.vbt.signals.generate_stop_exits(ts, stop=-0.1, trailing=True)
                    a      b      c
    2020-01-01  False  False  False
    2020-01-02  False  False  False
    2020-01-03  False  False  False
    2020-01-04   True   True   True
    2020-01-05  False  False  False
    ```

    * Testing multiple take profit stops:

    ```pycon
    >>> mask.vbt.signals.generate_stop_exits(ts, stop=vbt.Param([1.0, 1.5]))
    stop                        1.0                  1.5
                    a      b      c      a      b      c
    2020-01-01  False  False  False  False  False  False
    2020-01-02   True   True  False  False  False  False
    2020-01-03  False  False  False   True  False  False
    2020-01-04  False  False  False  False  False  False
    2020-01-05  False  False  False  False  False  False
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: generate_ohlc_stop_exits
```
Generate exits based on when the price hits (trailing) stop loss or take profit.

Use `out_dict` as a dict to pass `stop_price` and `stop_type` arrays. You can also
set `out_dict` to {} to produce these arrays automatically and still have access to them.

For arguments, see `vectorbtpro.signals.nb.ohlc_stop_place_nb`.
If `chain` is True, uses `SignalsAccessor.generate_both`.
Otherwise, uses `SignalsAccessor.generate_exits`.

All array-like arguments including stops and `out_dict` will broadcast using
`vectorbtpro.base.reshaping.broadcast` and `broadcast_kwargs`.

For arguments, see `vectorbtpro.signals.nb.ohlc_stop_place_nb`.

!!! hint
    Default arguments will generate an exit signal strictly between two entry signals.
    If both entry signals are too close to each other, no exit will be generated.

    To ignore all entries that come between an entry and its exit,
    set `until_next` to False and `skip_until_exit` to True.

    To remove all entries that come between an entry and its exit,
    set `chain` to True. This will return two arrays: new entries and exits.

Usage:
    * Generate exits for TSL and TP of 10%:

    ```pycon
    >>> price = pd.DataFrame({
    ...     'open': [10, 11, 12, 11, 10],
    ...     'high': [11, 12, 13, 12, 11],
    ...     'low': [9, 10, 11, 10, 9],
    ...     'close': [10, 11, 12, 11, 10]
    ... })
    >>> out_dict = {}
    >>> exits = mask.vbt.signals.generate_ohlc_stop_exits(
    ...     price["open"],
    ...     price['open'],
    ...     price['high'],
    ...     price['low'],
    ...     price['close'],
    ...     tsl_stop=0.1,
    ...     tp_stop=0.1,
    ...     is_entry_open=True,
    ...     out_dict=out_dict,
    ... )
    >>> exits
                    a      b      c
    2020-01-01  False  False  False
    2020-01-02   True   True  False
    2020-01-03  False  False  False
    2020-01-04  False   True   True
    2020-01-05  False  False  False

    >>> out_dict['stop_price']
                   a     b     c
    2020-01-01   NaN   NaN   NaN
    2020-01-02  11.0  11.0   NaN
    2020-01-03   NaN   NaN   NaN
    2020-01-04   NaN  10.8  10.8
    2020-01-05   NaN   NaN   NaN

    >>> out_dict['stop_type'].vbt(mapping=vbt.sig_enums.StopType).apply_mapping()
                   a     b     c
    2020-01-01  None  None  None
    2020-01-02    TP    TP  None
    2020-01-03  None  None  None
    2020-01-04  None   TSL   TSL
    2020-01-05  None  None  None
    ```

    Notice how the first two entry signals in the third column have no exit signal -
    there is no room between them for an exit signal.

    * To find an exit for the first entry and ignore all entries that are in-between them,
    we can pass `until_next=False` and `skip_until_exit=True`:

    ```pycon
    >>> out_dict = {}
    >>> exits = mask.vbt.signals.generate_ohlc_stop_exits(
    ...     price['open'],
    ...     price['open'],
    ...     price['high'],
    ...     price['low'],
    ...     price['close'],
    ...     tsl_stop=0.1,
    ...     tp_stop=0.1,
    ...     is_entry_open=True,
    ...     out_dict=out_dict,
    ...     until_next=False,
    ...     skip_until_exit=True
    ... )
    >>> exits
                    a      b      c
    2020-01-01  False  False  False
    2020-01-02   True   True   True
    2020-01-03  False  False  False
    2020-01-04  False   True   True
    2020-01-05  False  False  False

    >>> out_dict['stop_price']
                   a     b     c
    2020-01-01   NaN   NaN   NaN
    2020-01-02  11.0  11.0  11.0
    2020-01-03   NaN   NaN   NaN
    2020-01-04   NaN  10.8  10.8
    2020-01-05   NaN   NaN   NaN

    >>> out_dict['stop_type'].vbt(mapping=vbt.sig_enums.StopType).apply_mapping()
                   a     b     c
    2020-01-01  None  None  None
    2020-01-02    TP    TP    TP
    2020-01-03  None  None  None
    2020-01-04  None   TSL   TSL
    2020-01-05  None  None  None
    ```

    Now, the first signal in the third column gets executed regardless of the entries that come next,
    which is very similar to the logic that is implemented in `vectorbtpro.portfolio.base.Portfolio.from_signals`.

    * To automatically remove all ignored entry signals, pass `chain=True`.
    This will return a new entries array:

    ```pycon
    >>> out_dict = {}
    >>> new_entries, exits = mask.vbt.signals.generate_ohlc_stop_exits(
    ...     price['open'],
    ...     price['open'],
    ...     price['high'],
    ...     price['low'],
    ...     price['close'],
    ...     tsl_stop=0.1,
    ...     tp_stop=0.1,
    ...     is_entry_open=True,
    ...     out_dict=out_dict,
    ...     chain=True
    ... )
    >>> new_entries
                    a      b      c
    2020-01-01   True   True   True
    2020-01-02  False  False  False  << removed entry in the third column
    2020-01-03  False   True   True
    2020-01-04  False  False  False
    2020-01-05  False   True  False
    >>> exits
                    a      b      c
    2020-01-01  False  False  False
    2020-01-02   True   True   True
    2020-01-03  False  False  False
    2020-01-04  False   True   True
    2020-01-05  False  False  False
    ```

    !!! warning
        The last two examples above make entries dependent upon exits - this makes only sense
        if you have no other exit arrays to combine this stop exit array with.

    * Test multiple parameter combinations:

    ```pycon
    >>> exits = mask.vbt.signals.generate_ohlc_stop_exits(
    ...     price['open'],
    ...     price['open'],
    ...     price['high'],
    ...     price['low'],
    ...     price['close'],
    ...     sl_stop=vbt.Param([False, 0.1]),
    ...     tsl_stop=vbt.Param([False, 0.1]),
    ...     is_entry_open=True
    ... )
    >>> exits
    sl_stop     False                                       0.1                \
    tsl_stop    False                  0.1                False
                    a      b      c      a      b      c      a      b      c
    2020-01-01  False  False  False  False  False  False  False  False  False
    2020-01-02  False  False  False  False  False  False  False  False  False
    2020-01-03  False  False  False  False  False  False  False  False  False
    2020-01-04  False  False  False   True   True   True  False   True   True
    2020-01-05  False  False  False  False  False  False   True  False  False

    sl_stop
    tsl_stop      0.1
                    a      b      c
    2020-01-01  False  False  False
    2020-01-02  False  False  False
    2020-01-03  False  False  False
    2020-01-04   True   True   True
    2020-01-05  False  False  False
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: rank
```
See `vectorbtpro.signals.nb.rank_nb`.

Arguments to `rank_func_nb` can be passed either as `*args` or `rank_args` (but not both!).

Will broadcast with `reset_by` using `vectorbtpro.base.reshaping.broadcast` and `broadcast_kwargs`.

Set `as_mapped` to True to return an instance of `vectorbtpro.records.mapped_array.MappedArray`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: pos_rank
```
Get signal position ranks.

Uses `SignalsAccessor.rank` with `vectorbtpro.signals.nb.sig_pos_rank_nb`.

Usage:
    * Rank each True value in each partition in `mask`:

    ```pycon
    >>> mask.vbt.signals.pos_rank()
                a  b  c
    2020-01-01  0  0  0
    2020-01-02 -1 -1  1
    2020-01-03 -1  0  2
    2020-01-04 -1 -1 -1
    2020-01-05 -1  0 -1

    >>> mask.vbt.signals.pos_rank(after_false=True)
                a  b  c
    2020-01-01 -1 -1 -1
    2020-01-02 -1 -1 -1
    2020-01-03 -1  0 -1
    2020-01-04 -1 -1 -1
    2020-01-05 -1  0 -1

    >>> mask.vbt.signals.pos_rank(allow_gaps=True)
                a  b  c
    2020-01-01  0  0  0
    2020-01-02 -1 -1  1
    2020-01-03 -1  1  2
    2020-01-04 -1 -1 -1
    2020-01-05 -1  2 -1

    >>> mask.vbt.signals.pos_rank(reset_by=~mask, allow_gaps=True)
                a  b  c
    2020-01-01  0  0  0
    2020-01-02 -1 -1  1
    2020-01-03 -1  0  2
    2020-01-04 -1 -1 -1
    2020-01-05 -1  0 -1
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: pos_rank_after
```
Get signal position ranks after each signal in `reset_by`.

!!! note
    `allow_gaps` is enabled by default.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: partition_pos_rank
```
Get partition position ranks.

Uses `SignalsAccessor.rank` with `vectorbtpro.signals.nb.part_pos_rank_nb`.

Usage:
    * Rank each partition of True values in `mask`:

    ```pycon
    >>> mask.vbt.signals.partition_pos_rank()
                a  b  c
    2020-01-01  0  0  0
    2020-01-02 -1 -1  0
    2020-01-03 -1  1  0
    2020-01-04 -1 -1 -1
    2020-01-05 -1  2 -1

    >>> mask.vbt.signals.partition_pos_rank(after_false=True)
                a  b  c
    2020-01-01 -1 -1 -1
    2020-01-02 -1 -1 -1
    2020-01-03 -1  0 -1
    2020-01-04 -1 -1 -1
    2020-01-05 -1  1 -1

    >>> mask.vbt.signals.partition_pos_rank(reset_by=mask)
                a  b  c
    2020-01-01  0  0  0
    2020-01-02 -1 -1  0
    2020-01-03 -1  0  0
    2020-01-04 -1 -1 -1
    2020-01-05 -1  0 -1
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: partition_pos_rank_after
```
Get partition position ranks after each signal in `reset_by`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: first
```
Select signals that satisfy the condition `pos_rank == 0`.

Uses `SignalsAccessor.pos_rank`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: first_after
```
Select signals that satisfy the condition `pos_rank == 0`.

Uses `SignalsAccessor.pos_rank_after`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: nth
```
Select signals that satisfy the condition `pos_rank == n`.

Uses `SignalsAccessor.pos_rank`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: nth_after
```
Select signals that satisfy the condition `pos_rank == n`.

Uses `SignalsAccessor.pos_rank_after`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: from_nth
```
Select signals that satisfy the condition `pos_rank >= n`.

Uses `SignalsAccessor.pos_rank`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: from_nth_after
```
Select signals that satisfy the condition `pos_rank >= n`.

Uses `SignalsAccessor.pos_rank_after`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: to_nth
```
Select signals that satisfy the condition `pos_rank < n`.

Uses `SignalsAccessor.pos_rank`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: to_nth_after
```
Select signals that satisfy the condition `pos_rank < n`.

Uses `SignalsAccessor.pos_rank_after`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: pos_rank_mapped
```
Get a mapped array of signal position ranks.

Uses `SignalsAccessor.pos_rank`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: partition_pos_rank_mapped
```
Get a mapped array of partition position ranks.

Uses `SignalsAccessor.partition_pos_rank`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: distance_from_last
```
See `vectorbtpro.signals.nb.distance_from_last_nb`.

Usage:
    * Get the distance to the last signal:

    ```pycon
    >>> mask.vbt.signals.distance_from_last()
                a  b  c
    2020-01-01 -1 -1 -1
    2020-01-02  1  1  1
    2020-01-03  2  2  1
    2020-01-04  3  1  1
    2020-01-05  4  2  2
    ```

    * Get the distance to the second last signal:

    ```pycon
    >>> mask.vbt.signals.distance_from_last(nth=2)
                a  b  c
    2020-01-01 -1 -1 -1
    2020-01-02 -1 -1  1
    2020-01-03 -1  2  1
    2020-01-04 -1  3  2
    2020-01-05 -1  2  3
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: to_mapped
```
Convert this object into an instance of `vectorbtpro.records.mapped_array.MappedArray`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: get_relation_str
```
Get direction string for `relation`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: delta_ranges
```
Build a record array of the type `vectorbtpro.generic.ranges.Ranges`
from a delta applied after each signal (or before if delta is negative).
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: between_ranges
```
Wrap the result of `vectorbtpro.signals.nb.between_ranges_nb`
with `vectorbtpro.generic.ranges.Ranges`.

If `target` specified, see `vectorbtpro.signals.nb.between_two_ranges_nb`.
Both will broadcast using `vectorbtpro.base.reshaping.broadcast` and `broadcast_kwargs`.

Usage:
    * One array:

    ```pycon
    >>> mask_sr = pd.Series([True, False, False, True, False, True, True])
    >>> ranges = mask_sr.vbt.signals.between_ranges()
    >>> ranges
    <vectorbtpro.generic.ranges.Ranges at 0x7ff29ea7c7b8>

    >>> ranges.readable
       Range Id  Column  Start Index  End Index  Status
    0         0       0            0          3  Closed
    1         1       0            3          5  Closed
    2         2       0            5          6  Closed

    >>> ranges.duration.values
    array([3, 2, 1])
    ```

    * Two arrays, traversing the signals of the first array:

    ```pycon
    >>> mask_sr1 = pd.Series([True, True, True, False, False])
    >>> mask_sr2 = pd.Series([False, False, True, False, True])
    >>> ranges = mask_sr1.vbt.signals.between_ranges(target=mask_sr2)
    >>> ranges
    <vectorbtpro.generic.ranges.Ranges at 0x7ff29e3b80f0>

    >>> ranges.readable
       Range Id  Column  Start Index  End Index  Status
    0         0       0            2          2  Closed
    1         1       0            2          4  Closed

    >>> ranges.duration.values
    array([0, 2])
    ```

    * Two arrays, traversing the signals of the second array:

    ```pycon
    >>> ranges = mask_sr1.vbt.signals.between_ranges(target=mask_sr2, relation="manyone")
    >>> ranges
    <vectorbtpro.generic.ranges.Ranges at 0x7ff29eccbd68>

    >>> ranges.readable
       Range Id  Column  Start Index  End Index  Status
    0         0       0            0          2  Closed
    1         1       0            1          2  Closed
    2         2       0            2          2  Closed

    >>> ranges.duration.values
    array([0, 2])
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: partition_ranges
```
Wrap the result of `vectorbtpro.signals.nb.partition_ranges_nb`
with `vectorbtpro.generic.ranges.Ranges`.

If `use_end_idxs` is True, uses the index of the last signal in each partition as `idx_arr`.
Otherwise, uses the index of the first signal.

Usage:
    ```pycon
    >>> mask_sr = pd.Series([True, True, True, False, True, True])
    >>> mask_sr.vbt.signals.partition_ranges().readable
       Range Id  Column  Start Timestamp  End Timestamp  Status
    0         0       0                0              3  Closed
    1         1       0                4              5    Open
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: between_partition_ranges
```
Wrap the result of `vectorbtpro.signals.nb.between_partition_ranges_nb`
with `vectorbtpro.generic.ranges.Ranges`.

Usage:
    ```pycon
    >>> mask_sr = pd.Series([True, False, False, True, False, True, True])
    >>> mask_sr.vbt.signals.between_partition_ranges().readable
       Range Id  Column  Start Timestamp  End Timestamp  Status
    0         0       0                0              3  Closed
    1         1       0                3              5  Closed
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: index_from_unravel
```
Get index from an unraveling operation.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: unravel
```
Unravel signals.

See `vectorbtpro.signals.nb.unravel_nb`.

Argument `signal_index_type` takes the following values:

* "range": Basic signal counter in a column
* "position(s)": Integer position (row) of signal in a column
* "label(s)": Label of signal in a column

Usage:
    ```pycon
    >>> mask.vbt.signals.unravel()
    signal          0      0      1      2      0      1      2
                    a      b      b      b      c      c      c
    2020-01-01   True   True  False  False   True  False  False
    2020-01-02  False  False  False  False  False   True  False
    2020-01-03  False  False   True  False  False  False   True
    2020-01-04  False  False  False  False  False  False  False
    2020-01-05  False  False  False   True  False  False  False
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: unravel_between
```
Unravel signal pairs.

If one array is passed, see `vectorbtpro.signals.nb.unravel_between_nb`.
If two arrays are passed, see `vectorbtpro.signals.nb.unravel_between_two_nb`.

Argument `signal_index_type` takes the following values:

* "pair_range": Basic pair counter in a column
* "range": Basic signal counter in a column
* "source_range": Basic signal counter in a source column
* "target_range": Basic signal counter in a target column
* "position(s)": Integer position (row) of signal in a column
* "source_position(s)": Integer position (row) of signal in a source column
* "target_position(s)": Integer position (row) of signal in a target column
* "label(s)": Label of signal in a column
* "source_label(s)": Label of signal in a source column
* "target_label(s)": Label of signal in a target column

Usage:
    * One mask:

    ```pycon
    >>> mask.vbt.signals.unravel_between()
    signal         -1      0      1      0      1
                    a      b      b      c      c
    2020-01-01  False   True  False   True  False
    2020-01-02  False  False  False   True   True
    2020-01-03  False   True   True  False   True
    2020-01-04  False  False  False  False  False
    2020-01-05  False  False   True  False  False

    >>> mask.vbt.signals.unravel_between(signal_index_type="position")
    source_signal     -1      0      2      0      1
    target_signal     -1      2      4      1      2
                       a      b      b      c      c
    2020-01-01     False   True  False   True  False
    2020-01-02     False  False  False   True   True
    2020-01-03     False   True   True  False   True
    2020-01-04     False  False  False  False  False
    2020-01-05     False  False   True  False  False
    ```

    * Two masks:

    ```pycon
    >>> source_mask = pd.Series([True, True, False, False, True, True])
    >>> target_mask = pd.Series([False, False, True, True, False, False])
    >>> new_source_mask, new_target_mask = vbt.pd_acc.signals.unravel_between(
    ...     source_mask,
    ...     target_mask
    ... )
    >>> new_source_mask
    signal      0      1
    0       False  False
    1        True   True
    2       False  False
    3       False  False
    4       False  False
    5       False  False
    >>> new_target_mask
    signal      0      1
    0       False  False
    1       False  False
    2        True  False
    3       False   True
    4       False  False
    5       False  False

    >>> new_source_mask, new_target_mask = vbt.pd_acc.signals.unravel_between(
    ...     source_mask,
    ...     target_mask,
    ...     relation="chain"
    ... )
    >>> new_source_mask
    signal      0      1
    0        True  False
    1       False  False
    2       False  False
    3       False  False
    4       False   True
    5       False  False
    >>> new_target_mask
    signal      0      1
    0       False  False
    1       False  False
    2        True   True
    3       False  False
    4       False  False
    5       False  False
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: ravel
```
Ravel signals.

See `vectorbtpro.signals.nb.ravel_nb`.

Usage:
    ```pycon
    >>> unravel_mask = mask.vbt.signals.unravel()
    >>> original_mask = unravel_mask.vbt.signals.ravel(group_by=vbt.ExceptLevel("signal"))
    >>> original_mask
                    a      b      c
    2020-01-01   True   True   True
    2020-01-02  False  False   True
    2020-01-03  False   True   True
    2020-01-04  False  False  False
    2020-01-05  False   True  False
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: nth_index
```
See `vectorbtpro.signals.nb.nth_index_nb`.

Usage:
    ```pycon
    >>> mask.vbt.signals.nth_index(0)
    a   2020-01-01
    b   2020-01-01
    c   2020-01-01
    Name: nth_index, dtype: datetime64[ns]

    >>> mask.vbt.signals.nth_index(2)
    a          NaT
    b   2020-01-05
    c   2020-01-03
    Name: nth_index, dtype: datetime64[ns]

    >>> mask.vbt.signals.nth_index(-1)
    a   2020-01-01
    b   2020-01-05
    c   2020-01-03
    Name: nth_index, dtype: datetime64[ns]

    >>> mask.vbt.signals.nth_index(-1, group_by=True)
    Timestamp('2020-01-05 00:00:00')
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: norm_avg_index
```
See `vectorbtpro.signals.nb.norm_avg_index_nb`.

Normalized average index measures the average signal location relative to the middle of the column.
This way, we can quickly see where the majority of signals are located.

Common values are:

* -1.0: only the first signal is set
* 1.0: only the last signal is set
* 0.0: symmetric distribution around the middle
* [-1.0, 0.0): average signal is on the left
* (0.0, 1.0]: average signal is on the right

Usage:
    ```pycon
    >>> pd.Series([True, False, False, False]).vbt.signals.norm_avg_index()
    -1.0

    >>> pd.Series([False, False, False, True]).vbt.signals.norm_avg_index()
    1.0

    >>> pd.Series([True, False, False, True]).vbt.signals.norm_avg_index()
    0.0
    ```
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: index_mapped
```
Get a mapped array of indices.

See `vectorbtpro.generic.accessors.GenericAccessor.to_mapped`.

Only True values will be considered.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: total
```
Total number of True values in each column/group.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: rate
```
`SignalsAccessor.total` divided by the total index length in each column/group.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: total_partitions
```
Total number of partitions of True values in each column/group.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: partition_rate
```
`SignalsAccessor.total_partitions` divided by `SignalsAccessor.total` in each column/group.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: stats_defaults
```
Defaults for `SignalsAccessor.stats`.

Merges `vectorbtpro.generic.accessors.GenericAccessor.stats_defaults` and
`stats` from `vectorbtpro._settings.signals`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: plot
```
Plot signals.

Args:
    yref (str): Y coordinate axis.
    column (hashable): Column to plot.
    **kwargs: Keyword arguments passed to `vectorbtpro.generic.accessors.GenericAccessor.lineplot`.

Usage:
    ```pycon
    >>> mask[['a', 'c']].vbt.signals.plot().show()
    ```

    ![](/assets/images/api/signals_df_plot.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/signals_df_plot.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: plot_as_markers
```
Plot Series as markers.

Args:
    y (array_like): Y-axis values to plot markers on.
    column (hashable): Column to plot.
    **kwargs: Keyword arguments passed to `vectorbtpro.generic.accessors.GenericAccessor.scatterplot`.

Usage:
    ```pycon
    >>> ts = pd.Series([1, 2, 3, 2, 1], index=mask.index)
    >>> fig = ts.vbt.lineplot()
    >>> mask['b'].vbt.signals.plot_as_entries(y=ts, fig=fig)
    >>> (~mask['b']).vbt.signals.plot_as_exits(y=ts, fig=fig).show()
    ```

    ![](/assets/images/api/signals_plot_as_markers.light.svg#only-light){: .iimg loading=lazy }
    ![](/assets/images/api/signals_plot_as_markers.dark.svg#only-dark){: .iimg loading=lazy }
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: plot_as_entries
```
Plot signals as entry markers.

See `SignalsSRAccessor.plot_as_markers`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: plot_as_exits
```
Plot signals as exit markers.

See `SignalsSRAccessor.plot_as_markers`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: plot_as_entry_marks
```
Plot signals as marked entry markers.

See `SignalsSRAccessor.plot_as_markers`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: plot_as_exit_marks
```
Plot signals as marked exit markers.

See `SignalsSRAccessor.plot_as_markers`.
```

---

# Pasta: signals
### Arquivo: accessors.py
#### Classe: SignalsAccessor
#### Função: plots_defaults
```
Defaults for `SignalsAccessor.plots`.

Merges `vectorbtpro.generic.accessors.GenericAccessor.plots_defaults` and
`plots` from `vectorbtpro._settings.signals`.
```

---

# Pasta: signals
### Arquivo: enums.py
#### Docstring do Módulo
```
Named tuples and enumerated types for signals.

Defines enums and other schemas for `vectorbtpro.signals`.
```

---

# Pasta: signals
### Arquivo: factory.py
#### Docstring do Módulo
```
Factory for building signal generators.

The signal factory class `SignalFactory` extends `vectorbtpro.indicators.factory.IndicatorFactory`
to offer a convenient way to create signal generators of any complexity. By providing it with information
such as entry and exit functions and the names of inputs, parameters, and outputs, it will create a
stand-alone class capable of generating signals for an arbitrary combination of inputs and parameters.
```

---

# Pasta: signals
### Arquivo: factory.py
#### Classe: SignalFactory
```
A factory for building signal generators.

Extends `vectorbtpro.indicators.factory.IndicatorFactory` with place functions.

Generates a fixed number of outputs (depending upon `mode`).
If you need to generate other outputs, use in-place outputs (via `in_output_names`).

See `vectorbtpro.signals.enums.FactoryMode` for supported generation modes.

Other arguments are passed to `vectorbtpro.indicators.factory.IndicatorFactory`.
```

---

# Pasta: signals
### Arquivo: factory.py
#### Classe: SignalFactory
#### Função: mode
```
Factory mode.
```

---

# Pasta: signals
### Arquivo: factory.py
#### Classe: SignalFactory
#### Função: with_place_func
```
Build signal generator class around entry and exit placement functions.

A placement function is simply a function that places signals.
There are two types of it: entry placement function and exit placement function.
Each placement function takes broadcast time series, broadcast in-place output time series,
broadcast parameter arrays, and other arguments, and returns an array of indices
corresponding to chosen signals. See `vectorbtpro.signals.nb.generate_nb`.

Args:
    entry_place_func_nb (callable): `place_func_nb` that returns indices of entries.

        Defaults to `vectorbtpro.signals.nb.first_place_nb` for `FactoryMode.Chain`.
    exit_place_func_nb (callable): `place_func_nb` that returns indices of exits.
    generate_func_nb (callable): Entry generation function.

        Defaults to `vectorbtpro.signals.nb.generate_nb`.
    generate_ex_func_nb (callable): Exit generation function.

        Defaults to `vectorbtpro.signals.nb.generate_ex_nb`.
    generate_enex_func_nb (callable): Entry and exit generation function.

        Defaults to `vectorbtpro.signals.nb.generate_enex_nb`.
    cache_func (callable): A caching function to preprocess data beforehand.

        All returned objects will be passed as last arguments to placement functions.
    entry_settings (dict): Settings dict for `entry_place_func_nb`.
    exit_settings (dict): Settings dict for `exit_place_func_nb`.
    cache_settings (dict): Settings dict for `cache_func`.
    jit_kwargs (dict): Keyword arguments passed to `@njit` decorator of the parameter selection function.

        By default, has `nogil` set to True.
    jitted (any): See `vectorbtpro.utils.jitting.resolve_jitted_option`.

        Gets applied to generation functions only. If the respective generation
        function is not jitted, then the apply function won't be jitted as well.
    **kwargs: Keyword arguments passed to `IndicatorFactory.with_custom_func`.

!!! note
    Choice functions must be Numba-compiled.

    Which inputs, parameters and arguments to pass to each function must be
    explicitly indicated in the function's settings dict. By default, nothing is passed.

    Passing keyword arguments directly to the placement functions is not supported.
    Use `pass_kwargs` in a settings dict to pass keyword arguments as positional.

Settings dict of each function can have the following keys:

Attributes:
    pass_inputs (list of str): Input names to pass to the placement function.

        Defaults to []. Order matters. Each name must be in `input_names`.
    pass_in_outputs (list of str): In-place output names to pass to the placement function.

        Defaults to []. Order matters. Each name must be in `in_output_names`.
    pass_params (list of str): Parameter names to pass to the placement function.

        Defaults to []. Order matters. Each name must be in `param_names`.
    pass_kwargs (dict, list of str or list of tuple): Keyword arguments from `kwargs` dict to
        pass as positional arguments to the placement function.

        Defaults to []. Order matters.

        If any element is a tuple, must contain the name and the default value.
        If any element is a string, the default value is None.

        Built-in keys include:

        * `input_shape`: Input shape if no input time series passed.
            Default is provided by the pipeline if `pass_input_shape` is True.
        * `wait`: Number of ticks to wait before placing signals.
            Default is 1.
        * `until_next`: Whether to place signals up to the next entry signal.
            Default is True. Applied in `generate_ex_func_nb` only.
        * `skip_until_exit`: Whether to skip processing entry signals until the next exit.
            Default is False. Applied in `generate_ex_func_nb` only.
        * `pick_first`: Whether to stop as soon as the first exit signal is found.
            Default is False with `FactoryMode.Entries`, otherwise is True.
        * `temp_idx_arr`: Empty integer array used to temporarily store indices.
            Default is an automatically generated array of shape `input_shape[0]`.
            You can also pass `temp_idx_arr1`, `temp_idx_arr2`, etc. to generate multiple.
    pass_cache (bool): Whether to pass cache from `cache_func` to the placement function.

        Defaults to False. Cache is passed unpacked.

The following arguments can be passed to `run` and `run_combs` methods:

Args:
    *args: Can be used instead of `place_args`.
    place_args (tuple): Arguments passed to any placement function (depending on the mode).
    entry_place_args (tuple): Arguments passed to the entry placement function.
    exit_place_args (tuple): Arguments passed to the exit placement function.
    entry_args (tuple): Alias for `entry_place_args`.
    exit_args (tuple): Alias for `exit_place_args`.
    cache_args (tuple): Arguments passed to the cache function.
    entry_kwargs (tuple): Settings for the entry placement function. Also contains arguments
        passed as positional if in `pass_kwargs`.
    exit_kwargs (tuple): Settings for the exit placement function. Also contains arguments
        passed as positional if in `pass_kwargs`.
    cache_kwargs (tuple): Settings for the cache function. Also contains arguments
        passed as positional if in `pass_kwargs`.
    return_cache (bool): Whether to return only cache.
    use_cache (any): Cache to use.
    **kwargs: Default keyword arguments (depending on the mode).

For more arguments, see `vectorbtpro.indicators.factory.IndicatorBase.run_pipeline`.

Usage:
    * The simplest signal indicator that places True at the very first index:

    ```pycon
    >>> from vectorbtpro import *

    >>> @njit
    ... def entry_place_func_nb(c):
    ...     c.out[0] = True
    ...     return 0

    >>> @njit
    ... def exit_place_func_nb(c):
    ...     c.out[0] = True
    ...     return 0

    >>> MySignals = vbt.SignalFactory().with_place_func(
    ...     entry_place_func_nb=entry_place_func_nb,
    ...     exit_place_func_nb=exit_place_func_nb,
    ...     entry_kwargs=dict(wait=1),
    ...     exit_kwargs=dict(wait=1)
    ... )

    >>> my_sig = MySignals.run(input_shape=(3, 3))
    >>> my_sig.entries
           0      1      2
    0   True   True   True
    1  False  False  False
    2   True   True   True
    >>> my_sig.exits
           0      1      2
    0  False  False  False
    1   True   True   True
    2  False  False  False
    ```

    * Take the first entry and place an exit after waiting `n` ticks. Find the next entry and repeat.
    Test three different `n` values.

    ```pycon
    >>> from vectorbtpro.signals.factory import SignalFactory

    >>> @njit
    ... def wait_place_nb(c, n):
    ...     if n < len(c.out):
    ...         c.out[n] = True
    ...         return n
    ...     return -1

    >>> # Build signal generator
    >>> MySignals = SignalFactory(
    ...     mode='chain',
    ...     param_names=['n']
    ... ).with_place_func(
    ...     exit_place_func_nb=wait_place_nb,
    ...     exit_settings=dict(
    ...         pass_params=['n']
    ...     )
    ... )

    >>> # Run signal generator
    >>> entries = [True, True, True, True, True]
    >>> my_sig = MySignals.run(entries, [0, 1, 2])

    >>> my_sig.entries  # input entries
    custom_n     0     1     2
    0         True  True  True
    1         True  True  True
    2         True  True  True
    3         True  True  True
    4         True  True  True

    >>> my_sig.new_entries  # output entries
    custom_n      0      1      2
    0          True   True   True
    1         False  False  False
    2          True  False  False
    3         False   True  False
    4          True  False   True

    >>> my_sig.exits  # output exits
    custom_n      0      1      2
    0         False  False  False
    1          True  False  False
    2         False   True  False
    3          True  False   True
    4         False  False  False
    ```

    * To combine multiple iterative signals, you would need to create a custom placement function.
    Here is an example of combining two random generators using "OR" rule (the first signal wins):

    ```pycon
    >>> from vectorbtpro.indicators.configs import flex_elem_param_config
    >>> from vectorbtpro.signals.factory import SignalFactory
    >>> from vectorbtpro.signals.nb import rand_by_prob_place_nb

    >>> # Enum to distinguish random generators
    >>> RandType = namedtuple('RandType', ['R1', 'R2'])(0, 1)

    >>> # Define exit placement function
    >>> @njit
    ... def rand_exit_place_nb(c, rand_type, prob1, prob2):
    ...     for out_i in range(len(c.out)):
    ...         if np.random.uniform(0, 1) < prob1:
    ...             c.out[out_i] = True
    ...             rand_type[c.from_i + out_i] = RandType.R1
    ...             return out_i
    ...         if np.random.uniform(0, 1) < prob2:
    ...             c.out[out_i] = True
    ...             rand_type[c.from_i + out_i] = RandType.R2
    ...             return out_i
    ...     return -1

    >>> # Build signal generator
    >>> MySignals = SignalFactory(
    ...     mode='chain',
    ...     in_output_names=['rand_type'],
    ...     param_names=['prob1', 'prob2'],
    ...     attr_settings=dict(
    ...         rand_type=dict(dtype=RandType)  # creates rand_type_readable
    ...     )
    ... ).with_place_func(
    ...     exit_place_func_nb=rand_exit_place_nb,
    ...     exit_settings=dict(
    ...         pass_in_outputs=['rand_type'],
    ...         pass_params=['prob1', 'prob2']
    ...     ),
    ...     param_settings=dict(
    ...         prob1=flex_elem_param_config,  # param per frame/row/col/element
    ...         prob2=flex_elem_param_config
    ...     ),
    ...     rand_type=-1  # fill with this value
    ... )

    >>> # Run signal generator
    >>> entries = [True, True, True, True, True]
    >>> my_sig = MySignals.run(entries, [0., 1.], [0., 1.], param_product=True)

    >>> my_sig.new_entries
    custom_prob1           0.0           1.0
    custom_prob2    0.0    1.0    0.0    1.0
    0              True   True   True   True
    1             False  False  False  False
    2             False   True   True   True
    3             False  False  False  False
    4             False   True   True   True

    >>> my_sig.exits
    custom_prob1           0.0           1.0
    custom_prob2    0.0    1.0    0.0    1.0
    0             False  False  False  False
    1             False   True   True   True
    2             False  False  False  False
    3             False   True   True   True
    4             False  False  False  False

    >>> my_sig.rand_type_readable
    custom_prob1     0.0     1.0
    custom_prob2 0.0 1.0 0.0 1.0
    0
    1                 R2  R1  R1
    2
    3                 R2  R1  R1
    4
    ```
```

---

# Pasta: signals
### Arquivo: nb.py
#### Docstring do Módulo
```
Numba-compiled functions for signals.

Provides an arsenal of Numba-compiled functions that are used by accessors
and in many other parts of the backtesting pipeline, such as technical indicators.
These only accept NumPy arrays and other Numba-compatible types.

!!! note
    vectorbt treats matrices as first-class citizens and expects input arrays to be
    2-dim, unless function has suffix `_1d` or is meant to be input to another function. 
    Data is processed along index (axis 0).
    
    All functions passed as argument must be Numba-compiled.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: generate_nb
```
Create a boolean matrix of `target_shape` and place signals using `place_func_nb`.

Args:
    target_shape (array): Target shape.
    place_func_nb (callable): Signal placement function.

        `place_func_nb` must accept a context of type `vectorbtpro.signals.enums.GenEnContext`,
        and return the index of the last signal (-1 to break the loop).
    place_args: Arguments passed to `place_func_nb`.
    only_once (bool): Whether to run the placement function only once.
    wait (int): Number of ticks to wait before placing the next entry.

!!! note
    The first argument is always a 1-dimensional boolean array that contains only those
    elements where signals can be placed. The range and column indices only describe which
    range this array maps to.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: generate_ex_nb
```
Place exit signals using `exit_place_func_nb` after each signal in `entries`.

Args:
    entries (array): Boolean array with entry signals.
    exit_place_func_nb (callable): Exit place function.

        `exit_place_func_nb` must accept a context of type `vectorbtpro.signals.enums.GenExContext`,
        and return the index of the last signal (-1 to break the loop).
    exit_place_args (callable): Arguments passed to `exit_place_func_nb`.
    wait (int): Number of ticks to wait before placing exits.

        !!! note
            Setting `wait` to 0 or False may result in two signals at one bar.
    until_next (int): Whether to place signals up to the next entry signal.

        !!! note
            Setting it to False makes it difficult to tell which exit belongs to which entry.
    skip_until_exit (bool): Whether to skip processing entry signals until the next exit.

        Has only effect when `until_next` is disabled.

        !!! note
            Setting it to True makes it impossible to tell which exit belongs to which entry.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: generate_enex_nb
```
Place entry signals using `entry_place_func_nb` and exit signals using
`exit_place_func_nb` one after another.

Args:
    target_shape (array): Target shape.
    entry_place_func_nb (callable): Entry place function.

        `entry_place_func_nb` must accept a context of type `vectorbtpro.signals.enums.GenEnExContext`,
        and return the index of the last signal (-1 to break the loop).
    entry_place_args (tuple): Arguments unpacked and passed to `entry_place_func_nb`.
    exit_place_func_nb (callable): Exit place function.

        `exit_place_func_nb` must accept a context of type `vectorbtpro.signals.enums.GenEnExContext`,
        and return the index of the last signal (-1 to break the loop).
    exit_place_args (tuple): Arguments unpacked and passed to `exit_place_func_nb`.
    entry_wait (int): Number of ticks to wait before placing entries.

        !!! note
            Setting `entry_wait` to 0 or False assumes that both entry and exit can be processed
            within the same bar, and exit can be processed before entry.
    exit_wait (int): Number of ticks to wait before placing exits.

        !!! note
            Setting `exit_wait` to 0 or False assumes that both entry and exit can be processed
            within the same bar, and entry can be processed before exit.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: rand_place_nb
```
`place_func_nb` to randomly pick `n` values.

`n` uses flexible indexing.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: rand_by_prob_place_nb
```
`place_func_nb` to randomly place signals with probability `prob`.

`prob` uses flexible indexing.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: generate_rand_enex_nb
```
Pick a number of entries and the same number of exits one after another.

Respects `entry_wait` and `exit_wait` constraints through a number of tricks.
Tries to mimic a uniform distribution as much as possible.

The idea is the following: with constraints, there is some fixed amount of total
space required between first entry and last exit. Upscale this space in a way that
distribution of entries and exit is similar to a uniform distribution. This means
randomizing the position of first entry, last exit, and all signals between them.

`n` uses flexible indexing and thus must be at least a 0-dim array.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: rand_enex_apply_nb
```
`apply_func_nb` that calls `generate_rand_enex_nb`.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: first_place_nb
```
`place_func_nb` that keeps only the first signal in `mask`.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: stop_place_nb
```
`place_func_nb` that places an exit signal whenever a threshold is being hit.

!!! note
    Waiting time cannot be higher than 1.

    If waiting time is 0, `entry_ts` should be the first value in the bar.
    If waiting time is 1, `entry_ts` should be the last value in the bar.

Args:
    c (GenExContext or GenEnExContext): Signal context.
    entry_ts (array of float): Entry price.

        Utilizes flexible indexing.
    ts (array of float): Price to compare the stop value against.

        Utilizes flexible indexing. If NaN, defaults to `entry_ts`.
    follow_ts (array of float): Following price.

        Utilizes flexible indexing. If NaN, defaults to `ts`. Applied only if the stop is trailing.
    stop_ts_out (array of float): Array where hit price of each exit will be stored.

        Must be of the full shape.
    stop (array of float): Stop value.

        Utilizes flexible indexing. Set an element to `np.nan` to disable it.
    trailing (array of bool): Whether the stop is trailing.

        Utilizes flexible indexing. Set an element to False to disable it.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: ohlc_stop_place_nb
```
`place_func_nb` that places an exit signal whenever a threshold is being hit using OHLC.

Compared to `stop_place_nb`, takes into account the whole bar, can check for both
(trailing) stop loss and take profit simultaneously, and tracks hit price and stop type.

!!! note
    Waiting time cannot be higher than 1.

Args:
    c (GenExContext or GenEnExContext): Signal context.
    entry_price (array of float): Entry price.

        Utilizes flexible indexing.
    open (array of float): Open price.

        Utilizes flexible indexing. If Nan and `is_entry_open` is True, defaults to entry price.
    high (array of float): High price.

        Utilizes flexible indexing. If NaN, gets calculated from open and close.
    low (array of float): Low price.

        Utilizes flexible indexing. If NaN, gets calculated from open and close.
    close (array of float): Close price.

        Utilizes flexible indexing. If Nan and `is_entry_open` is False, defaults to entry price.
    stop_price_out (array of float): Array where hit price of each exit will be stored.

        Must be of the full shape.
    stop_type_out (array of int): Array where stop type of each exit will be stored.

        Must be of the full shape. 0 for stop loss, 1 for take profit.
    sl_stop (array of float): Stop loss as a percentage.

        Utilizes flexible indexing. Set an element to `np.nan` to disable.
    tsl_th (array of float): Take profit threshold as a percentage for the trailing stop loss.

        Utilizes flexible indexing. Set an element to `np.nan` to disable.
    tsl_stop (array of float): Trailing stop loss as a percentage for the trailing stop loss.

        Utilizes flexible indexing. Set an element to `np.nan` to disable.
    tp_stop (array of float): Take profit as a percentage.

        Utilizes flexible indexing. Set an element to `np.nan` to disable.
    reverse (array of float): Whether to do the opposite, i.e.: prices are followed downwards.

        Utilizes flexible indexing.
    is_entry_open (bool): Whether entry price comes right at or before open.

        If True, uses high and low of the entry bar. Otherwise, uses only close.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: rank_nb
```
Rank each signal using `rank_func_nb`.

Applies `rank_func_nb` on each True value. Must accept a context of type
`vectorbtpro.signals.enums.RankContext`. Must return -1 for no rank, otherwise 0 or greater.

Setting `after_false` to True will disregard the first partition of True values
if there is no False value before them. Setting `after_reset` to True will disregard
the first partition of True values coming before the first reset signal. Setting `reset_wait`
to 0 will treat the signal at the same position as the reset signal as the first signal in
the next partition. Setting it to 1 will treat it as the last signal in the previous partition.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: sig_pos_rank_nb
```
`rank_func_nb` that returns the rank of each signal by its position in the partition
if `allow_gaps` is False, otherwise globally.

Resets at each reset signal.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: part_pos_rank_nb
```
`rank_func_nb` that returns the rank of each partition by its position in the series.

Resets at each reset signal.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: distance_from_last_1d_nb
```
Distance from the last n-th True value to the current value.

Unless `nth` is zero, the current True value isn't counted as one of the last True values.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: distance_from_last_nb
```
2-dim version of `distance_from_last_1d_nb`.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: clean_enex_1d_nb
```
Clean entry and exit arrays by picking the first signal out of each.

Set `force_first` to True to force placing the first entry/exit before the first exit/entry.
Set `keep_conflicts` to True to process signals at the same timestamp sequentially instead of removing them.
Set `reverse_order` to True to reverse the order of signals.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: clean_enex_nb
```
2-dim version of `clean_enex_1d_nb`.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: relation_idxs_1d_nb
```
Get index pairs of True values between a source and target mask.

For `relation`, see `vectorbtpro.signals.enums.SignalRelation`.

!!! note
    If both True values happen at the same time, source signal is assumed to come first.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: between_ranges_nb
```
Create a record of type `vectorbtpro.generic.enums.range_dt` for each range between two signals in `mask`.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: between_two_ranges_nb
```
Create a record of type `vectorbtpro.generic.enums.range_dt` for each range
between a source and target mask.

Index pairs are resolved with `relation_idxs_1d_nb`.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: partition_ranges_nb
```
Create a record of type `vectorbtpro.generic.enums.range_dt` for each partition of signals in `mask`.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: between_partition_ranges_nb
```
Create a record of type `vectorbtpro.generic.enums.range_dt` for each range between two partitions in `mask`.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: unravel_nb
```
Unravel each True value in a mask to a separate column.

Returns the new mask, the index of each True value in its column, the row index of each
True value in its column, and the column index of each True value in the original mask.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: unravel_between_nb
```
Unravel each pair of successive True values in a mask to a separate column.

Returns the new mask, the index of each source True value in its column, the index of
each target True value in its column, the row index of each source True value in the original
mask, the row index of each target True value in the original mask, and the column index of
each True value in the original mask.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: unravel_between_two_nb
```
Unravel each pair of successive True values between a source and target mask to a separate column.

Index pairs are resolved with `relation_idxs_1d_nb`.

Returns the new source mask, the new target mask, the index of each source True value in its column,
the index of each target True value in its column, the row index of each True value in each
original mask, and the column index of each True value in both original masks.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: ravel_nb
```
Ravel True values of each group into a separate column.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: nth_index_1d_nb
```
Get the index of the n-th True value.

!!! note
    `n` starts with 0 and can be negative.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: nth_index_nb
```
2-dim version of `nth_index_1d_nb`.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: norm_avg_index_1d_nb
```
Get mean index normalized to (-1, 1).
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: norm_avg_index_nb
```
2-dim version of `norm_avg_index_1d_nb`.
```

---

# Pasta: signals
### Arquivo: nb.py
#### Função: norm_avg_index_grouped_nb
```
Grouped version of `norm_avg_index_nb`.
```

---

# Pasta: signals
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules for working with signals.
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: ohlcstcx.py
#### Docstring do Módulo
```
Module with `OHLCSTCX`.
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: ohlcstcx.py
#### Classe: _OHLCSTCX
```
Exit signal generator based on OHLC and stop values.

Generates chain of `new_entries` and `exits` based on `entries` and
`vectorbtpro.signals.nb.ohlc_stop_place_nb`.

See `OHLCSTX` for notes on parameters.
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: ohlcstx.py
#### Docstring do Módulo
```
Module with `OHLCSTX`.
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: ohlcstx.py
#### Classe: _OHLCSTX
```
Exit signal generator based on OHLC and stop values.

Generates `exits` based on `entries` and `vectorbtpro.signals.nb.ohlc_stop_place_nb`.

!!! hint
    All parameters can be either a single value (per frame) or a NumPy array (per row, column,
    or element). To generate multiple combinations, pass them as lists.

!!! warning
    Searches for an exit after each entry. If two entries come one after another, no exit can be placed.
    Consider either cleaning up entry signals prior to passing, or using `OHLCSTCX`.

Usage:
    Test each stop type:

    ```pycon
    >>> from vectorbtpro import *

    >>> entries = pd.Series([True, False, False, False, False, False])
    >>> price = pd.DataFrame({
    ...     'open': [10, 11, 12, 11, 10, 9],
    ...     'high': [11, 12, 13, 12, 11, 10],
    ...     'low': [9, 10, 11, 10, 9, 8],
    ...     'close': [10, 11, 12, 11, 10, 9]
    ... })
    >>> ohlcstx = vbt.OHLCSTX.run(
    ...     entries,
    ...     price['open'],
    ...     price['open'],
    ...     price['high'],
    ...     price['low'],
    ...     price['close'],
    ...     sl_stop=[0.1, np.nan, np.nan, np.nan],
    ...     tsl_th=[np.nan, np.nan, 0.2, np.nan],
    ...     tsl_stop=[np.nan, 0.1, 0.3, np.nan],
    ...     tp_stop=[np.nan, np.nan, np.nan, 0.1],
    ...     is_entry_open=True
    ... )

    >>> ohlcstx.entries
    ohlcstx_sl_stop      0.1    NaN    NaN    NaN
    ohlcstx_tsl_th       NaN    NaN    0.2    NaN
    ohlcstx_tsl_stop     NaN    0.1    0.3    NaN
    ohlcstx_tp_stop      NaN    NaN    NaN    0.1
    0                   True   True   True   True
    1                  False  False  False  False
    2                  False  False  False  False
    3                  False  False  False  False
    4                  False  False  False  False
    5                  False  False  False  False

    >>> ohlcstx.exits
    ohlcstx_sl_stop      0.1    NaN    NaN    NaN
    ohlcstx_tsl_th       NaN    NaN    0.2    NaN
    ohlcstx_tsl_stop     NaN    0.1    0.3    NaN
    ohlcstx_tp_stop      NaN    NaN    NaN    0.1
    0                  False  False  False  False
    1                  False  False  False   True
    2                  False  False  False  False
    3                  False   True  False  False
    4                   True  False   True  False
    5                  False  False  False  False

    >>> ohlcstx.stop_price
    ohlcstx_sl_stop    0.1   NaN  NaN   NaN
    ohlcstx_tsl_th     NaN   NaN  0.2   NaN
    ohlcstx_tsl_stop   NaN   0.1  0.3   NaN
    ohlcstx_tp_stop    NaN   NaN  NaN   0.1
    0                  NaN   NaN  NaN   NaN
    1                  NaN   NaN  NaN  11.0
    2                  NaN   NaN  NaN   NaN
    3                  NaN  11.7  NaN   NaN
    4                  9.0   NaN  9.1   NaN
    5                  NaN   NaN  NaN   NaN

    >>> ohlcstx.stop_type_readable
    ohlcstx_sl_stop     0.1   NaN   NaN   NaN
    ohlcstx_tsl_th      NaN   NaN   0.2   NaN
    ohlcstx_tsl_stop    NaN   0.1   0.3   NaN
    ohlcstx_tp_stop     NaN   NaN   NaN   0.1
    0                  None  None  None  None
    1                  None  None  None    TP
    2                  None  None  None  None
    3                  None   TSL  None  None
    4                    SL  None   TTP  None
    5                  None  None  None  None
    ```
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: rand.py
#### Docstring do Módulo
```
Module with `RAND`.
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: rand.py
#### Classe: _RAND
```
Random entry signal generator based on the number of signals.

Generates `entries` based on `vectorbtpro.signals.nb.rand_place_nb`.

!!! hint
    Parameter `n` can be either a single value (per frame) or a NumPy array (per column).
    To generate multiple combinations, pass it as a list.

Usage:
    Test three different entry counts values:

    ```pycon
    >>> from vectorbtpro import *

    >>> rand = vbt.RAND.run(input_shape=(6,), n=[1, 2, 3], seed=42)

    >>> rand.entries
    rand_n      1      2      3
    0        True   True   True
    1       False  False   True
    2       False  False  False
    3       False   True  False
    4       False  False   True
    5       False  False  False
    ```

    Entry count can also be set per column:

    ```pycon
    >>> rand = vbt.RAND.run(input_shape=(8, 2), n=[np.array([1, 2]), 3], seed=42)

    >>> rand.entries
    rand_n      1      2      3      3
                0      1      0      1
    0       False  False   True  False
    1        True  False  False  False
    2       False  False  False   True
    3       False   True   True  False
    4       False  False  False  False
    5       False  False  False   True
    6       False  False   True  False
    7       False   True  False   True
    ```
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: randnx.py
#### Docstring do Módulo
```
Module with `RANDNX`.
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: randnx.py
#### Classe: _RANDNX
```
Random entry and exit signal generator based on the number of signals.

Generates `entries` and `exits` based on `vectorbtpro.signals.nb.rand_enex_apply_nb`.

See `RAND` for notes on parameters.

Usage:
    Test three different entry and exit counts:

    ```pycon
    >>> from vectorbtpro import *

    >>> randnx = vbt.RANDNX.run(
    ...     input_shape=(6,),
    ...     n=[1, 2, 3],
    ...     seed=42)

    >>> randnx.entries
    randnx_n      1      2      3
    0          True   True   True
    1         False  False  False
    2         False   True   True
    3         False  False  False
    4         False  False   True
    5         False  False  False

    >>> randnx.exits
    randnx_n      1      2      3
    0         False  False  False
    1          True   True   True
    2         False  False  False
    3         False   True   True
    4         False  False  False
    5         False  False   True
    ```
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: randx.py
#### Docstring do Módulo
```
Module with `RANDX`.
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: randx.py
#### Classe: _RANDX
```
Random exit signal generator based on the number of signals.

Generates `exits` based on `entries` and `vectorbtpro.signals.nb.rand_place_nb`.

See `RAND` for notes on parameters.

Usage:
    Generate an exit for each entry:

    ```pycon
    >>> from vectorbtpro import *

    >>> entries = pd.Series([True, False, False, True, False, False])
    >>> randx = vbt.RANDX.run(entries, seed=42)

    >>> randx.exits
    0    False
    1    False
    2     True
    3    False
    4     True
    5    False
    dtype: bool
    ```
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: rprob.py
#### Docstring do Módulo
```
Module with `RPROB`.
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: rprob.py
#### Classe: _RPROB
```
Random entry signal generator based on probabilities.

Generates `entries` based on `vectorbtpro.signals.nb.rand_by_prob_place_nb`.

!!! hint
    All parameters can be either a single value (per frame) or a NumPy array (per row, column,
    or element). To generate multiple combinations, pass them as lists.

Usage:
    Generate three columns with different entry probabilities:

    ```pycon
    >>> from vectorbtpro import *

    >>> rprob = vbt.RPROB.run(input_shape=(5,), prob=[0., 0.5, 1.], seed=42)

    >>> rprob.entries
    rprob_prob    0.0    0.5   1.0
    0           False   True  True
    1           False   True  True
    2           False  False  True
    3           False  False  True
    4           False  False  True
    ```

    Probability can also be set per row, column, or element:

    ```pycon
    >>> rprob = vbt.RPROB.run(input_shape=(5,), prob=np.array([0., 0., 1., 1., 1.]), seed=42)

    >>> rprob.entries
    0    False
    1    False
    2     True
    3     True
    4     True
    Name: array_0, dtype: bool
    ```
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: rprobcx.py
#### Docstring do Módulo
```
Module with `RPROBCX`.
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: rprobcx.py
#### Classe: _RPROBCX
```
Random exit signal generator based on probabilities.

Generates chain of `new_entries` and `exits` based on `entries` and
`vectorbtpro.signals.nb.rand_by_prob_place_nb`.

See `RPROB` for notes on parameters.
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: rprobnx.py
#### Docstring do Módulo
```
Module with `RPROBNX`.
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: rprobnx.py
#### Classe: _RPROBNX
```
Random entry and exit signal generator based on probabilities.

Generates `entries` and `exits` based on `vectorbtpro.signals.nb.rand_by_prob_place_nb`.

See `RPROB` for notes on parameters.

Usage:
    Test all probability combinations:

    ```pycon
    >>> from vectorbtpro import *

    >>> rprobnx = vbt.RPROBNX.run(
    ...     input_shape=(5,),
    ...     entry_prob=[0.5, 1.],
    ...     exit_prob=[0.5, 1.],
    ...     param_product=True,
    ...     seed=42)

    >>> rprobnx.entries
    rprobnx_entry_prob    0.5    0.5    1.0    0.5
    rprobnx_exit_prob     0.5    1.0    0.5    1.0
    0                    True   True   True   True
    1                   False  False  False  False
    2                   False  False  False   True
    3                   False  False  False  False
    4                   False  False   True   True

    >>> rprobnx.exits
    rprobnx_entry_prob    0.5    0.5    1.0    1.0
    rprobnx_exit_prob     0.5    1.0    0.5    1.0
    0                   False  False  False  False
    1                   False   True  False   True
    2                   False  False  False  False
    3                   False  False   True   True
    4                    True  False  False  False
    ```

    Probabilities can also be set per row, column, or element:

    ```pycon
    >>> entry_prob1 = np.array([1., 0., 1., 0., 1.])
    >>> entry_prob2 = np.array([0., 1., 0., 1., 0.])
    >>> rprobnx = vbt.RPROBNX.run(
    ...     input_shape=(5,),
    ...     entry_prob=[entry_prob1, entry_prob2],
    ...     exit_prob=1.,
    ...     seed=42)

    >>> rprobnx.entries
    rprobnx_entry_prob array_0 array_1
    rprobnx_exit_prob      1.0     1.0
    0                     True   False
    1                    False    True
    2                     True   False
    3                    False    True
    4                     True   False

    >>> rprobnx.exits
    rprobnx_entry_prob array_0 array_1
    rprobnx_exit_prob      1.0     1.0
    0                    False   False
    1                     True   False
    2                    False    True
    3                     True   False
    4                    False    True
    ```
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: rprobx.py
#### Docstring do Módulo
```
Module with `RPROBX`.
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: rprobx.py
#### Classe: _RPROBX
```
Random exit signal generator based on probabilities.

Generates `exits` based on `entries` and `vectorbtpro.signals.nb.rand_by_prob_place_nb`.

See `RPROB` for notes on parameters.
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: stcx.py
#### Docstring do Módulo
```
Module with `STCX`.
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: stcx.py
#### Classe: _STCX
```
Exit signal generator based on stop values.

Generates chain of `new_entries` and `exits` based on `entries` and
`vectorbtpro.signals.nb.stop_place_nb`.

See `STX` for notes on parameters.
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: stx.py
#### Docstring do Módulo
```
Module with `STX`.
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: stx.py
#### Classe: _STX
```
Exit signal generator based on stop values.

Generates `exits` based on `entries` and `vectorbtpro.signals.nb.stop_place_nb`.

!!! hint
    All parameters can be either a single value (per frame) or a NumPy array (per row, column,
    or element). To generate multiple combinations, pass them as lists.
```

---

# Pasta: signals
## Subpasta: generators
### Arquivo: __init__.py
#### Docstring do Módulo
```
Custom signal generators built with the signal factory.

You can access all the indicators by `vbt.*`.
```

---

# Pasta: utils
### Arquivo: annotations.py
#### Docstring do Módulo
```
Utilities for annotations.
```

---

# Pasta: utils
### Arquivo: annotations.py
#### Função: get_annotations
```
Get annotations.
```

---

# Pasta: utils
### Arquivo: annotations.py
#### Função: flatten_annotations
```
Flatten annotations of variable arguments.
```

---

# Pasta: utils
### Arquivo: annotations.py
#### Classe: MetaAnnotatable
```
Metaclass that can be used in annotations.
```

---

# Pasta: utils
### Arquivo: annotations.py
#### Classe: Annotatable
```
Class that can be used in annotations.
```

---

# Pasta: utils
### Arquivo: annotations.py
#### Função: has_annotatables
```
Check if a function has subclasses or instances of `Annotatable` in its signature.
```

---

# Pasta: utils
### Arquivo: annotations.py
#### Classe: VarArgs
```
Class representing annotations for variable positional arguments.
```

---

# Pasta: utils
### Arquivo: annotations.py
#### Classe: VarKwargs
```
Class representing annotations for variable keyword arguments.
```

---

# Pasta: utils
### Arquivo: annotations.py
#### Classe: Union
```
Class representing a union of one to multiple annotations.
```

---

# Pasta: utils
### Arquivo: annotations.py
#### Classe: Union
#### Função: resolve
```
Resolve the union.
```

---

# Pasta: utils
### Arquivo: annotations.py
#### Função: get_raw_annotations
```
A backport of Python 3.10's inspect.get_annotations() function.

See https://github.com/python/cpython/blob/main/Lib/inspect.py
```

---

# Pasta: utils
### Arquivo: array_.py
#### Docstring do Módulo
```
Utilities for working with arrays.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: is_sorted
```
Checks if array is sorted.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: is_sorted_nb
```
Numba-compiled version of `is_sorted`.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: is_range
```
Checks if array is arr range.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: is_range_nb
```
Numba-compiled version of `is_range`.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: insert_argsort_nb
```
Perform argsort using insertion sort.

In-memory and without recursion -> very fast for smaller arrays.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: get_ranges_arr
```
Build array from start and end indices.

Based on https://stackoverflow.com/a/37626057
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: uniform_summing_to_one_nb
```
Generate random floats summing to one.

See # https://stackoverflow.com/a/2640067/8141780
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: rescale
```
Renormalize `arr` from one range to another.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: rescale_nb
```
Numba-compiled version of `rescale`.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: min_rel_rescale
```
Rescale elements in `arr` relatively to minimum.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: max_rel_rescale
```
Rescale elements in `arr` relatively to maximum.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: rescale_float_to_int_nb
```
Rescale a float array into an int array.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: int_digit_count_nb
```
Get the digit count in a number.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: hash_int_rows_nb
```
Hash rows in a 2-dim array.

First digits of each hash correspond to the left-most column, the last digits to the right-most column.
Thus, the resulting hashes are not suitable for sorting by value.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: index_repeating_rows_nb
```
Index repeating rows using monotonically increasing numbers.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: build_nan_mask
```
Build a NaN mask out of one to multiple arrays via the OR rule.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: squeeze_nan
```
Squeeze NaN values using a mask.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: unsqueeze_nan
```
Un-squeeze NaN values using a mask.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: cast_to_min_precision
```
Cast an array to a minimum integer/floating precision.

Argument must be either an integer denoting the number of bits,
or one of 'half', 'single', and 'double'.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: cast_to_max_precision
```
Cast an array to a maximum integer/floating precision.

Argument must be either an integer denoting the number of bits,
or one of 'half', 'single', and 'double'.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: min_count_nb
```
Get the first position, the value, and the count of the array's minimum.
```

---

# Pasta: utils
### Arquivo: array_.py
#### Função: max_count_nb
```
Get the first position, the value, and the count of the array's maximum.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Docstring do Módulo
```
Utilities for working with class/instance attributes.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: _Missing
```
Sentinel that represents a missing value.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: DefineMixin
```
Mixin class for `define`.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: define
```
Prepare a class decorated with `define`.

Attaches `DefineMixin` as a base class (if not present) and applies `attr.define`.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Função: get_dict_attr
```
Get attribute without invoking the attribute lookup machinery.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Função: default_getattr_func
```
Default `getattr_func`.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Função: deep_getattr
```
Retrieve attribute consecutively.

The attribute chain `attr_chain` can be:

* string -> get variable/property or method without arguments
* tuple of string -> call method without arguments
* tuple of string and tuple -> call method and pass positional arguments (unpacked)
* tuple of string, tuple, and dict -> call method and pass positional and keyword arguments (unpacked)
* iterable of any of the above

Use `getattr_func` to overwrite the default behavior of accessing an attribute (see `default_getattr_func`).

!!! hint
    If your chain includes only attributes and functions without arguments,
    you can represent this chain as a single (but probably long) string.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: AttrResolverMixin
```
Class that implements resolution of self and its attributes.

Resolution is `getattr` that works for self, properties, and methods. It also utilizes built-in caching.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Função: parse_attrs
```
Parse attributes of a class, object, or a module, and return a DataFrame with types and paths.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: DefineMixin
#### Função: fields
```
Get a tuple of fields.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: DefineMixin
#### Função: fields_dict
```
Get a dict of fields.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: DefineMixin
#### Função: get_field
```
Get field.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: DefineMixin
#### Função: is_field_required
```
Return whether a field is required.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: DefineMixin
#### Função: is_field_optional
```
Return whether a field is optional.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: DefineMixin
#### Função: resolve_field
```
Resolve a field.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: DefineMixin
#### Função: is_field_missing
```
Raise an error if a field is missing.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: DefineMixin
#### Função: assert_field_not_missing
```
Raise an error if a field is missing.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: DefineMixin
#### Função: resolve
```
Resolve a field or all fields.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: DefineMixin
#### Função: asdict
```
Convert this instance to a dictionary.

If `full` is False, won't include fields with `MISSING` as value.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: DefineMixin
#### Função: replace
```
Create a new instance by making changes to the attribute values.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: DefineMixin
#### Função: merge_with
```
Merge this instance with another instance.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: DefineMixin
#### Função: merge_over
```
Merge this instance over another instance.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: define
#### Função: field
```
Alias for `attr.field`.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: define
#### Função: required_field
```
Alias for `attr.field` with `MISSING` as default.

Doesn't have `default` in metadata.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: define
#### Função: optional_field
```
Alias for `attr.field` with `MISSING` as default.

Has `default` in metadata.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: AttrResolverMixin
#### Função: self_aliases
```
Names to associate with this object.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: AttrResolverMixin
#### Função: resolve_self
```
Resolve self.

!!! note
    `cond_kwargs` can be modified in-place.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: AttrResolverMixin
#### Função: pre_resolve_attr
```
Pre-process an attribute before resolution.

Must return an attribute.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: AttrResolverMixin
#### Função: post_resolve_attr
```
Post-process an object after resolution.

Must return an object.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: AttrResolverMixin
#### Função: cls_dir
```
Get set of attribute names.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: AttrResolverMixin
#### Função: resolve_shortcut_attr
```
Resolve an attribute that may have shortcut properties.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: AttrResolverMixin
#### Função: resolve_attr
```
Resolve an attribute using keyword arguments and built-in caching.

* If there is a `get_{arg}` method, uses `get_{arg}` as `attr`.
* If `attr` is a property, returns its value.
* If `attr` is a method, passes `*args`, `**kwargs`, and `**cond_kwargs` with keys found in the signature.
* If `use_shortcuts` is True, resolves the potential shortcuts using `AttrResolverMixin.resolve_shortcut_attr`.

Won't cache if `use_caching` is False or any passed argument is in `custom_arg_names`.

Use `passed_kwargs_out` to get keyword arguments that were passed.
```

---

# Pasta: utils
### Arquivo: attr_.py
#### Classe: AttrResolverMixin
#### Função: deep_getattr
```
See `deep_getattr`.
```

---

# Pasta: utils
### Arquivo: caching.py
#### Docstring do Módulo
```
Utilities for caching.
```

---

# Pasta: utils
### Arquivo: caching.py
#### Função: clear_pycache
```
Clear __pycache__ folders and .pyc files.
```

---

# Pasta: utils
### Arquivo: caching.py
#### Classe: Cacheable
```
Class that contains cacheable properties and methods.

Required to register `vectorbtpro.utils.decorators.cacheable_property` and
`vectorbtpro.utils.decorators.cacheable_method`.

See `vectorbtpro.registries.ca_registry` for details on the caching procedure.
```

---

# Pasta: utils
### Arquivo: caching.py
#### Classe: Cacheable
#### Função: get_ca_setup
```
Get instance setup of type `vectorbtpro.registries.ca_registry.CAInstanceSetup` if the instance method
was called and class setup of type `vectorbtpro.registries.ca_registry.CAClassSetup` otherwise.
```

---

# Pasta: utils
### Arquivo: chaining.py
#### Docstring do Módulo
```
Utilities for chaining.
```

---

# Pasta: utils
### Arquivo: chaining.py
#### Classe: Chainable
```
Class representing an object that can be chained.
```

---

# Pasta: utils
### Arquivo: chaining.py
#### Classe: Chainable
#### Função: pipe
```
Apply a chainable function that expects a `Chainable` instance.

Can be called as a class method, but then will pass only `*args` and `**kwargs`.

Argument `func` can be a function, a string denoting a (deep) attribute to be resolved
with `vectorbtpro.utils.attr_.deep_getattr`, or a tuple where the first element is one
of the above and the second element is a positional argument or keyword argument where
to pass the instance. If not a tuple, passes the instance as the first positional argument.
If a string and the target function is an instance method, won't pass the instance since
it's already bound to this instance.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Docstring do Módulo
```
Utilities for validation during runtime.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Classe: Comparable
```
Class representing an object that can be compared to another object.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_bool
```
Check whether the argument is a bool.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_int
```
Check whether the argument is an integer (and not a timedelta, for example).
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_float
```
Check whether the argument is a float.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_number
```
Check whether the argument is a number.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_np_scalar
```
Check whether the argument is a NumPy scalar.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_td
```
Check whether the argument is a timedelta object.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_td_like
```
Check whether the argument is a timedelta-like object.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_frequency
```
Check whether the argument is a frequency object.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_frequency_like
```
Check whether the argument is a frequency-like object.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_dt
```
Check whether the argument is a datetime object.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_dt_like
```
Check whether the argument is a datetime-like object.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_time
```
Check whether the argument is a time object.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_time_like
```
Check whether the argument is a time-like object.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_np_array
```
Check whether the argument is a NumPy array.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_record_array
```
Check whether the argument is a structured NumPy array.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_series
```
Check whether the argument is `pd.Series`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_index
```
Check whether the argument is `pd.Index`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_multi_index
```
Check whether the argument is `pd.MultiIndex`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_frame
```
Check whether the argument is `pd.DataFrame`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_pandas
```
Check whether the argument is `pd.Series`, `pd.Index`, or `pd.DataFrame`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_any_array
```
Check whether the argument is a NumPy array or a Pandas object.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: _to_any_array
```
Convert any array-like object to an array.

Pandas objects are kept as-is.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_sequence
```
Check whether the argument is a sequence.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_complex_sequence
```
Check whether the argument is a sequence but not a string or bytes object.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_iterable
```
Check whether the argument is iterable.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_complex_iterable
```
Check whether the argument is iterable but not a string or bytes object.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_numba_enabled
```
Check whether Numba is enabled globally.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_numba_func
```
Check whether the argument is a Numba-compiled function.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_hashable
```
Check whether the argument can be hashed.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_index_equal
```
Check whether indexes are equal.

If `check_names` is True, checks whether names are equal on top of `pd.Index.equals`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_default_index
```
Check whether index is a basic range.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_namedtuple
```
Check whether object is an instance of namedtuple.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_record
```
Check whether object is a NumPy record.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: func_accepts_arg
```
Check whether `func` accepts a positional or keyword argument with name `arg_name`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_equal
```
Check whether two objects are equal.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_deep_equal
```
Check whether two objects are equal (deep check).
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_class
```
Check whether the argument is `types`.

`types` can be one or multiple types, strings, or patterns of type `vectorbtpro.utils.parsing.Regex`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_subclass_of
```
Check whether the argument is a subclass of `types`.

`types` can be one or multiple types, strings, or patterns of type `vectorbtpro.utils.parsing.Regex`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_instance_of
```
Check whether the argument is an instance of `types`.

`types` can be one or multiple types or strings.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_mapping
```
Check whether the arguments is a mapping.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_mapping_like
```
Check whether the arguments is a mapping-like object.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_valid_variable_name
```
Check whether the argument is a valid variable name.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: is_notebook
```
Check whether the code runs in a notebook.

Credit: https://stackoverflow.com/a/39662359
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_in
```
Raise exception if the first argument is not in the second argument.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_numba_func
```
Raise exception if `func` is not Numba-compiled.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_not_none
```
Raise exception if the argument is None.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_instance_of
```
Raise exception if the argument is none of types `types`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_not_instance_of
```
Raise exception if the argument is one of types `types`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_subclass_of
```
Raise exception if the argument is not a subclass of classes `classes`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_not_subclass_of
```
Raise exception if the argument is a subclass of classes `classes`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_type_equal
```
Raise exception if the first argument and the second argument have different types.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_dtype
```
Raise exception if the argument is not of data type `dtype`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_subdtype
```
Raise exception if the argument is not a sub data type of `dtype`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_dtype_equal
```
Raise exception if the first argument and the second argument have different data types.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_ndim
```
Raise exception if the argument has a different number of dimensions than `ndims`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_len_equal
```
Raise exception if the first argument and the second argument have different length.

Does not transform arguments to NumPy arrays.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_shape_equal
```
Raise exception if the first argument and the second argument have different shapes along `axis`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_index_equal
```
Raise exception if the first argument and the second argument have different index.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_columns_equal
```
Raise exception if the first argument and the second argument have different columns.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_meta_equal
```
Raise exception if the first argument and the second argument have different metadata.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_array_equal
```
Raise exception if the first argument and the second argument have different metadata or values.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_level_not_exists
```
Raise exception if index the argument has level `level_name`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_equal
```
Raise exception if the first argument and the second argument are different.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_dict_valid
```
Raise exception if dict the argument has keys that are not in `lvl_keys`.

`lvl_keys` must be a list of lists, each corresponding to a level in the dict.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_dict_sequence_valid
```
Raise exception if a dict or any dict in a sequence of dicts has keys that are not in `lvl_keys`.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_sequence
```
Raise exception if the argument is not a sequence.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Função: assert_iterable
```
Raise exception if the argument is not an iterable.
```

---

# Pasta: utils
### Arquivo: checks.py
#### Classe: Comparable
#### Função: equals
```
Check two objects for (deep) equality.

Should accept the keyword arguments accepted by `is_deep_equal`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Docstring do Módulo
```
Utilities for chunking.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ArgGetter
```
Class for getting an argument from annotated arguments.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: AxisSpecifier
```
Class with an attribute for specifying an axis.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: DimRetainer
```
Class with an attribute for retaining dimensions.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Sizer
```
Abstract class for getting the size from annotated arguments.

!!! note
    Use `Sizer.apply` instead of `Sizer.get_size`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ArgSizer
```
Class for getting the size from an argument.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: CountSizer
```
Class for getting the size from a count.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: LenSizer
```
Class for getting the size from the length of an argument.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ShapeSizer
```
Class for getting the size from the length of an axis in a shape.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ArraySizer
```
Class for getting the size from the length of an axis in an array.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ChunkMeta
```
Class that represents a chunk metadata.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ChunkMetaGenerator
```
Abstract class for generating chunk metadata from annotated arguments.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ArgChunkMeta
```
Class for generating chunk metadata from an argument.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: LenChunkMeta
```
Class for generating chunk metadata from a sequence of chunk lengths.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Função: yield_chunk_meta
```
Yield meta of each successive chunk from a sequence with a number of elements.

Args:
    size (int): Size of the space to split.
    min_size (int): Minimum size.

        If `size` is lower than this number, returns a single chunk.
    n_chunks (int or str): Number of chunks.

        If "auto", becomes the number of cores.
    chunk_len (int or str): Length of each chunk.

        If "auto", becomes the number of cores.

If `size`, `n_chunks`, and `chunk_len` are None (after resolving them from settings),
returns a single chunk. If only `n_chunks` and `chunk_len` are None, sets `n_chunks` to "auto".
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ChunkMapper
```
Abstract class for mapping chunk metadata.

Implements the abstract `ChunkMapper.map` method.

Supports caching of each pair of incoming and outgoing `ChunkMeta` instances.

!!! note
    Use `ChunkMapper.apply` instead of `ChunkMapper.map`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: NotChunked
```
Class that represents an argument that shouldn't be chunked.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ChunkTaker
```
Abstract class for taking one or more elements based on the chunk index or range.

!!! note
    Use `ChunkTaker.apply` instead of `ChunkTaker.take`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ChunkSelector
```
Class for selecting one element based on the chunk index.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ChunkSlicer
```
Class for slicing multiple elements based on the chunk range.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: CountAdapter
```
Class for adapting a count based on the chunk range.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ShapeSelector
```
Class for selecting one element from a shape's axis based on the chunk index.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ShapeSlicer
```
Class for slicing multiple elements from a shape's axis based on the chunk range.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ArraySelector
```
Class for selecting one element from an array's axis based on the chunk index.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ArraySlicer
```
Class for slicing multiple elements from an array's axis based on the chunk range.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ContainerTaker
```
Class for taking from a container with other chunk takers.

Accepts the specification of the container.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: SequenceTaker
```
Class for taking from a sequence container.

Calls `Chunker.take_from_arg` on each element.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: MappingTaker
```
Class for taking from a mapping container.

Calls `Chunker.take_from_arg` on each element.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ArgsTaker
```
Class for taking from a variable arguments container.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: KwargsTaker
```
Class for taking from a variable keyword arguments container.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunkable
```
Abstract class representing a value and a chunk taking specification.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunked
```
Class representing a chunkable value.

Can take a variable number of keyword arguments, which will be used as `Chunked.take_spec_kwargs`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ChunkedCount
```
Class representing a chunkable count.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ChunkedShape
```
Class representing a chunkable shape.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ChunkedArray
```
Class representing a chunkable array.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
```
Class responsible for chunking arguments of a function and running the function.

Does the following:

1. Generates chunk metadata by passing `n_chunks`, `size`, `min_size`, `chunk_len`,
    and `chunk_meta` to `Chunker.get_chunk_meta_from_args`.
2. Splits arguments and keyword arguments by passing chunk metadata, `arg_take_spec`,
    and `template_context` to `Chunker.yield_tasks`, which yields one chunk at a time.
3. Executes all chunks by passing `**execute_kwargs` to `vectorbtpro.utils.execution.execute`.
4. Optionally, post-processes and merges the results by passing them and
    `**merge_kwargs` to `merge_func`.

For defaults, see `vectorbtpro._settings.chunking`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Função: chunked
```
Decorator that chunks the inputs of a function using `Chunker`.

Returns a new function with the same signature as the passed one.

Each option can be modified in the `options` attribute of the wrapper function or
directly passed as a keyword argument with a leading underscore.

Keyword arguments `**kwargs` and `execute_kwargs` are merged into `execute_kwargs`
if `merge_to_execute_kwargs` is True, otherwise, `**kwargs` are passed directly to `Chunker`.

Chunking can be disabled using `disable` argument. Additionally, the entire wrapping mechanism
can be disabled by using the global setting `disable_wrapping` (=> returns the wrapped function).

Usage:
    For testing purposes, let's divide the input array into 2 chunks and compute
    the mean in a sequential manner:

    ```pycon
    >>> from vectorbtpro import *

    >>> @vbt.chunked(
    ...     n_chunks=2,
    ...     size=vbt.LenSizer(arg_query='a'),
    ...     arg_take_spec=dict(a=vbt.ChunkSlicer())
    ... )
    ... def f(a):
    ...     return np.mean(a)

    >>> f(np.arange(10))
    [2.0, 7.0]
    ```

    Same can be done using annotations:

    ```pycon
    >>> @vbt.chunked(n_chunks=2)
    ... def f(a: vbt.LenSizer() | vbt.ChunkSlicer()):
    ...     return np.mean(a)

    >>> f(np.arange(10))
    [2.0, 7.0]
    ```

    Sizer can be omitted most of the time:

    ```pycon
    >>> @vbt.chunked(n_chunks=2)
    ... def f(a: vbt.ChunkSlicer()):
    ...     return np.mean(a)

    >>> f(np.arange(10))
    [2.0, 7.0]
    ```

    Another way is by using specialized `Chunker` subclasses that depend on the type of the argument:

    ```pycon
    >>> @vbt.chunked(n_chunks=2)
    ... def f(a: vbt.ChunkedArray()):
    ...     return np.mean(a)

    >>> f(np.arange(10))
    ```

    Also, instead of specifying the chunk taking specification beforehand, it can be passed
    dynamically by wrapping each value to be chunked with `Chunked` or any of its subclasses:

    ```pycon
    >>> @vbt.chunked(n_chunks=2)
    ... def f(a):
    ...     return np.mean(a)

    >>> f(vbt.ChunkedArray(np.arange(10)))
    [2.0, 7.0]
    ```

    The `chunked` function is a decorator that takes `f` and creates a function that splits
    passed arguments, runs each chunk using an engine, and optionally, merges the results.
    It has the same signature as the original function:

    ```pycon
    >>> f
    <function __main__.f(a)>
    ```

    We can change any option at any time:

    ```pycon
    >>> # Change the option directly on the function
    >>> f.options.n_chunks = 3

    >>> f(np.arange(10))
    [1.5, 5.0, 8.0]

    >>> # Pass a new option with a leading underscore
    >>> f(np.arange(10), _n_chunks=4)
    [1.0, 4.0, 6.5, 8.5]
    ```

    When we run the wrapped function, it first generates a list of chunk metadata of type `ChunkMeta`.
    Chunk metadata contains the chunk index that can be used to split any input:

    ```pycon
    >>> list(vbt.yield_chunk_meta(n_chunks=2))
    [ChunkMeta(uuid='84d64eed-fbac-41e7-ad61-c917e809b3b8', idx=0, start=None, end=None, indices=None),
     ChunkMeta(uuid='577817c4-fdee-4ceb-ab38-dcd663d9ab11', idx=1, start=None, end=None, indices=None)]
    ```

    Additionally, it may contain the start and end index of the space we want to split.
    The space can be defined by the length of an input array, for example. In our case:

    ```pycon
    >>> list(vbt.yield_chunk_meta(n_chunks=2, size=10))
    [ChunkMeta(uuid='c1593842-dc31-474c-a089-e47200baa2be', idx=0, start=0, end=5, indices=None),
     ChunkMeta(uuid='6d0265e7-1204-497f-bc2c-c7b7800ec57d', idx=1, start=5, end=10, indices=None)]
    ```

    If we know the size of the space in advance, we can pass it as an integer constant.
    Otherwise, we need to tell `chunked` to derive the size from the inputs dynamically
    by passing any subclass of `Sizer`. In the example above, we instruct the wrapped function
    to derive the size from the length of the input array `a`.

    Once all chunks are generated, the wrapped function attempts to split inputs into chunks.
    The specification for this operation can be provided by the `arg_take_spec` argument, which
    in most cases is a dictionary of `ChunkTaker` instances keyed by the input name.
    Here's an example of a complex specification:

    ```pycon
    >>> arg_take_spec = dict(
    ...     a=vbt.ChunkSelector(),
    ...     args=vbt.ArgsTaker(
    ...         None,
    ...         vbt.ChunkSelector()
    ...     ),
    ...     b=vbt.SequenceTaker([
    ...         None,
    ...         vbt.ChunkSelector()
    ...     ]),
    ...     kwargs=vbt.KwargsTaker(
    ...         c=vbt.MappingTaker(dict(
    ...             d=vbt.ChunkSelector(),
    ...             e=None
    ...         ))
    ...     )
    ... )

    >>> @vbt.chunked(
    ...     n_chunks=vbt.LenSizer(arg_query='a'),
    ...     arg_take_spec=arg_take_spec
    ... )
    ... def f(a, *args, b=None, **kwargs):
    ...     return a + sum(args) + sum(b) + sum(kwargs['c'].values())

    >>> f([1, 2, 3], 10, [1, 2, 3], b=(100, [1, 2, 3]), c=dict(d=[1, 2, 3], e=1000))
    [1114, 1118, 1122]
    ```

    After splitting all inputs into chunks, the wrapped function forwards them to the engine function.
    The engine argument can be either the name of a supported engine, or a callable. Once the engine
    has finished all tasks and returned a list of results, we can merge them back using `merge_func`:

    ```pycon
    >>> @vbt.chunked(
    ...     n_chunks=2,
    ...     size=vbt.LenSizer(arg_query='a'),
    ...     arg_take_spec=dict(a=vbt.ChunkSlicer()),
    ...     merge_func="concat"
    ... )
    ... def f(a):
    ...     return a

    >>> f(np.arange(10))
    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
    ```

    The same using annotations:

    ```pycon
    >>> @vbt.chunked(n_chunks=2)
    ... def f(a: vbt.ChunkSlicer()) -> vbt.MergeFunc("concat"):
    ...     return a

    >>> f(np.arange(10))
    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
    ```

    Instead of (or in addition to) specifying `arg_take_spec`, we can define our function with the
    first argument being `chunk_meta` to be able to split the arguments during the execution.
    The `chunked` decorator will automatically recognize and replace it with the actual `ChunkMeta` object:

    ```pycon
    >>> @vbt.chunked(
    ...     n_chunks=2,
    ...     size=vbt.LenSizer(arg_query='a'),
    ...     arg_take_spec=dict(a=None),
    ...     merge_func="concat"
    ... )
    ... def f(chunk_meta, a):
    ...     return a[chunk_meta.start:chunk_meta.end]

    >>> f(np.arange(10))
    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
    ```

    This may be a good idea in multi-threading, but a bad idea in multi-processing.

    The same can be accomplished by using templates (here we tell `chunked` to not replace
    the first argument by setting `prepend_chunk_meta` to False):

    ```pycon
    >>> @vbt.chunked(
    ...     n_chunks=2,
    ...     size=vbt.LenSizer(arg_query='a'),
    ...     arg_take_spec=dict(a=None),
    ...     merge_func="concat",
    ...     prepend_chunk_meta=False
    ... )
    ... def f(chunk_meta, a):
    ...     return a[chunk_meta.start:chunk_meta.end]

    >>> f(vbt.Rep('chunk_meta'), np.arange(10))
    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
    ```

    Templates in arguments are substituted right before taking a chunk from them.

    Keyword arguments to the engine can be provided using `execute_kwargs`:

    ```pycon
    >>> @vbt.chunked(
    ...     n_chunks=2,
    ...     size=vbt.LenSizer(arg_query='a'),
    ...     arg_take_spec=dict(a=vbt.ChunkSlicer()),
    ...     show_progress=True
    ... )
    ... def f(a):
    ...     return np.mean(a)

    >>> f(np.arange(10))
    100% |█████████████████████████████████| 2/2 [00:00<00:00, 81.11it/s]
    [2.0, 7.0]
    ```
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Função: resolve_chunked_option
```
Return keyword arguments for `chunked`.

`option` can be:

* True: Chunk using default settings
* None or False: Do not chunk
* string: Use `option` as the name of an execution engine (see `vectorbtpro.utils.execution.execute`)
* dict: Use `option` as keyword arguments passed to `chunked`

For defaults, see `option` in `vectorbtpro._settings.chunking`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Função: specialize_chunked_option
```
Resolve `option` and merge it with `kwargs` if it's not None so the dict can be passed
as an option to other functions.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Função: resolve_chunked
```
Decorate with `chunked` based on an option.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ArgGetter
#### Função: get_arg
```
Get argument using `vectorbtpro.utils.parsing.match_ann_arg`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Sizer
#### Função: get_size
```
Get the size given the annotated arguments.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Sizer
#### Função: apply
```
Apply the sizer.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: CountSizer
#### Função: get_obj_size
```
Get size of an object.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: LenSizer
#### Função: get_obj_size
```
Get size of an object.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ShapeSizer
#### Função: get_obj_size
```
Get size of an object.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ArraySizer
#### Função: get_obj_size
```
Get size of an object.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ChunkMetaGenerator
#### Função: get_chunk_meta
```
Get chunk metadata.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ChunkMapper
#### Função: apply
```
Apply the mapper.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ChunkMapper
#### Função: map
```
Abstract method for mapping chunk metadata.

Takes the chunk metadata of type `ChunkMeta` and returns a new chunk metadata of the same type.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ChunkTaker
#### Função: get_size
```
Get the actual size of the argument.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ChunkTaker
#### Função: suggest_size
```
Suggest a global size based on the argument's size.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ChunkTaker
#### Função: should_take
```
Check whether to take a chunk or leave the argument as it is.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ChunkTaker
#### Função: apply
```
Apply the taker.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ChunkTaker
#### Função: take
```
Abstract method for taking subset of data.

Takes the argument object, the chunk meta (tuple out of the index, start index,
and end index of the chunk), and other keyword arguments passed down the stack,
such as `chunker` and `silence_warnings`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: ContainerTaker
#### Função: check_cont_take_spec
```
Check that `ContainerTaker.cont_take_spec` is not None.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: SequenceTaker
#### Função: adapt_cont_take_spec
```
Prepare the specification of the container to the object.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: MappingTaker
#### Função: adapt_cont_take_spec
```
Prepare the specification of the container to the object.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunkable
#### Função: get_value
```
Get the value.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunkable
#### Função: get_take_spec
```
Get the chunk taking specification.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunked
#### Função: take_spec_missing
```
Check whether `Chunked.take_spec` is missing.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunked
#### Função: resolve_take_spec
```
Resolve `take_spec`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: func
```
Function.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: size
```
See `Chunker.get_chunk_meta_from_args`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: min_size
```
See `Chunker.get_chunk_meta_from_args`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: n_chunks
```
See `Chunker.get_chunk_meta_from_args`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: chunk_len
```
See `Chunker.get_chunk_meta_from_args`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: chunk_meta
```
See `Chunker.get_chunk_meta_from_args`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: prepend_chunk_meta
```
Whether to prepend an instance of `ChunkMeta` to the arguments.

If None, prepends automatically if the first argument is named 'chunk_meta'.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: skip_single_chunk
```
Whether to execute the function directly if there's only one chunk.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: arg_take_spec
```
See `yield_tasks`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: template_context
```
Template context.

Any template in both `execute_kwargs` and `merge_kwargs` will be substituted. You can use
the keys `ann_args`, `chunk_meta`, `arg_take_spec`, and `tasks` to be replaced by
the actual objects.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: merge_func
```
Merging function.

Resolved using `vectorbtpro.base.merging.resolve_merge_func`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: merge_kwargs
```
Keyword arguments passed to the merging function.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: return_raw_chunks
```
Whether to return chunks in a raw format.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: silence_warnings
```
Whether to silence any warnings.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: forward_kwargs_as
```
Map to rename keyword arguments.

Can also pass any variable from the scope of `Chunker.run`
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: execute_kwargs
```
Keyword arguments passed to `vectorbtpro.utils.execution.execute`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: disable
```
Whether to disable chunking.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: get_chunk_meta_from_args
```
Get chunk metadata from annotated arguments.

Args:
    ann_args (dict): Arguments annotated with `vectorbtpro.utils.parsing.annotate_args`.
    size (int, Sizer, or callable): See `yield_chunk_meta`.

        Can be an integer, an instance of `Sizer`, or a callable taking
        the annotated arguments and returning a value.
    min_size (int): See `yield_chunk_meta`.
    n_chunks (int, str, Sizer, or callable): See `yield_chunk_meta`.

        Can be an integer, a string, an instance of `Sizer`, or a callable taking
        the annotated arguments and other keyword arguments and returning a value.
    chunk_len (int, str, Sizer, or callable): See `yield_chunk_meta`.

        Can be an integer, a string, an instance of `Sizer`, or a callable taking
        the annotated arguments and returning a value.
    chunk_meta (iterable of ChunkMeta, ChunkMetaGenerator, or callable): Chunk meta.

        Can be an iterable of `ChunkMeta`, an instance of `ChunkMetaGenerator`, or
        a callable taking the annotated arguments and other arguments and returning an iterable.
    **kwargs: Other keyword arguments passed to any callable.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: resolve_take_spec
```
Resolve the chunk taking specification.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: take_from_arg
```
Take from the argument given the specification `take_spec`.

If `take_spec` is None or it's an instance of `NotChunked`, returns the original object.
Otherwise, must be an instance of `ChunkTaker`.

`**kwargs` are passed to `ChunkTaker.apply`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: find_take_spec
```
Resolve the specification for an argument.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: take_from_args
```
Take from each in the annotated arguments given the specification using `Chunker.take_from_arg`.

Additionally, passes to `Chunker.take_from_arg` as keyword arguments `ann_args` and `arg_take_spec`.

`arg_take_spec` must be a dictionary, with keys being argument positions or names as generated by
`vectorbtpro.utils.parsing.annotate_args`. For values, see `Chunker.take_from_arg`.

Returns arguments and keyword arguments that can be directly passed to the function
using `func(*args, **kwargs)`.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: yield_tasks
```
Split annotated arguments into chunks using `Chunker.take_from_args` and yield each chunk as a task.

Args:
    func (callable): Callable.
    ann_args (dict): Arguments annotated with `vectorbtpro.utils.parsing.annotate_args`.
    chunk_meta (iterable of ChunkMeta): Chunk metadata.
    arg_take_spec (mapping, sequence, callable, or CustomTemplate): Chunk taking specification.

        Can be a dictionary (see `Chunker.take_from_args`), or a sequence that will be
        converted into a dictionary. If a callable, will be called instead of `Chunker.take_from_args`,
        thus it must have the same arguments apart from `arg_take_spec`.
    template_context (mapping): Context used to substitute templates in arguments and specification.
    **kwargs: Keyword arguments passed to `Chunker.take_from_args` or to `arg_take_spec`
        if it's a callable.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: parse_sizer_from_func
```
Parse the sizer from a function.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: parse_spec_from_annotations
```
Parse the chunk taking specification from annotations.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: parse_spec_from_func
```
Parse the chunk taking specification from a function.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: parse_spec_from_args
```
Parse the chunk taking specification from (annotated) arguments.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: fill_arg_take_spec
```
Fill the chunk taking specification with None to avoid warnings.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: adapt_ann_args
```
Adapt annotated arguments.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: suggest_size
```
Suggest a global size given the annotated arguments and the chunk taking specification.
```

---

# Pasta: utils
### Arquivo: chunking.py
#### Classe: Chunker
#### Função: run
```
Chunk arguments and run the function.
```

---

# Pasta: utils
### Arquivo: colors.py
#### Docstring do Módulo
```
Utilities for working with colors.
```

---

# Pasta: utils
### Arquivo: colors.py
#### Função: map_value_to_cmap
```
Get RGB of `value` from the colormap.
```

---

# Pasta: utils
### Arquivo: colors.py
#### Função: parse_rgba_tuple
```
Parse floating RGBA tuple from string.
```

---

# Pasta: utils
### Arquivo: colors.py
#### Função: parse_rgb_tuple
```
Parse floating RGB tuple from string.
```

---

# Pasta: utils
### Arquivo: colors.py
#### Função: adjust_opacity
```
Adjust opacity of color.
```

---

# Pasta: utils
### Arquivo: colors.py
#### Função: adjust_lightness
```
Lightens the given color by multiplying (1-luminosity) by the given amount.

Input can be matplotlib color string, hex string, or RGB tuple.
Output will be an RGB string.
```

---

# Pasta: utils
### Arquivo: config.py
#### Docstring do Módulo
```
Utilities for configuration.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: hdict
```
Hashable dict.
```

---

# Pasta: utils
### Arquivo: config.py
#### Função: resolve_dict
```
Select keyword arguments.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: atomic_dict
```
Dict that behaves like a single value when merging.
```

---

# Pasta: utils
### Arquivo: config.py
#### Função: convert_to_dict
```
Convert any config to `dict`.

Set `nested` to True to convert all child dicts in recursive manner.

If a config is an instance of `AtomicConfig`, will convert it to `atomic_dict`.
```

---

# Pasta: utils
### Arquivo: config.py
#### Função: resolve_pathlike_key
```
Convert a path-like key into a tuple.
```

---

# Pasta: utils
### Arquivo: config.py
#### Função: combine_pathlike_keys
```
Combine two path-like keys.
```

---

# Pasta: utils
### Arquivo: config.py
#### Função: get_dict_item
```
Get dict item under the key `k`.

The key can be nested using the dot notation, `pathlib.Path`, or a tuple, and must be hashable.
```

---

# Pasta: utils
### Arquivo: config.py
#### Função: set_dict_item
```
Set dict item.

If the dict is of the type `Config`, also passes `force` keyword to override blocking flags.
```

---

# Pasta: utils
### Arquivo: config.py
#### Função: del_dict_item
```
Delete dict item.

If the dict is of the type `Config`, also passes `force` keyword to override blocking flags.
```

---

# Pasta: utils
### Arquivo: config.py
#### Função: copy_dict
```
Copy dict based on a copy mode.

The following modes are supported:

* 'none': Does not copy
* 'shallow': Copies keys only
* 'hybrid': Copies keys and values using `copy.copy`
* 'deep': Copies the whole thing using `copy.deepcopy`

Set `nested` to True to copy all child dicts in recursive manner.
```

---

# Pasta: utils
### Arquivo: config.py
#### Função: update_dict
```
Update dict with keys and values from other dict.

Set `nested` to True to update all child dicts in recursive manner.

For `force`, see `set_dict_item`.

If you want to treat any dict as a single value, wrap it with `atomic_dict`.

If `nested` is True, a value in `x` is an instance of `Configured`, and the corresponding
value in `y` is a dictionary, calls `Configured.replace`.

!!! note
    If the child dict is not atomic, it will copy only its values, not its meta.
```

---

# Pasta: utils
### Arquivo: config.py
#### Função: unset_keys
```
Unset the keys that have the value `unsetkey`.
```

---

# Pasta: utils
### Arquivo: config.py
#### Função: merge_dicts
```
Merge dicts.

Args:
    *dicts (dict): Dicts.
    to_dict (bool): Whether to call `convert_to_dict` on each dict prior to copying.
    copy_mode (str): Mode for `copy_dict` to copy each dict prior to merging.
    nested (bool): Whether to merge all child dicts in recursive manner.

        If None, checks whether any dict is nested.
    same_keys (bool): Whether to merge on the overlapping keys only.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: child_dict
```
Subclass of `dict` acting as a child dict.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Config
```
Extends pickleable dict with config features such as nested updates, freezing, and resetting.

Args:
    *args: Arguments to construct the dict from.
    options_ (dict): Config options (see below).
    **kwargs: Keyword arguments to construct the dict from.

Options can have the following keys:

Attributes:
    copy_kwargs (dict): Keyword arguments passed to `copy_dict` for copying main dict and `reset_dct`.

        Copy mode defaults to 'none'.
    reset_dct (dict): Dict to fall back to in case of resetting.

        Defaults to None. If None, copies main dict using `reset_dct_copy_kwargs`.

        !!! note
            Defaults to main dict in case it's None and `readonly` is True.
    reset_dct_copy_kwargs (dict): Keyword arguments that override `copy_kwargs` for `reset_dct`.

        Copy mode defaults to 'none' if `readonly` is True, else to 'hybrid'.
    pickle_reset_dct (bool): Whether to pickle `reset_dct`.
    frozen_keys (bool): Whether to deny updates to the keys of the config.

        Defaults to False.
    readonly (bool): Whether to deny updates to the keys and values of the config.

        Defaults to False.
    nested (bool): Whether to do operations recursively on each child dict.

        Such operations include copy, update, and merge.
        Disable to treat each child dict as a single value. Defaults to True.
    convert_children (bool or type): Whether to convert child dicts of type `child_dict` to configs
        with the same configuration.

        This will trigger a waterfall reaction across all child dicts. Won't convert dicts that
        are already configs. Apart from boolean, you can set it to any subclass of `Config` to use
        it for construction. Requires `nested` to be True. Defaults to False.
    as_attrs (bool): Whether to enable accessing dict keys via the dot notation.

        Enables autocompletion (but only during runtime!). Raises error in case of naming conflicts.
        Defaults to True if `frozen_keys` or `readonly`, otherwise False.

        To make nested dictionaries also accessible via the dot notation, wrap
        them with `child_dict` and set `convert_children` and `nested` to True.

Defaults can be overridden with settings under `vectorbtpro._settings.config`.

If another config is passed, its properties are copied over, but they can still be overridden
with the arguments passed to the initializer.

!!! note
    All arguments are applied only once during initialization.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: AtomicConfig
```
Config that behaves like a single value when merging.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: FrozenConfig
```
`Config` with `frozen_keys` flag set to True.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: ReadonlyConfig
```
`Config` with `readonly` flag set to True.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: HybridConfig
```
`Config` with `copy_kwargs` set to `copy_mode='hybrid'`.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: SettingsNotFoundError
```
Gets raised if settings could not be found.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: SettingNotFoundError
```
Gets raised if a setting could not be found.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: HasSettings
```
Class that has settings in `vectorbtpro._settings`.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Configured
```
Class with an initialization config.

All subclasses of `Configured` are initialized using `Config`, which makes it easier to pickle.

Settings are defined under `vectorbtpro._settings.configured`.

!!! warning
    If any attribute has been overwritten that isn't listed in `Configured._writeable_attrs`,
    or if any `Configured.__init__` argument depends upon global defaults,
    their values won't be copied over. Make sure to pass them explicitly to
    make that the saved & loaded / copied instance is resilient to any changes in globals.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Config
#### Função: options_
```
Config options.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Config
#### Função: get_option
```
Get an option.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Config
#### Função: set_option
```
Set an option.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Config
#### Função: pop
```
Remove and return the pair by the key.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Config
#### Função: popitem
```
Remove and return some pair.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Config
#### Função: clear
```
Remove all items.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Config
#### Função: update
```
Update the config.

See `update_dict`.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Config
#### Função: __copy__
```
Shallow operation, primarily used by `copy.copy`.

Does not take into account copy settings.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Config
#### Função: __deepcopy__
```
Deep operation, primarily used by `copy.deepcopy`.

Does not take into account copy settings.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Config
#### Função: copy
```
Copy the instance.

By default, copies in the same way as during the initialization.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Config
#### Função: merge_with
```
Merge with another dict into one single dict.

See `merge_dicts`.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Config
#### Função: to_dict
```
Convert to dict.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Config
#### Função: reset
```
Clears the config and updates it with the initial config.

`reset_dct_copy_kwargs` override `reset_dct_copy_kwargs`.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Config
#### Função: make_checkpoint
```
Replace `reset_dct` by the current state.

`reset_dct_copy_kwargs` override `reset_dct_copy_kwargs`.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Config
#### Função: load_update
```
Load dumps from a file and update this instance in-place.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: HasSettings
#### Função: get_path_settings
```
Get the settings under a path.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: HasSettings
#### Função: resolve_settings_paths
```
Resolve the settings paths associated with this class and its superclasses (if `inherit` is True).
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: HasSettings
#### Função: get_settings
```
Get the settings associated with this class and its superclasses (if `inherit` is True).
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: HasSettings
#### Função: has_path_settings
```
Return whether the settings under a path exist.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: HasSettings
#### Função: has_settings
```
Return whether there the settings associated with this class and its superclasses
(if `inherit` is True) exist.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: HasSettings
#### Função: get_path_setting
```
Get a value from the settings under a path.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: HasSettings
#### Função: get_setting
```
Get a value under the settings associated with this class and its superclasses
(if `inherit` is True).
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: HasSettings
#### Função: has_path_setting
```
Return whether the setting under a path exists.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: HasSettings
#### Função: has_setting
```
Return whether the settings associated with this class and its superclasses
(if `inherit` is True) exists.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: HasSettings
#### Função: resolve_setting
```
Resolve a value that has a key under the settings in `vectorbtpro._settings`
associated with this class.

If the provided value is None, returns the setting, otherwise the value.
If the value is a dict and `merge` is True, merges it over the corresponding dict in the settings.

If `sub_path` is provided, appends it to the resolved path and gives it more priority.
If only the `sub_path` should be considered, set `sub_path_only` to True.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: HasSettings
#### Função: set_settings
```
Set the settings in `vectorbtpro._settings` associated with this class.

If the settings do not exist yet, pass `populate_=True`.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: HasSettings
#### Função: reset_settings
```
Reset the settings in `vectorbtpro._settings` associated with this class.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Configured
#### Função: config
```
Initialization config.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Configured
#### Função: get_writeable_attrs
```
Get set of attributes that are writeable by this class or by any of its base classes.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Configured
#### Função: resolve_merge_kwargs
```
Resolve keyword arguments for initializing `Configured` after merging.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Configured
#### Função: replace
```
Create a new instance by copying and (optionally) changing the config.

!!! warning
    This operation won't return a copy of the instance but a new instance
    initialized with the same config and writeable attributes (or their copy, depending on `copy_mode`).
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Configured
#### Função: copy
```
Create a new instance by copying the config.

See `Configured.replace`.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Configured
#### Função: equals
```
Check two objects for equality.
```

---

# Pasta: utils
### Arquivo: config.py
#### Classe: Configured
#### Função: update_config
```
Force-update the config.
```

---

# Pasta: utils
### Arquivo: cutting.py
#### Docstring do Módulo
```
Utilities for cutting code.
```

---

# Pasta: utils
### Arquivo: cutting.py
#### Função: collect_blocks
```
Collect blocks in the lines.
```

---

# Pasta: utils
### Arquivo: cutting.py
#### Função: cut_from_code
```
Parse and cut an annotated section from the code.

The section should start with `# % <section section_name>` and end with `# % </section>`.

You can also define blocks. Each block should start with `# % <block block_name>` and end with `# % </block>`.
Blocks will be collected into the dictionary `blocks` before cutting and can be then inserted using
Python expressions (see below).

To skip multiple lines of code, place them between `# % <skip [expression]>` and `# % </skip>`,
where expression is optional.

To uncomment multiple lines of code, place them between `# % <uncomment [expression]>` and
`# % </uncomment>`, where expression is optional.

Everything else after `# %` will be evaluated as a Python expression and should return
either None (= skip), a string (= insert one line of code) or an iterable of strings
(= insert multiple lines of code). The latter will be appended to the queue and parsed.

Every expression is evaluated strictly, that is, any evaluation error will raise an error
and stop the program. To evaluate softly without raising any errors, prepend `?`.
The context includes `lines`, `blocks`, `section_name`, `line`, `out_lines`, and `**kwargs`.
```

---

# Pasta: utils
### Arquivo: cutting.py
#### Função: suggest_module_path
```
Suggest a path to the target file.
```

---

# Pasta: utils
### Arquivo: cutting.py
#### Função: cut_and_save
```
Cut an annotated section from the code and save to a file.

For arguments see `cut_from_code`.
```

---

# Pasta: utils
### Arquivo: cutting.py
#### Função: cut_and_save_module
```
Cut an annotated section from a module and save to a file.

For arguments see `cut_and_save`.
```

---

# Pasta: utils
### Arquivo: cutting.py
#### Função: cut_and_save_func
```
Cut an annotated function section from a module and save to a file.

For arguments see `cut_and_save`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Docstring do Módulo
```
Utilities for working with dates and time.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: split_freq_str
```
Split (human-readable) frequency into multiplier and unambiguous unit.

Can be used both as offset and timedelta.

For mappings, see `sharp_freq_str_config` and `fuzzy_freq_str_config`.
Sharp (case-sensitive) mappings are considered first, fuzzy (case-insensitive) mappings second.
If a mapping returns None, will return the original unit.

The following case-sensitive units are returned:
* "ns" for nanosecond
* "us" for microsecond
* "ms" for millisecond
* "s" for second
* "m" for minute
* "h" for hour
* "D" for day
* "W" for week
* "M" for month
* "Q" for quarter
* "Y" for year

If a unit isn't recognized, will return the original unit.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: prepare_offset_str
```
Prepare offset frequency string.

To include multiple units, separate them with comma, semicolon, or space if `allow_space` is True.
The output becomes comma-separated.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: to_offset
```
Convert a frequency-like object to `pd.DateOffset`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: prepare_timedelta_str
```
Prepare timedelta frequency string.

To include multiple units, separate them with comma, semicolon, or space if `allow_space` is True.
The output becomes comma-separated.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: offset_to_timedelta
```
Convert offset to a timedelta.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: fix_timedelta_precision
```
Fix the precision of timedelta.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: to_timedelta
```
Convert a frequency-like object to `pd.Timedelta`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: to_timedelta64
```
Convert a frequency-like object to `np.timedelta64`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: to_freq
```
Convert a frequency-like object to `pd.DateOffset` or `pd.Timedelta`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Classe: DTC
```
Class representing one or more datetime components.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: time_to_timedelta
```
Convert a time-like object into `pd.Timedelta`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: get_utc_tz
```
Get UTC timezone.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: get_local_tz
```
Get local timezone.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: convert_tzaware_time
```
Return as non-naive time.

`datetime.time` must have `tzinfo` set.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: tzaware_to_naive_time
```
Return as naive time.

`datetime.time` must have `tzinfo` set.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: naive_to_tzaware_time
```
Return as non-naive time.

`datetime.time` must not have `tzinfo` set.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: convert_naive_time
```
Return as naive time.

`datetime.time` must not have `tzinfo` set.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: is_tz_aware
```
Whether datetime is timezone-aware.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: to_timezone
```
Parse the timezone.

If the object is None, returns the local timezone. If a string, will parse with Pandas and
dateparser (`parse_with_dateparser` must be True).

If `to_fixed_offset` is set to True, will convert to `datetime.timezone`. See global settings.

`dateparser_kwargs` are passed to `dateparser.parse`.

For defaults, see `vectorbtpro._settings.datetime`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: to_timestamp
```
Parse the datetime as a `pd.Timestamp`.

If the object is a string, will parse with Pandas and dateparser (`parse_with_dateparser` must be True).

`dateparser_kwargs` are passed to `dateparser.parse` while `**kwargs` are passed to `pd.Timestamp`.

For defaults, see `vectorbtpro._settings.datetime`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: to_tzaware_timestamp
```
Parse the datetime as a timezone-aware `pd.Timestamp`.

Uses `to_timestamp`.

Raw timestamps are localized to UTC, while naive datetime is localized to `naive_tz`.
Set `naive_tz` to None to use the default value defined under `vectorbtpro._settings.datetime`.
To explicitly convert the datetime to a timezone, use `tz` (uses `to_timezone`).

For defaults, see `vectorbtpro._settings.datetime`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: to_naive_timestamp
```
Parse the datetime as a timezone-naive `pd.Timestamp`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: to_datetime
```
Parse the datetime as a `datetime.datetime`.

Uses `to_timestamp`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: to_tzaware_datetime
```
Parse the datetime as a timezone-aware `datetime.datetime`.

Uses `to_tzaware_timestamp`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: to_naive_datetime
```
Parse the datetime as a timezone-naive `datetime.datetime`.

Uses `to_naive_timestamp`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: get_min_td_component
```
Get index of the smallest timedelta component.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: readable_datetime
```
Get a human-readable datetime string.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: datetime_to_ms
```
Convert a datetime to milliseconds.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: interval_to_ms
```
Convert an interval string to milliseconds.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: to_ns
```
Convert a datetime, timedelta, integer, or any array-like object to nanoseconds since Unix Epoch.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: date_range
```
Same as `pd.date_range` but preprocesses `start` and `end` with `to_timestamp`,
`freq` with `to_freq`, and `tz` with `to_timezone`.

If `start` and `periods` are None, will set `start` to the beginning of the Unix epoch.
Same if pf `periods` is not None but `start` and `end` are None.

If `end` and `periods` are None, will set `end` to the current date and time.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: prepare_dt_index
```
Try converting an index to a datetime index.

If `parse_index` is True and the object has an object data type, will parse with Pandas
(`parse_index` must be True) and dateparser (in addition `parse_with_dateparser` must be True).

`dateparser_kwargs` are passed to `dateparser.parse` while `**kwargs` are passed to `pd.to_datetime`.

For defaults, see `vectorbtpro._settings.datetime`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: try_align_to_dt_index
```
Try aligning an index to another datetime index.

Keyword arguments are passed to `prepare_dt_index`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: try_align_dt_to_index
```
Try aligning a datetime-like object to another datetime index.

Keyword arguments are passed to `to_timestamp`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: auto_detect_freq
```
Auto-detect frequency from a datetime index.

Returns the minimal frequency if it's being encountered in most of the index.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: parse_index_freq
```
Parse frequency from a datetime index.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: freq_depends_on_index
```
Return whether frequency depends on index.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: infer_index_freq
```
Infer frequency of a datetime index if `freq` is None, otherwise convert `freq`.

If `freq` is "auto", uses `auto_detect_freq`. If `freq` is "index_[method_name]", applies the
method to the `pd.TimedeltaIndex` resulting from the difference between each pair of index points.
If `freq_from_n` is a positive or negative number, limits the index to the first or the
last N index points respectively.

For defaults, see `vectorbtpro._settings.datetime`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: get_dt_index_gaps
```
Get gaps in a datetime index.

Returns two indexes: start indexes (inclusive) and end indexes (exclusive).

Keyword arguments are passed to `prepare_dt_index`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Função: get_rangebreaks
```
Get `rangebreaks` based on `get_dt_index_gaps`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Classe: DTC
#### Função: from_datetime
```
Get `DTC` instance from a `datetime.datetime` object.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Classe: DTC
#### Função: from_date
```
Get `DTC` instance from a `datetime.date` object.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Classe: DTC
#### Função: from_time
```
Get `DTC` instance from a `datetime.time` object.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Classe: DTC
#### Função: parse_time_str
```
Parse `DTC` instance from a time string.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Classe: DTC
#### Função: from_namedtuple
```
Get `DTC` instance from a named tuple of the type `DTCNT`.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Classe: DTC
#### Função: parse
```
Parse `DTC` instance from a datetime-component-like object.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Classe: DTC
#### Função: is_parsable
```
Check whether a datetime-component-like object is parsable.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Classe: DTC
#### Função: has_date
```
Whether any date component is set.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Classe: DTC
#### Função: has_full_date
```
Whether all date components are set.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Classe: DTC
#### Função: has_weekday
```
Whether the weekday component is set.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Classe: DTC
#### Função: has_time
```
Whether any time component is set.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Classe: DTC
#### Função: has_full_time
```
Whether all time components are set.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Classe: DTC
#### Função: has_full_datetime
```
Whether all components are set.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Classe: DTC
#### Função: is_not_none
```
Check whether any component is set.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Classe: DTC
#### Função: to_time
```
Convert to a `datetime.time` instance.

Fields that are None will become 0.
```

---

# Pasta: utils
### Arquivo: datetime_.py
#### Classe: DTC
#### Função: to_namedtuple
```
Convert to a named tuple.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Docstring do Módulo
```
Numba-compiled utilities for working with dates and time.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: second_remainder_nb
```
Get the nanosecond remainder after the second.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: nanosecond_nb
```
Get the nanosecond.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: microseconds_nb
```
Get the number of microseconds.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: microsecond_nb
```
Get the microsecond.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: milliseconds_nb
```
Get the number of milliseconds.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: millisecond_nb
```
Get the millisecond.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: seconds_nb
```
Get the number of seconds.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: second_nb
```
Get the seconds.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: minutes_nb
```
Get the number of minutes.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: minute_nb
```
Get the minute.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: hours_nb
```
Get the number of hours.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: hour_nb
```
Get the hour.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: days_nb
```
Get the number of days.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: to_civil_nb
```
Convert a timestamp into a tuple of the year, month, and day.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: from_civil_nb
```
Convert a year, month, and day into the timestamp.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: matches_date_nb
```
Check whether the timestamp match the date provided in the civil format.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: day_nb
```
Get the day of the month.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: midnight_nb
```
Get the midnight of this day.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: day_changed_nb
```
Whether the day changed.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: weekday_from_days_nb
```
Get the weekday from the total number of days.

Weekdays are ranging from 0 (Monday) to 6 (Sunday).
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: weekday_nb
```
Get the weekday.

Weekdays are ranging from 0 (Monday) to 6 (Sunday).
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: weekday_diff_nb
```
Get the difference in days between two weekdays.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: past_weekday_nb
```
Get the timestamp of a weekday in the past.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: future_weekday_nb
```
Get the timestamp of a weekday in the future.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: day_of_year_nb
```
Get the day of the year.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: week_nb
```
Get the week of the year.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: month_nb
```
Get the month of the year.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: year_nb
```
Get the year.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: is_leap_year_nb
```
Get whether the year is a leap year.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: last_day_of_month_nb
```
Get the last day of the month.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: matches_dtc_nb
```
Return whether one or more datetime components match other components.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: index_matches_dtc_nb
```
Run `matches_dtc_nb` on each element in an index and return a mask.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: within_fixed_dtc_nb
```
Return whether a single datetime component is within a fixed range.

Returns a status of the type `DTCS`.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: within_periodic_dtc_nb
```
Return whether a single datetime component is within a periodic range.

Returns a status of the type `DTCS`.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: must_resolve_dtc_nb
```
Return whether the component must be resolved.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: start_dtc_lt_nb
```
Return whether the start component is less than the end component.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: start_dtc_eq_nb
```
Return whether the start component equals to the end component.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: start_dtc_gt_nb
```
Return whether the start component is greater than the end component.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: within_dtc_range_nb
```
Return whether one or more datetime components are within a range.
```

---

# Pasta: utils
### Arquivo: datetime_nb.py
#### Função: index_within_dtc_range_nb
```
Run `within_dtc_range_nb` on each element in an index and return a mask.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Docstring do Módulo
```
Class and function decorators.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Classe: classproperty
```
Property that can be called on a class.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Classe: class_or_instanceproperty
```
Property that binds `self` to a class if the function is called as class method,
otherwise to an instance.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Classe: class_or_instancemethod
```
Function decorator that binds `self` to a class if the function is called as class method,
otherwise to an instance.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Classe: custom_property
```
Custom extensible property that stores function and options as attributes.

!!! note
    `custom_property` instances belong to classes, not class instances. Thus changing the property
    will do the same for each instance of the class where the property has been defined initially.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Classe: cacheable_property
```
Extends `custom_property` for cacheable properties.

!!! note
    Assumes that the instance (provided as `self`) won't change. If calculation depends
    upon object attributes that can be changed, it won't notice the change.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Classe: cached_property
```
`cacheable_property` with `use_cache` set to True.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Função: custom_function
```
Custom function decorator.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Função: cacheable
```
Cacheable function decorator.

See notes on `cacheable_property`.

!!! note
    To decorate an instance method, use `cacheable_method`.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Função: cached
```
`cacheable` with `use_cache` set to True.

!!! note
    To decorate an instance method, use `cached_method`.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Função: custom_method
```
Custom method decorator.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Função: cacheable_method
```
Cacheable method decorator.

See notes on `cacheable_property`.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Função: cached_method
```
`cacheable_method` with `use_cache` set to True.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Classe: classproperty
#### Função: func
```
Wrapped function.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Classe: class_or_instanceproperty
#### Função: func
```
Wrapped function.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Classe: custom_property
#### Função: func
```
Wrapped function.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Classe: custom_property
#### Função: name
```
Wrapped function name.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Classe: custom_property
#### Função: options
```
Options.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Classe: cacheable_property
#### Função: init_use_cache
```
Initial value for `use_cache`.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Classe: cacheable_property
#### Função: init_whitelist
```
Initial value for `whitelist`.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Classe: cacheable_property
#### Função: get_ca_setup
```
Get setup of type `vectorbtpro.registries.ca_registry.CARunSetup` if instance is known,
or `vectorbtpro.registries.ca_registry.CAUnboundSetup` otherwise.

See `vectorbtpro.registries.ca_registry` for details on the caching procedure.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Função: get_ca_setup
```
Get setup of type `vectorbtpro.registries.ca_registry.CARunSetup`.

See `vectorbtpro.registries.ca_registry` for details on the caching procedure.
```

---

# Pasta: utils
### Arquivo: decorators.py
#### Função: get_ca_setup
```
Get setup of type `vectorbtpro.registries.ca_registry.CARunSetup` if instance is known,
or `vectorbtpro.registries.ca_registry.CAUnboundSetup` otherwise.

See `vectorbtpro.registries.ca_registry` for details on the caching procedure.
```

---

# Pasta: utils
### Arquivo: enum_.py
#### Docstring do Módulo
```
Enum utilities.

In vectorbt, enums are represented by instances of named tuples to be easily used in Numba.
Their values start with 0, while -1 means there is no value.
```

---

# Pasta: utils
### Arquivo: enum_.py
#### Função: map_enum_fields
```
Map fields to values.

See `vectorbtpro.utils.mapping.apply_mapping`.
```

---

# Pasta: utils
### Arquivo: enum_.py
#### Função: map_enum_values
```
Map values to fields.

See `vectorbtpro.utils.mapping.apply_mapping`.
```

---

# Pasta: utils
### Arquivo: eval_.py
#### Docstring do Módulo
```
Utilities for evaluation and compilation.
```

---

# Pasta: utils
### Arquivo: eval_.py
#### Função: multiline_eval
```
Evaluate several lines of input, returning the result of the last line.
```

---

# Pasta: utils
### Arquivo: eval_.py
#### Classe: Evaluable
```
Abstract class for instances that can be evaluated.
```

---

# Pasta: utils
### Arquivo: eval_.py
#### Classe: Evaluable
#### Função: meets_eval_id
```
Return whether the evaluation id of the instance meets the global evaluation id.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Docstring do Módulo
```
Engines for executing functions.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Task
```
Class that represents an executable task.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: _NoResult
```
Sentinel that represents no result.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: NoResultsException
```
Gets raised when there are no results.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Função: filter_out_no_results
```
Filter objects and keys by removing `NoResult` objects.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: ExecutionEngine
```
Abstract class for executing functions.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: SerialEngine
```
Class for executing functions sequentially.

For defaults, see `engines.serial` in `vectorbtpro._settings.execution`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: ThreadPoolEngine
```
Class for executing functions using `ThreadPoolExecutor` from `concurrent.futures`.

For defaults, see `engines.threadpool` in `vectorbtpro._settings.execution`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: ProcessPoolEngine
```
Class for executing functions using `ProcessPoolExecutor` from `concurrent.futures`.

For defaults, see `engines.processpool` in `vectorbtpro._settings.execution`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Função: pass_kwargs_as_args
```
Helper function for `pathos.pools.ParallelPool`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: PathosEngine
```
Class for executing functions using `pathos`.

For defaults, see `engines.pathos` in `vectorbtpro._settings.execution`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: MpireEngine
```
Class for executing functions using `WorkerPool` from `mpire`.

For defaults, see `engines.mpire` in `vectorbtpro._settings.execution`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: DaskEngine
```
Class for executing functions in parallel using Dask.

For defaults, see `engines.dask` in `vectorbtpro._settings.execution`.

!!! note
    Use multi-threading mainly on numeric code that releases the GIL
    (like NumPy, Pandas, Scikit-Learn, Numba).
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: RayEngine
```
Class for executing functions in parallel using Ray.

For defaults, see `engines.ray` in `vectorbtpro._settings.execution`.

!!! note
    Ray spawns multiple processes as opposed to threads, so any argument and keyword argument must first
    be put into an object store to be shared. Make sure that the computation with `func` takes
    a considerable amount of time compared to this copying operation, otherwise there will be
    a little to no speedup.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: _Dummy
```
Sentinel that represents a dummy value.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
```
Class responsible executing functions.

Supported values for `engine`:

* Name of the engine (see supported engines)
* Subclass of `ExecutionEngine` - initializes with `engine_config`
* Instance of `ExecutionEngine` - calls `ExecutionEngine.execute` with `size`
* Callable - passes `tasks`, `size` (if not None), and `engine_config`

Can execute per chunk if `chunk_meta` is provided. Otherwise, if any of `n_chunks` and `chunk_len`
are set, passes them to `vectorbtpro.utils.chunking.yield_chunk_meta` to generate `chunk_meta`.
Arguments `n_chunks` and `chunk_len` can be set globally in the engine-specific settings.
Set `n_chunks` and `chunk_len` to 'auto' to set them to the number of cores.

If `distribute` is "tasks", distributes tasks within each chunk.
If indices in `chunk_meta` are perfectly sorted and `tasks` is an iterable, iterates
over `tasks` to avoid converting it into a list. Otherwise, iterates over `chunk_meta`.
If `in_chunk_order` is True, returns the results in the order they appear in `chunk_meta`.
Otherwise, always returns them in the same order as in `tasks`.

If `distribute` is "chunks", distributes chunks. For this, executes tasks within each chunk serially
using `Executor.execute_serially`. Also, compresses each chunk such that each unique function,
positional argument, and keyword argument is serialized only once.

If `tasks` is a custom template, substitutes it once `chunk_meta` is established.
Use `template_context` as an additional context. All the resolved functions and arguments
will be immediately passed to the executor.

If `pre_chunk_func` is not None, calls the function before processing a chunk. If it returns anything
other than None, the returned object will be appended to the results and the chunk won't be executed.
This enables use cases such as caching. If `post_chunk_func` is not None, calls the function after
processing the chunk. It should return either None to keep the old call results, or return new ones.
Will also substitute any templates in `pre_chunk_kwargs` and `post_chunk_kwargs` and pass them as
keyword arguments. The following additional arguments are available in the contexts: the index of
the current chunk `chunk_idx`, the list of call indices `call_indices` in the chunk, the list of call
results `chunk_cache` returned from caching (only for `pre_chunk_func`), the list of call results
`call_results` returned by executing the chunk (only for `post_chunk_func`), and whether the chunk
was executed `chunk_executed` or otherwise returned by `pre_chunk_func` (only for `post_chunk_func`).

!!! note
    The both callbacks above are effective only when `distribute` is "tasks" and chunking is enabled.

If `pre_execute_func` is not None, calls the function before processing all tasks. Should return
nothing (None). Will also substitute any templates in `post_execute_kwargs` and pass them as keyword
arguments. The following additional arguments are available in the context: the number of chunks `n_chunks`.

If `post_execute_func` is not None, calls the function after processing all tasks. Will also substitute
any templates in `post_execute_kwargs` and pass them as keyword arguments. Should return either None
to keep the default results or return the new ones. The following additional arguments are available
in the context: the number of chunks `n_chunks` and the generated flattened list of results `results`.
If `post_execute_on_sorted` is True, will run the callback after sorting the call indices.

!!! info
    Chunks are processed sequentially, while functions within each chunk can be processed distributively.

For defaults, see `vectorbtpro._settings.execution`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Função: execute
```
Execute functions and their arguments using `Executor`.

Keyword arguments `**kwargs` and `engine_config` are merged into `engine_config`
if `merge_to_engine_config` is True, otherwise, `**kwargs` are passed directly to `Executor`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Função: parse_iterable_and_keys
```
Parse the iterable and the keys from an iterable-like and a keys-like object respectively.

Object can be an integer that will be interpreted as a total or any iterable.

If object is a dictionary, a Pandas Index, or a Pandas Series, keys will be set to the index.
Otherwise, keys will be extracted using `vectorbtpro.base.indexes.index_from_values`.
Keys won't be extracted if the object is not a sequence to avoid materializing it.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Função: iterated
```
Decorator that executes a function in iteration using `Executor`.

Returns a new function with the same signature as the passed one.

Use `over_arg` to specify which argument (position or name) should be iterated over.
If it's None (default), uses the first argument.

Each option can be modified in the `options` attribute of the wrapper function or
directly passed as a keyword argument with a leading underscore. You can also explicitly specify
keys and size by passing them as `_keys` and `_size` respectively if the range-like object is an iterator.

Keyword arguments `**kwargs` and `engine_config` are merged into `engine_config`
if `merge_to_engine_config` is True, otherwise, `**kwargs` are passed directly to `Executor`.

If `NoResult` is returned, will skip the current iteration and remove it from the final index.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Task
#### Função: execute
```
Execute the task.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: ExecutionEngine
#### Função: execute
```
Run an iterable of tuples out of a function, arguments, and keyword arguments.

Provide `size` in case `tasks` is a generator and the underlying engine needs it.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: SerialEngine
#### Função: show_progress
```
Whether to show the progress bar.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: SerialEngine
#### Função: pbar_kwargs
```
Keyword arguments passed to `vectorbtpro.utils.pbar.ProgressBar`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: SerialEngine
#### Função: clear_cache
```
Whether to clear vectorbt's cache after each iteration.

If integer, do it once a number of tasks.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: SerialEngine
#### Função: collect_garbage
```
Whether to clear garbage after each iteration.

If integer, do it once a number of tasks.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: SerialEngine
#### Função: delay
```
Number of seconds to sleep after each call.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: ThreadPoolEngine
#### Função: init_kwargs
```
Keyword arguments used to initialize `ThreadPoolExecutor`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: ThreadPoolEngine
#### Função: timeout
```
Timeout.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: ProcessPoolEngine
#### Função: init_kwargs
```
Keyword arguments used to initialize `ProcessPoolExecutor`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: ProcessPoolEngine
#### Função: timeout
```
Timeout.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: PathosEngine
#### Função: pool_type
```
Pool type.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: PathosEngine
#### Função: init_kwargs
```
Keyword arguments used to initialize the pool.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: PathosEngine
#### Função: timeout
```
Timeout.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: PathosEngine
#### Função: check_delay
```
Number of seconds to sleep between checks.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: PathosEngine
#### Função: show_progress
```
Whether to show the progress bar.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: PathosEngine
#### Função: pbar_kwargs
```
Keyword arguments passed to `vectorbtpro.utils.pbar.ProgressBar`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: PathosEngine
#### Função: join_pool
```
Whether to join the pool.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: MpireEngine
#### Função: init_kwargs
```
Keyword arguments used to initialize `WorkerPool`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: MpireEngine
#### Função: apply_kwargs
```
Keyword arguments passed to `WorkerPool.async_apply`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: DaskEngine
#### Função: compute_kwargs
```
Keyword arguments passed to `dask.compute`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: RayEngine
#### Função: restart
```
Whether to terminate the Ray runtime and initialize a new one.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: RayEngine
#### Função: reuse_refs
```
Whether to re-use function and object references, such that each unique object
will be copied only once.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: RayEngine
#### Função: del_refs
```
Whether to explicitly delete the result object references.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: RayEngine
#### Função: shutdown
```
Whether to True to terminate the Ray runtime upon the job end.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: RayEngine
#### Função: init_kwargs
```
Keyword arguments passed to `ray.init`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: RayEngine
#### Função: remote_kwargs
```
Keyword arguments passed to `ray.remote`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: RayEngine
#### Função: get_ray_refs
```
Get result references by putting each argument and keyword argument into the object store
and invoking the remote decorator on each function using Ray.

If `reuse_refs` is True, will generate one reference per unique object id.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: get_engine_settings
```
`Executor.get_settings` with `sub_path=engine_name`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: has_engine_settings
```
`Executor.has_settings` with `sub_path=engine_name`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: get_engine_setting
```
`Executor.get_setting` with `sub_path=engine_name`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: has_engine_setting
```
`Executor.has_setting` with `sub_path=engine_name`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: resolve_engine_setting
```
`Executor.resolve_setting` with `sub_path=engine_name`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: set_engine_settings
```
`Executor.set_settings` with `sub_path=engine_name`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: resolve_engine
```
Resolve engine and its name in settings.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: engine
```
Engine resolved with `Executor.resolve_engine`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: min_size
```
See `vectorbtpro.utils.chunking.yield_chunk_meta`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: n_chunks
```
See `vectorbtpro.utils.chunking.yield_chunk_meta`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: chunk_len
```
See `vectorbtpro.utils.chunking.yield_chunk_meta`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: chunk_meta
```
See `vectorbtpro.utils.chunking.yield_chunk_meta`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: distribute
```
Distribution mode.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: warmup
```
Whether to call the first item of `tasks` once before distribution.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: in_chunk_order
```
Whether to return the results in the order they appear in `chunk_meta`.

Otherwise, always returns them in the same order as in `tasks`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: cache_chunks
```
Whether to cache chunks.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: chunk_cache_dir
```
Directory where to put chunk cache files.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: chunk_cache_save_kwargs
```
Keyword arguments passed to `vectorbtpro.utils.pickling.save` for chunk caching.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: chunk_cache_load_kwargs
```
Keyword arguments passed to `vectorbtpro.utils.pickling.load` for chunk caching.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: pre_clear_chunk_cache
```
Whether to remove the chunk cache directory before execution.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: post_clear_chunk_cache
```
Whether to remove the chunk cache directory after execution.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: release_chunk_cache
```
Whether to replace chunk cache with dummy objects once the chunk has been executed
and then load all cache at once after all chunks have been executed.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: chunk_clear_cache
```
Whether to clear global cache after each chunk or every n chunks.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: chunk_collect_garbage
```
Whether to collect garbage after each chunk or every n chunks.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: chunk_delay
```
Number of seconds to sleep after each chunk.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: pre_execute_func
```
Function to call before processing all tasks.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: pre_execute_kwargs
```
Keyword arguments passed to `Executor.pre_execute_func`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: pre_chunk_func
```
Function to call before processing a chunk.

If it returns anything other than None, the returned object will be appended to the
results and the chunk won't be executed. This enables use cases such as caching.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: pre_chunk_kwargs
```
Keyword arguments passed to `Executor.pre_chunk_func`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: post_chunk_func
```
Function to call after processing the chunk.

It should return either None to keep the old call results, or return new ones.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: post_chunk_kwargs
```
Keyword arguments passed to `Executor.post_chunk_func`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: post_execute_func
```
Function to call after processing all tasks.

Should return either None to keep the default results, or return the new ones.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: post_execute_kwargs
```
Keyword arguments passed to `Executor.post_execute_func`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: post_execute_on_sorted
```
Whether to run `Executor.post_execute_func` after sorting the call indices.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: filter_results
```
Whether to filter `NoResult` results.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: raise_no_results
```
Whether to raise `NoResultsException` if there are no results. Otherwise, returns `NoResult`.

Has effect only if `Executor.filter_results` is True. But regardless of this setting,
gets passed to the merging function if the merging function is pre-configured.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: merge_func
```
Merging function.

Resolved using `vectorbtpro.base.merging.resolve_merge_func`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: merge_kwargs
```
Keyword arguments passed to the merging function.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: template_context
```
Context used to substitute templates.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: show_progress
```
Whether to show progress bar when iterating over chunks.

If `Executor.engine` accepts `show_progress` and there's no key `show_progress`
in `Executor.engine_config`, then passes it to the engine as well.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: pbar_kwargs
```
Keyword arguments passed to `vectorbtpro.utils.pbar.ProgressBar`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: execute_serially
```
Execute serially.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: build_serial_chunk
```
Build a serial chunk.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: call_pre_execute_func
```
Call `Executor.pre_execute_func`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: call_pre_chunk_func
```
Call `Executor.pre_chunk_func`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: call_execute
```
Call `ExecutionEngine.execute`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: call_post_chunk_func
```
Call `Executor.post_chunk_func`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: call_post_execute_func
```
Call `Executor.post_execute_func`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: merge_results
```
Merge results using `Executor.merge_func` and `Executor.merge_kwargs`.
```

---

# Pasta: utils
### Arquivo: execution.py
#### Classe: Executor
#### Função: run
```
Execute functions and their arguments.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Docstring do Módulo
```
Utilities for constructing and displaying figures.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Função: resolve_axis_refs
```
Get x-axis and y-axis references.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Função: get_domain
```
Get domain of a coordinate axis.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Classe: FigureMixin
```
Mixin class for figures.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Classe: Figure
```
Figure.

Extends `plotly.graph_objects.Figure`.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Classe: FigureWidget
```
Figure widget.

Extends `plotly.graph_objects.FigureWidget`.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Função: make_figure
```
Make a new Plotly figure.

If `use_widgets` is True, returns `FigureWidget`, otherwise `Figure`.

If `use_resampler` is True, additionally wraps the class using `plotly_resampler`.

Defaults are defined under `vectorbtpro._settings.plotting`.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Função: make_subplots
```
Make Plotly subplots using `make_figure`.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Classe: FigureMixin
#### Função: copy
```
Create a copy of the figure.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Classe: FigureMixin
#### Função: select_range
```
Select a range.

Start and end index can be integers but also datetime-like objects.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Classe: FigureMixin
#### Função: auto_rangebreaks
```
Set range breaks automatically based on `vectorbtpro.utils.datetime_.get_rangebreaks`.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Classe: FigureMixin
#### Função: skip_index
```
Skip index values.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Classe: FigureMixin
#### Função: resolve_show_args
```
Display the figure.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Classe: FigureMixin
#### Função: show
```
Display the figure.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Classe: FigureMixin
#### Função: show_png
```
Display the figure in PNG format.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Classe: FigureMixin
#### Função: show_svg
```
Display the figure in SVG format.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Classe: FigureMixin
#### Função: save_svg_for_docs
```
Save the figure in both light and dark SVG format for documentation.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Classe: FigureResampler
```
Figure resampler.

Extends `plotly.graph_objects.Figure`.
```

---

# Pasta: utils
### Arquivo: figure.py
#### Classe: FigureWidgetResampler
```
Figure widget resampler.

Extends `plotly.graph_objects.FigureWidget`.
```

---

# Pasta: utils
### Arquivo: formatting.py
#### Docstring do Módulo
```
Utilities for formatting.
```

---

# Pasta: utils
### Arquivo: formatting.py
#### Função: camel_to_snake_case
```
Convert a camel case string to a snake case string.
```

---

# Pasta: utils
### Arquivo: formatting.py
#### Classe: Prettified
```
Abstract class that can be prettified.
```

---

# Pasta: utils
### Arquivo: formatting.py
#### Função: prettify_inited
```
Prettify an instance initialized with keyword arguments.
```

---

# Pasta: utils
### Arquivo: formatting.py
#### Função: prettify_dict
```
Prettify a dictionary.
```

---

# Pasta: utils
### Arquivo: formatting.py
#### Função: prettify
```
Prettify an object.

Unfolds regular Python data structures such as lists and tuples.

If `obj` is an instance of `Prettified`, calls `Prettified.prettify`.
```

---

# Pasta: utils
### Arquivo: formatting.py
#### Função: pprint
```
Print the output of `prettify`.
```

---

# Pasta: utils
### Arquivo: formatting.py
#### Função: format_array
```
Format an array.

Arguments are passed to `pd.DataFrame.to_string` or `pd.DataFrame.to_html` if `tabulate` is False,
otherwise to `tabulate.tabulate`. If `tabulate` is None, will be set to True if the `tabulate`
library is installed and `html` is disabled.
```

---

# Pasta: utils
### Arquivo: formatting.py
#### Função: ptable
```
Print the output of `format_array`.

If `display_html` is None, checks whether the code runs in a IPython notebook, and if so, becomes True.
If `display_html` is True, displays the table in HTML format.
```

---

# Pasta: utils
### Arquivo: formatting.py
#### Função: format_parameter
```
Format a parameter of a signature.
```

---

# Pasta: utils
### Arquivo: formatting.py
#### Função: format_signature
```
Format a signature.
```

---

# Pasta: utils
### Arquivo: formatting.py
#### Função: format_func
```
Format a function.
```

---

# Pasta: utils
### Arquivo: formatting.py
#### Função: phelp
```
Print the output of `format_func`.
```

---

# Pasta: utils
### Arquivo: formatting.py
#### Função: pdir
```
Print the output of `vectorbtpro.utils.attr_.parse_attrs`.
```

---

# Pasta: utils
### Arquivo: formatting.py
#### Classe: Prettified
#### Função: prettify
```
Prettify this object.

!!! warning
    Calling `prettify` can lead to an infinite recursion.
    Make sure to pre-process this object.
```

---

# Pasta: utils
### Arquivo: hashing.py
#### Docstring do Módulo
```
Utilities for hashing.
```

---

# Pasta: utils
### Arquivo: hashing.py
#### Classe: Hashable
```
Hashable class.
```

---

# Pasta: utils
### Arquivo: hashing.py
#### Classe: Hashable
#### Função: get_hash
```
Static method to get the hash of the instance based on its arguments.
```

---

# Pasta: utils
### Arquivo: hashing.py
#### Classe: Hashable
#### Função: hash_key
```
Key that can be used for hashing the instance.
```

---

# Pasta: utils
### Arquivo: hashing.py
#### Classe: Hashable
#### Função: hash
```
Hash of the instance.
```

---

# Pasta: utils
### Arquivo: image_.py
#### Docstring do Módulo
```
Utilities for images.
```

---

# Pasta: utils
### Arquivo: image_.py
#### Função: hstack_image_arrays
```
Stack NumPy images horizontally.
```

---

# Pasta: utils
### Arquivo: image_.py
#### Função: vstack_image_arrays
```
Stack NumPy images vertically.
```

---

# Pasta: utils
### Arquivo: image_.py
#### Função: save_animation
```
Save animation to a file.

Args:
    fname (str): File name.
    index (sequence): Index to iterate over.
    plot_func (callable): Plotting function.

        Must take subset of `index`, `*args`, and `**kwargs`, and return either a Plotly figure,
        image that can be read by `imageio.imread`, or a NumPy array.
    *args: Positional arguments passed to `plot_func`.
    delta (int): Window size of each iteration.
    step (int): Step of each iteration.
    fps (int): Frames per second.

        Will be translated to `duration` by `1000 / fps`.
    writer_kwargs (dict): Keyword arguments passed to `imageio.get_writer`.
    show_progress (bool): Whether to show the progress bar.
    pbar_kwargs (dict): Keyword arguments passed to `vectorbtpro.utils.pbar.ProgressBar`.
    to_image_kwargs (dict): Keyword arguments passed to `plotly.graph_objects.Figure.to_image`.
    **kwargs: Keyword arguments passed to `plot_func`.

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> def plot_data_window(index, data):
    ...     return data.loc[index].plot()

    >>> data = vbt.YFData.pull("BTC-USD", start="2020", end="2021")
    >>> vbt.save_animation(
    ...     "plot_data_window.gif",
    ...     data.index,
    ...     plot_data_window,
    ...     data,
    ...     delta=90,
    ...     step=10
    ... )
    ```
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Docstring do Módulo
```
Utilities for jitting.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Classe: Jitter
```
Abstract class for decorating jitable functions.

Represents a single configuration for jitting.

When overriding `Jitter.decorate`, make sure to check whether wrapping is disabled
globally using `Jitter.wrapping_disabled`.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Classe: NumPyJitter
```
Class for decorating functions that use NumPy.

Returns the function without decorating.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Classe: NumbaJitter
```
Class for decorating functions using Numba.

!!! note
    If `fix_cannot_parallel` is True, `parallel=True` will be ignored if there is no `can_parallel` tag.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Função: get_func_suffix
```
Get the suffix of the function.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Função: resolve_jitter_type
```
Resolve `jitter`.

* If `jitter` is None and `py_func` is not None, uses `get_func_suffix`
* If `jitter` is a string, looks in `jitters` in `vectorbtpro._settings.jitting`
* If `jitter` is a subclass of `Jitter`, returns it
* If `jitter` is an instance of `Jitter`, returns its class
* Otherwise, throws an error
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Função: get_id_of_jitter_type
```
Get id of a jitter type using `jitters` in `vectorbtpro._settings.jitting`.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Função: resolve_jitted_option
```
Return keyword arguments for `jitted`.

`option` can be:

* True: Decorate using default settings
* False: Do not decorate (returns None)
* string: Use `option` as the name of the jitter
* dict: Use `option` as keyword arguments for jitting

For defaults, see `option` in `vectorbtpro._settings.jitting`.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Função: specialize_jitted_option
```
Resolve `option` and merge it with `kwargs` if it's not None so the dict can be passed
as an option to other functions.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Função: resolve_jitted_kwargs
```
Resolve keyword arguments for `jitted`.

Resolves `option` using `resolve_jitted_option`.

!!! note
    Keys in `option` have more priority than in `kwargs`.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Função: resolve_jitter
```
Resolve jitter.

!!! note
    If `jitter` is already an instance of `Jitter` and there are other keyword arguments, discards them.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Função: jitted
```
Decorate a jitable function.

Resolves `jitter` using `resolve_jitter`.

The wrapping mechanism can be disabled by using the global setting `disable_wrapping`
(=> returns the wrapped function).

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> @vbt.jitted
    ... def my_func_nb():
    ...     total = 0
    ...     for i in range(1000000):
    ...         total += 1
    ...     return total

    >>> %timeit my_func_nb()
    68.1 ns ± 0.32 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)
    ```

    Jitter is automatically detected using the suffix of the wrapped function.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Classe: Jitter
#### Função: wrapping_disabled
```
Whether wrapping is disabled globally.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Classe: Jitter
#### Função: decorate
```
Decorate a jitable function.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Classe: NumbaJitter
#### Função: fix_cannot_parallel
```
Whether to set `parallel` to False if there is no 'can_parallel' tag.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Classe: NumbaJitter
#### Função: options
```
Options passed to the Numba decorator.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Classe: NumbaJitter
#### Função: nopython
```
Whether to run in nopython mode.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Classe: NumbaJitter
#### Função: nogil
```
Whether to release the GIL.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Classe: NumbaJitter
#### Função: parallel
```
Whether to enable automatic parallelization.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Classe: NumbaJitter
#### Função: boundscheck
```
Whether to enable bounds checking for array indices.
```

---

# Pasta: utils
### Arquivo: jitting.py
#### Classe: NumbaJitter
#### Função: cache
```
Whether to write the result of function compilation into a file-based cache.
```

---

# Pasta: utils
### Arquivo: magic_decorators.py
#### Docstring do Módulo
```
Class decorators for attaching magic methods.
```

---

# Pasta: utils
### Arquivo: magic_decorators.py
#### Função: attach_binary_magic_methods
```
Class decorator to attach binary magic methods to a class.

`translate_func` must

* take `self`, `other`, and unary function,
* perform computation, and
* return the result.

`config` defaults to `binary_magic_config` and must contain target method names (keys)
and dictionaries (values) with the following keys:

* `func`: Function that combines two array-like objects.
```

---

# Pasta: utils
### Arquivo: magic_decorators.py
#### Função: attach_unary_magic_methods
```
Class decorator to attach unary magic methods to a class.

`translate_func` must

* take `self` and unary function,
* perform computation, and
* return the result.

`config` defaults to `unary_magic_config` and must contain target method names (keys)
and dictionaries (values) with the following keys:

* `func`: Function that transforms one array-like object.
```

---

# Pasta: utils
### Arquivo: mapping.py
#### Docstring do Módulo
```
Mapping utilities.
```

---

# Pasta: utils
### Arquivo: mapping.py
#### Função: reverse_mapping
```
Reverse a mapping.

Returns a dict.
```

---

# Pasta: utils
### Arquivo: mapping.py
#### Função: to_field_mapping
```
Convert mapping-like object to a field mapping.
```

---

# Pasta: utils
### Arquivo: mapping.py
#### Função: to_value_mapping
```
Convert mapping-like object to a value mapping.

Enable `reverse` to apply `reverse_mapping` on the result dict.
```

---

# Pasta: utils
### Arquivo: mapping.py
#### Função: apply_mapping
```
Apply mapping on object using a mapping-like object.

Args:
    obj (any): Any object.

        Can take a scalar, tuple, list, set, frozenset, NumPy array, Index, Series, and DataFrame.
    mapping_like (mapping_like): Any mapping-like object.

        See `to_value_mapping`.
    enum_unkval (any): Missing value for enumerated types.
    reverse (bool): See `reverse` in `to_value_mapping`.
    ignore_case (bool): Whether to ignore the case if the key is a string.
    ignore_underscores (bool): Whether to ignore underscores if the key is a string.
    ignore_invalid (bool): Whether to remove any character that is not allowed in a Python variable.
    ignore_type (dtype_like or tuple): One or multiple types or data types to ignore.
    ignore_missing (bool): Whether to ignore missing values.
    na_sentinel (any): Value to mark “not found”.
```

---

# Pasta: utils
### Arquivo: math_.py
#### Docstring do Módulo
```
Math utilities.
```

---

# Pasta: utils
### Arquivo: math_.py
#### Função: is_close_nb
```
Tell whether two values are approximately equal.
```

---

# Pasta: utils
### Arquivo: math_.py
#### Função: is_less_nb
```
Tell whether the first value is approximately less than the second value.
```

---

# Pasta: utils
### Arquivo: math_.py
#### Função: is_close_or_less_nb
```
Tell whether the first value is approximately equal to or less than the second value.
```

---

# Pasta: utils
### Arquivo: math_.py
#### Função: is_close_or_greater_nb
```
Tell whether the first value is approximately equal to or greater than the second value.
```

---

# Pasta: utils
### Arquivo: math_.py
#### Função: is_greater_nb
```
Tell whether the first value is approximately greater than the second value.
```

---

# Pasta: utils
### Arquivo: math_.py
#### Função: is_addition_zero_nb
```
Tell whether addition of two values yields zero.
```

---

# Pasta: utils
### Arquivo: math_.py
#### Função: add_nb
```
Add two floats.
```

---

# Pasta: utils
### Arquivo: math_.py
#### Função: round_nb
```
Round a float to a number of decimals.
```

---

# Pasta: utils
### Arquivo: merging.py
#### Docstring do Módulo
```
Utilities for merging.
```

---

# Pasta: utils
### Arquivo: merging.py
#### Classe: MergeFunc
```
Class representing a merging function and its keyword arguments.

Can be directly called to call the underlying (already resolved and with keyword
arguments attached) merging function.
```

---

# Pasta: utils
### Arquivo: merging.py
#### Função: parse_merge_func
```
Parser the merging function from the function's annotations.
```

---

# Pasta: utils
### Arquivo: merging.py
#### Classe: MergeFunc
#### Função: resolve_merge_func
```
Get the merging function where keyword arguments are hard-coded.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Docstring do Módulo
```
Utilities for modules.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: get_module
```
Get module of an object.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: is_from_module
```
Return whether `obj` is from module `module`.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: list_module_keys
```
List the names of all public functions and classes defined in the module `module_name`.

Includes the names listed in `whitelist` and excludes the names listed in `blacklist`.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: search_package_for_funcs
```
Search a package for all functions.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: find_class
```
Find the class by its path.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: check_installed
```
Check if a package is installed.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: get_installed_overview
```
Get an overview of installed packages in `opt_dep_config`.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: get_package_meta
```
Get metadata of a package.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: assert_can_import
```
Assert that a package can be imported.

Must be listed in `opt_dep_config`.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: assert_can_import_any
```
Assert that any from packages can be imported.

Must be listed in `opt_dep_config`.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: warn_cannot_import
```
Warn if a package is cannot be imported.

Must be listed in `opt_dep_config`.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: import_module_from_path
```
Import the module from a path.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: get_method_class
```
Get the class of a method.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: parse_refname
```
Get the reference name of an object.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: get_refname_module_and_qualname
```
Get the module and the qualified name from a reference name.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: resolve_refname
```
Resolve a reference name.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: get_refname
```
Parse and (optionally) resolve the reference name of an object.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: get_imlucky_url
```
Get the "I'm lucky" URL on DuckDuckGo for a query.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: imlucky
```
Open the "I'm lucky" URL on DuckDuckGo for a query.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: get_api_ref
```
Get the API reference to an object.
```

---

# Pasta: utils
### Arquivo: module_.py
#### Função: open_api_ref
```
Open the API reference to an object.
```

---

# Pasta: utils
### Arquivo: params.py
#### Docstring do Módulo
```
Utilities for working with parameters.
```

---

# Pasta: utils
### Arquivo: params.py
#### Função: to_typed_list
```
Cast Python list to typed list.

Direct construction is flawed in Numba 0.52.0.
See https://github.com/numba/numba/issues/6651
```

---

# Pasta: utils
### Arquivo: params.py
#### Função: flatten_param_tuples
```
Flattens a nested list of iterables using unzipping.
```

---

# Pasta: utils
### Arquivo: params.py
#### Função: generate_param_combs
```
Generate arbitrary parameter combinations from the operation tree `op_tree`.

`op_tree` is a tuple with nested instructions to generate parameters.
The first element of the tuple must be either the name of a callale from `itertools` or the
callable itself that takes remaining elements as arguments. If one of the elements is a tuple
itself and its first argument is a callable, it will be unfolded in the same way as above.

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> vbt.generate_param_combs(("product", ("combinations", [0, 1, 2, 3], 2), [4, 5]))
    [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 2, 2],
     [1, 1, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3],
     [4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5]]

    >>> vbt.generate_param_combs(("product", (zip, [0, 1, 2, 3], [4, 5, 6, 7]), [8, 9]))
    [[0, 0, 1, 1, 2, 2, 3, 3], [4, 4, 5, 5, 6, 6, 7, 7], [8, 9, 8, 9, 8, 9, 8, 9]]
    ```
```

---

# Pasta: utils
### Arquivo: params.py
#### Função: broadcast_params
```
Broadcast parameters in `params`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Função: create_param_product
```
Make Cartesian product out of all params in `params`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Função: is_single_param_value
```
Check whether `param_values` is a single value.
```

---

# Pasta: utils
### Arquivo: params.py
#### Função: params_to_list
```
Cast parameters to a list.
```

---

# Pasta: utils
### Arquivo: params.py
#### Função: get_param_grid_len
```
Get the number of parameter combinations in a parameter grid.

Parameter values can also be an integer to represent the number of values.
```

---

# Pasta: utils
### Arquivo: params.py
#### Função: pick_from_param_grid
```
Pick one or more parameter combinations from a parameter grid.

Parameter values can also be an integer to represent the number of values.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Param
```
Class that represents a parameter.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Itemable
```
Class representing an object that can be returned as items.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Paramable
```
Class representing an object that can be returned as a parameter.
```

---

# Pasta: utils
### Arquivo: params.py
#### Função: combine_params
```
Combine a dictionary with parameters of the type `Param`.

Returns a dictionary with combined parameters and an index if `build_index` is True.

If `build_grid` is True, first builds the entire grid and then filters parameter
combinations by conditions and selects random combinations. If `build_grid` is False,
doesn't build the entire grid, but selects and combines combinations on the fly.
Materializing the grid is recommended only when the number of combinations is relatively low
(less than one million) and parameters have conditions.

Argument `grid_indices` can be a slice (for example, `slice(None, None, 2)` for `::2`) or an array
with indices that map to the length of the grid. It can be used to skip a some combinations
before a random subset is drawn.

Argument `random_subset` can be an integer (number of combinations) or a float relative to the
length of the grid. If parameters have conditions, `random_subset` is drawn from the subset of
combinations whose conditions have been met, not the other way around. If `random_replace` is True,
draws random combinations with replacement (that is, duplicate combinations will likely occur).
If `random_replace` is False (default), each drawn combination will be unique. If `random_sort`
is True (default), positions of combinations will be sorted. Otherwise, they will remain in their
randomly-selected positions.

Arguments `max_guesses` and `max_misses` are effective only when the grid is not buildd
and parameters have conditions. They mean the maximum number of guesses and misses respectively
when doing the search for a valid combination in a while-loop. Once any of these two numbers
is reached, the search will stop. They are useful for limiting the number of guesses; without
them, the search may continue forever.

If a name of any parameter is a tuple, can convert this tuple into a string by setting
`name_tuple_to_str` either to True or providing a callable that does this.

Keyword arguments `clean_index_kwargs` are passed to `vectorbtpro.base.indexes.clean_index`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
```
Class responsible for parameterizing and running a function.

Does the following:

1. Searches for values wrapped with the class `Param` in any nested dicts and tuples
using `Parameterizer.find_params_in_obj`
2. Uses `combine_params` to build parameter combinations
3. Maps parameter combinations to configs using `Parameterizer.param_product_to_objs`
4. Generates and resolves parameter configs by combining combinations from the step above with
`param_configs` that is optionally passed by the user. User-defined `param_configs` have more priority.
5. If `selection` is not None, substitutes it as a template, translates it into indices that
can be mapped to `param_index`, and selects them from all the objects generated above.
6. Builds mono-chunks if `mono_n_chunks`, `mono_chunk_len`, or `mono_chunk_meta` is not None
7. Extracts arguments and keyword arguments from each parameter config and substitutes any templates (lazily)
8. Passes each set of the function and its arguments to `vectorbtpro.utils.execution.execute` for execution
9. Optionally, post-processes and merges the results by passing them and `**merge_kwargs` to `merge_func`

Argument `param_configs` accepts either a list of dictionaries with arguments named by their names
in the signature, or a dictionary of dictionaries, where keys are config names. If a list is passed,
each dictionary can also contain the key `_name` to give the config a name. Variable arguments
can be passed either in the rolled (`args=(...), kwargs={...}`) or unrolled
(`args_0=..., args_1=..., some_kwarg=...`) format.

!!! important
    Defining a parameter and listing the same argument in `param_configs` will prioritize
    the config over the parameter, even though the parameter will still be visible in the final columns.
    There are no checks implemented to raise an error when this happens!

If mono-chunking is enabled, parameter configs will be distributed over chunks. Any argument that
is wrapped with `Param` or appears in `mono_merge_func` will be aggregated into a list. It will
be merged using either `Param.mono_merge_func` or `mono_merge_func` in the same way as `merge_func`.
If an argument satisfies neither of the above requirements and all its values within a chunk are same,
this value will be passed as a scalar. Arguments `mono_merge_func` and `mono_merge_kwargs`
must be dictionaries where keys are argument names in the flattened signature and values are
functions and keyword arguments respectively.

If `vectorbtpro.utils.execution.NoResult` is returned, will skip the current iteration and
remove it from the final index.

For defaults, see `vectorbtpro._settings.params`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Função: parameterized
```
Decorator that parameterizes inputs of a function using `Parameterizer`.

Returns a new function with the same signature as the passed one.

Each option can be modified in the `options` attribute of the wrapper function or
directly passed as a keyword argument with a leading underscore.

Keyword arguments `**kwargs` and `execute_kwargs` are merged into `execute_kwargs`
if `merge_to_execute_kwargs` is True, otherwise, `**kwargs` are passed directly to `Parameterizer`.

Usage:
    * No parameters, no parameter configs:

    ```pycon
    >>> from vectorbtpro import *

    >>> @vbt.parameterized(merge_func="column_stack")
    ... def my_ma(sr_or_df, window, wtype="simple", minp=0, adjust=False):
    ...     return sr_or_df.vbt.ma(window, wtype=wtype, minp=minp, adjust=adjust)

    >>> sr = pd.Series([1, 2, 3, 4, 3, 2, 1])
    >>> my_ma(sr, 3)
    0    1.000000
    1    1.500000
    2    2.000000
    3    3.000000
    4    3.333333
    5    3.000000
    6    2.000000
    dtype: float64
    ```

    * One parameter, no parameter configs:

    ```pycon
    >>> my_ma(sr, vbt.Param([3, 4, 5]))
    window         3    4    5
    0       1.000000  1.0  1.0
    1       1.500000  1.5  1.5
    2       2.000000  2.0  2.0
    3       3.000000  2.5  2.5
    4       3.333333  3.0  2.6
    5       3.000000  3.0  2.8
    6       2.000000  2.5  2.6
    ```

    * Product of two parameters, no parameter configs:

    ```pycon
    >>> my_ma(
    ...     sr,
    ...     vbt.Param([3, 4, 5]),
    ...     wtype=vbt.Param(["simple", "exp"])
    ... )
    window         3                4                5
    wtype     simple       exp simple       exp simple       exp
    0       1.000000  1.000000    1.0  1.000000    1.0  1.000000
    1       1.500000  1.500000    1.5  1.400000    1.5  1.333333
    2       2.000000  2.250000    2.0  2.040000    2.0  1.888889
    3       3.000000  3.125000    2.5  2.824000    2.5  2.592593
    4       3.333333  3.062500    3.0  2.894400    2.6  2.728395
    5       3.000000  2.531250    3.0  2.536640    2.8  2.485597
    6       2.000000  1.765625    2.5  1.921984    2.6  1.990398
    ```

    * No parameters, one partial parameter config:

    ```pycon
    >>> my_ma(sr, param_configs=[dict(window=3)])
    param_config         0
    0             1.000000
    1             1.500000
    2             2.000000
    3             3.000000
    4             3.333333
    5             3.000000
    6             2.000000
    ```

    * No parameters, one full parameter config:

    ```pycon
    >>> my_ma(param_configs=[dict(sr_or_df=sr, window=3)])
    param_config         0
    0             1.000000
    1             1.500000
    2             2.000000
    3             3.000000
    4             3.333333
    5             3.000000
    6             2.000000
    ```

    * No parameters, multiple parameter configs:

    ```pycon
    >>> my_ma(param_configs=[
    ...     dict(sr_or_df=sr + 1, window=2),
    ...     dict(sr_or_df=sr - 1, window=3)
    ... ], minp=None)
    param_config    0         1
    0             NaN       NaN
    1             2.5       NaN
    2             3.5  1.000000
    3             4.5  2.000000
    4             4.5  2.333333
    5             3.5  2.000000
    6             2.5  1.000000
    ```

    * Multiple parameters, multiple parameter configs:

    ```pycon
    >>> my_ma(param_configs=[
    ...     dict(sr_or_df=sr + 1, minp=0),
    ...     dict(sr_or_df=sr - 1, minp=None)
    ... ], window=vbt.Param([2, 3]))
    window          2              3
    param_config    0    1         0         1
    0             2.0  NaN  2.000000       NaN
    1             2.5  0.5  2.500000       NaN
    2             3.5  1.5  3.000000  1.000000
    3             4.5  2.5  4.000000  2.000000
    4             4.5  2.5  4.333333  2.333333
    5             3.5  1.5  4.000000  2.000000
    6             2.5  0.5  3.000000  1.000000
    ```

    * Using annotations:

    ```pycon
    >>> @vbt.parameterized
    ... def my_ma(
    ...     sr_or_df,
    ...     window: vbt.Param,
    ...     wtype: vbt.Param = "simple",
    ...     minp=0,
    ...     adjust=False
    ... ) -> vbt.MergeFunc("column_stack"):
    ...     return sr_or_df.vbt.ma(window, wtype=wtype, minp=minp, adjust=adjust)

    >>> my_ma(sr, [3, 4, 5], ["simple", "exp"])
    window         3                4                5
    wtype     simple       exp simple       exp simple       exp
    0       1.000000  1.000000    1.0  1.000000    1.0  1.000000
    1       1.500000  1.500000    1.5  1.400000    1.5  1.333333
    2       2.000000  2.250000    2.0  2.040000    2.0  1.888889
    3       3.000000  3.125000    2.5  2.824000    2.5  2.592593
    4       3.333333  3.062500    3.0  2.894400    2.6  2.728395
    5       3.000000  2.531250    3.0  2.536640    2.8  2.485597
    6       2.000000  1.765625    2.5  1.921984    2.6  1.990398
    ```
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Param
#### Função: map_value
```
Execute a function on each value in `Param.value` and create a new `Param` instance.

If `old_as_keys` is True, will use old values as keys, unless keys are already provided.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Itemable
#### Função: items
```
Return this instance as items.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Paramable
#### Função: as_param
```
Return this instance as a parameter.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: func
```
Function.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: param_search_kwargs
```
See `Parameterized.find_params_in_obj`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: skip_single_comb
```
Whether to execute the function directly if there's only one parameter combination.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: template_context
```
Template context.

Any template in both `execute_kwargs` and `merge_kwargs` will be substituted.
You can use the keys `param_configs`, `param_index`, all keys in `template_context`,
and all arguments as found in the signature of the function. To substitute templates
further down the pipeline, use substitution ids.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: build_grid
```
See `combine_params`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: grid_indices
```
See `combine_params`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: random_subset
```
See `combine_params`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: random_replace
```
See `combine_params`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: random_sort
```
See `combine_params`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: max_guesses
```
See `combine_params`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: max_misses
```
See `combine_params`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: seed
```
See `combine_params`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: clean_index_kwargs
```
See `combine_params`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: name_tuple_to_str
```
See `combine_params`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: selection
```
Parameter combination to select.

The selection can be one or more positions or labels from the parameter index.
The selection value(s) can be wrapped with `vectorbtpro.utils.selection.PosSel` or
`vectorbtpro.utils.selection.LabelSel` to instruct vectorbtpro what the value(s) should denote.
Make sure that it's a sequence (for example, by wrapping it with a list) to attach
the parameter index to the final result. It can be also `vectorbtpro.utils.execution.NoResult`
to indicate that there's no result, or a template to yield any of the above.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: forward_kwargs_as
```
Map to rename keyword arguments.

Can also pass any variable from the scope of `Parameterized.run`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: mono_min_size
```
See `vectorbtpro.utils.chunking.yield_chunk_meta`.

Applied to generate chunk meta.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: mono_n_chunks
```
See `vectorbtpro.utils.chunking.yield_chunk_meta`.

Applied to generate chunk meta.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: mono_chunk_len
```
See `vectorbtpro.utils.chunking.yield_chunk_meta`.

Applied to generate chunk meta.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: mono_chunk_meta
```
Chunk meta.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: mono_reduce
```
See `Param.mono_reduce`.

Can be a dictionary with a value per (unpacked) argument name. Otherwise,
gets applied to each parameter, unless the parameter overrides it.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: mono_merge_func
```
See `Param.mono_merge_func`.

Can be a dictionary with a value per (unpacked) argument name. Otherwise,
gets applied to each parameter, unless the parameter overrides it.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: mono_merge_kwargs
```
See `Param.mono_merge_kwargs`.

Can be a dictionary with a value per (unpacked) argument name. Otherwise,
gets applied to each parameter, unless the parameter overrides it.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: filter_results
```
Whether to filter `vectorbtpro.utils.execution.NoResult` results.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: raise_no_results
```
Whether to raise `vectorbtpro.utils.execution.NoResultsException` if there are no results.

Otherwise, returns `vectorbtpro.utils.execution.NoResult`.

Has effect only if `Parameterizer.filter_results` is True. But regardless of this setting,
gets passed to the merging function if the merging function is pre-configured.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: merge_func
```
Merging function.

Resolved using `vectorbtpro.base.merging.resolve_merge_func`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: merge_kwargs
```
Keyword arguments passed to the merging function.

When defining a custom merging function, make sure to make use of `param_index`
(via templates) to build the final index hierarchy.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: return_meta
```
Whether to return all the metadata generated before running the function.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: return_param_index
```
Whether to return the results along with the parameter index (as a tuple).
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: execute_kwargs
```
Keyword arguments passed to `vectorbtpro.utils.execution.execute`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: find_params_in_obj
```
Find values wrapped with `Param` in a recursive manner.

Uses `vectorbtpro.utils.search.find_in_obj`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: param_product_to_objs
```
Resolve parameter product into a list of objects based on the original object.

Uses `vectorbtpro.utils.search.replace_in_obj`.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: parse_and_inject_params
```
Parse `Param` instances from function annotations and inject them into flattened annotated arguments.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: get_var_arg_names
```
Get the name of any packed variable position arguments and keyword arguments.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: unroll_param_config
```
Unroll a parameter config.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: roll_param_config
```
Roll a parameter config.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: select_comb
```
Select a parameter combination from parameter configs and index.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: yield_tasks
```
Yield functions and their arguments for execution.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: get_mono_chunk_indices
```
Get the indices of each mono-chunk.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: build_mono_chunk_config
```
Build the parameter config for a mono-chunk.
```

---

# Pasta: utils
### Arquivo: params.py
#### Classe: Parameterizer
#### Função: run
```
Parameterize arguments and run the function.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Docstring do Módulo
```
Utilities for parsing.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Classe: Regex
```
Class for matching a regular expression.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Função: get_func_kwargs
```
Get keyword arguments with defaults of a function.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Função: get_func_arg_names
```
Get argument names of a function.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Função: extend_args
```
Extend arguments and keyword arguments with other arguments.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Função: annotate_args
```
Annotate arguments and keyword arguments using the function's signature.

If `allow_partial` is True, required arguments that weren't provided won't raise an error.
But regardless of `allow_partial`, arguments that aren't in the signature will still raise an error.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Função: ann_args_to_args
```
Convert annotated arguments back to positional and keyword arguments.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Função: flat_ann_args_to_args
```
Convert flattened annotated arguments back to positional and keyword arguments.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Função: flatten_ann_args
```
Flatten annotated arguments.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Função: unflatten_ann_args
```
Unflatten annotated arguments.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Função: match_flat_ann_arg
```
Match an argument from flattened annotated arguments.

A query can be an integer indicating the position of the argument, or a string containing the name
of the argument, or a regular expression for matching the name of the argument.

If multiple arguments were matched, returns the first one.

The position can stretch over any variable argument.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Função: match_ann_arg
```
Match an argument from annotated arguments.

See `match_flat_ann_arg` for matching logic.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Função: match_and_set_flat_ann_arg
```
Match an argument from flattened annotated arguments and set it to a new value.

See `match_flat_ann_arg` for matching logic.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Função: ignore_flat_ann_args
```
Ignore flattened annotated arguments.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Classe: UnhashableArgsError
```
Unhashable arguments error.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Função: hash_args
```
Get hash of arguments.

Use `ignore_args` to provide a sequence of queries for arguments that should be ignored.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Função: get_expr_var_names
```
Get variable names listed in the expression.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Função: get_context_vars
```
Get variables from the local/global context.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Função: suppress_stdout
```
Suppress output from a function.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Função: warn_stdout
```
Supress and convert to a warning output from a function.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Classe: PrintsSuppressed
```
Context manager to ignore print statements.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Classe: WarningsFiltered
```
Context manager to ignore warnings.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Classe: Regex
#### Função: matches
```
Return whether the string matches the regular expression pattern.
```

---

# Pasta: utils
### Arquivo: parsing.py
#### Classe: WarningsFiltered
#### Função: entries
```
One or more simple entries to add into the list of warnings filters.
```

---

# Pasta: utils
### Arquivo: path_.py
#### Docstring do Módulo
```
Utilities for working with paths.
```

---

# Pasta: utils
### Arquivo: path_.py
#### Função: list_any_files
```
List files and directories matching a path.

If the directory path is not provided, the current working directory is used.
```

---

# Pasta: utils
### Arquivo: path_.py
#### Função: list_files
```
List files matching a path using `list_any_files`.
```

---

# Pasta: utils
### Arquivo: path_.py
#### Função: list_dirs
```
List directories matching a path using `list_any_files`.
```

---

# Pasta: utils
### Arquivo: path_.py
#### Função: file_exists
```
Check whether a file exists.
```

---

# Pasta: utils
### Arquivo: path_.py
#### Função: dir_exists
```
Check whether a directory exists.
```

---

# Pasta: utils
### Arquivo: path_.py
#### Função: file_size
```
Get size of a file.
```

---

# Pasta: utils
### Arquivo: path_.py
#### Função: dir_size
```
Get size of a directory.
```

---

# Pasta: utils
### Arquivo: path_.py
#### Função: check_mkdir
```
Check whether the path to a directory exists and create if it doesn't.

For defaults, see `mkdir` in `vectorbtpro._settings.path`.
```

---

# Pasta: utils
### Arquivo: path_.py
#### Função: make_file
```
Make an empty file.
```

---

# Pasta: utils
### Arquivo: path_.py
#### Função: make_dir
```
Make an empty directory.
```

---

# Pasta: utils
### Arquivo: path_.py
#### Função: remove_file
```
Remove (delete) a file.
```

---

# Pasta: utils
### Arquivo: path_.py
#### Função: remove_dir
```
Remove (delete) a directory.
```

---

# Pasta: utils
### Arquivo: path_.py
#### Função: dir_tree
```
Given a directory Path object print a visual tree structure.

Inspired by this answer: https://stackoverflow.com/a/59109706
```

---

# Pasta: utils
### Arquivo: path_.py
#### Função: print_dir_tree
```
Generate a directory tree with `tree` and print it out.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Docstring do Módulo
```
Utilities for progress bars.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
```
Context manager to manage a progress bar.

Supported types:

* 'tqdm_auto'
* 'tqdm_notebook'
* 'tqdm_gui'
* 'tqdm'

For defaults, see `vectorbtpro._settings.pbar`.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressHidden
```
Context manager to hide progress.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Função: with_progress_hidden
```
Decorator to run a function with `ProgressHidden`.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressShown
```
Context manager to show progress.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Função: with_progress_shown
```
Decorator to run a function with `ProgressShown`.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: bar_id
```
Bar id.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: bar_type
```
Bar type.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: force_open_bar
```
Whether to force-open a bar even if progress is not shown.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: reuse
```
Whether the bar can be reused.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: show_progress
```
Whether to show the bar.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: iterable
```
Iterable.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: kwargs
```
Keyword arguments passed to initialize the bar.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: show_progress_desc
```
Whether show the bar description.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: desc_kwargs
```
Keyword arguments passed to `ProgressBar.set_description`.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: registry
```
Registry of type `vectorbtpro.registries.pbar_registry.PBarRegistry`.

If None, registry is disabled.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: silence_warnings
```
Whether to silence warnings.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: bar
```
Bar.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: open_time
```
Time the bar was opened.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: update_time
```
Time the bar was updated.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: refresh_time
```
Time the bar was refreshed.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: close_time
```
Time the bar was closed.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: set_bar
```
Set the bar.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: remove_bar
```
Remove the bar.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: reset
```
Reset the bar.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: open
```
Open the bar.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: close
```
Close the bar.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: active
```
Whether the bar is active.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: pending
```
Whether the bar is pending.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: disabled
```
Whether the bar is disabled.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: displayed
```
Whether the bar is displayed.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: should_display
```
Whether the bar should be displayed.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: refresh
```
Refresh the bar.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: before_update
```
Do something before an update.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: update
```
Update with one or more iterations.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: update_to
```
Update to a specific number.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: after_update
```
Do something after an update.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: prepare_desc
```
Prepare description.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: set_prefix
```
Set prefix.

Prepares it with `ProgressBar.prepare_desc`.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: set_prefix_str
```
Set prefix without preparation.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: set_postfix
```
Set postfix.

Prepares it with `ProgressBar.prepare_desc`.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: set_postfix_str
```
Set postfix without preparation.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: set_description
```
Set description.

Uses the method `ProgressBar.set_prefix` if `as_postfix=True` in `ProgressBar.desc_kwargs`.
Otherwise, uses the method `ProgressBar.set_postfix`.

Uses `ProgressBar.desc_kwargs` as keyword arguments.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: set_description_str
```
Set description without preparation.

Uses the method `ProgressBar.set_prefix_str` if `as_postfix=True` in `ProgressBar.desc_kwargs`.
Otherwise, uses the method `ProgressBar.set_postfix_str`.

Uses `ProgressBar.desc_kwargs` as keyword arguments.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: enter
```
Enter the bar.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: exit
```
Exit the bar.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressBar
#### Função: iter
```
Get iterator over `ProgressBar.iterable`.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressHidden
#### Função: disable_registry
```
Whether to disable registry.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressHidden
#### Função: disable_machinery
```
Whether to disable machinery.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressHidden
#### Função: init_settings
```
Initial settings.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressShown
#### Função: enable_registry
```
Whether to enable registry.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressShown
#### Função: enable_machinery
```
Whether to enable machinery.
```

---

# Pasta: utils
### Arquivo: pbar.py
#### Classe: ProgressShown
#### Função: init_settings
```
Initial settings.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Docstring do Módulo
```
Utilities for pickling.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Função: get_serialization_extensions
```
Get all supported serialization extensions from `vectorbtpro._settings`.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Função: get_compression_extensions
```
Get all supported compression extensions from `vectorbtpro._settings`.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Função: dumps
```
Pickle an object to a byte stream.

Uses `dill` when available.

For compression options, see `extensions`.

Keyword arguments `compress_kwargs` are passed to the `compress` method of the compressing package.

Other keyword arguments are passed to the `dumps` method of the pickling package.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Função: loads
```
Unpickle an object from a byte stream.

Accepts the same options for `compression` as in the `dumps` method.

Other keyword arguments are passed to the `loads` method of the pickling package.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Função: save
```
Pickle an object to a byte stream and write to a file.

Can recognize the compression algorithm based on the extension (see `dumps` for options).

Uses `dumps` for serialization.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Função: load
```
Read a byte stream from a file and unpickle.

Can recognize the compression algorithm based on the extension (see `dumps` for options).

Uses `loads` for deserialization.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: RecState
```
Class that represents a state used to reconstruct an instance.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: RecInfo
```
Class that represents information needed to reconstruct an instance.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Função: get_id_from_class
```
Get the class id from a class.

If the object is an instance or a subclass of `Pickleable` and `Pickleable._rec_id` is not None,
uses the reconstruction id. Otherwise, returns the path to the class definition
with `vectorbtpro.utils.module_.find_class`.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Função: get_class_from_id
```
Get the class from a class id.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Função: reconstruct
```
Reconstruct an instance using a class and a reconstruction state.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: Pickleable
```
Superclass that defines abstract properties and methods for pickle-able classes.

If any subclass cannot be pickled, override the `Pickleable.rec_state` property
to return an instance of `RecState` to be used in reconstruction. If the class definition cannot
be pickled (e.g., created dynamically), override its `_rec_id` with an arbitrary id string,
dump/save the class, and before loading, map this id to the class in `rec_id_map`. This will use the
mapped class to construct a new instance.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: pdict
```
Pickleable dict.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: RecInfo
#### Função: register
```
Register self in `rec_info_registry`.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: Pickleable
#### Função: dumps
```
Pickle the instance to a byte stream.

Optionally, you can set `rec_state_only` to True if the instance will be later unpickled
directly by the class.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: Pickleable
#### Função: loads
```
Unpickle an instance from a byte stream.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: Pickleable
#### Função: encode_config_node
```
Encode a config node.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: Pickleable
#### Função: decode_config_node
```
Decode a config node.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: Pickleable
#### Função: encode_config
```
Encode the instance to a config string.

Based on `Pickleable.rec_state`. Raises an error if None.

Encodes to a format that is guaranteed to be parsed using `Pickleable.decode_config`.
Otherwise, an error will be thrown. If any object cannot be represented using a string,
uses `dumps` to convert it to a byte stream.

When `unpack_objects` is True and an object is an instance of `Pickleable`, saves its
reconstruction state to a separate section rather than the byte stream. Appends `@` and
class name to the section name. If `compress_unpacked` is True, will hide keys in
`RecState` that have empty values. Keys in `RecState` will be appended with `~` to avoid
collision with user-defined keys having the same name.

If `use_refs` is True, out of unhashable objects sharing the same id, only the first one
will be defined while others will store the reference (`&` + key path) to the first one.

!!! note
    The initial order of keys can be preserved only by using references.

If `use_class_ids` is True, substitutes any class defined as a value by its id instead of
pickling its definition. If `get_id_from_class` returns None, will pickle the definition.

If the instance is nested, set `nested` to True to represent each sub-dict as a section.

Other keyword arguments are forwarded to `Pickleable.encode_config_node`.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: Pickleable
#### Função: decode_config
```
Decode an instance from a config string.

Can parse configs without sections. Sections can also become sub-dicts if their names use
the dot notation. For example, section `a.b` will become a sub-dict of the section `a`
and section `a.b.c` will become a sub-dict of the section `a.b`. You don't have to define
the section `a` explicitly, it will automatically become the outermost key.

If a section contains only one pair `_ = _`, it will become an empty dict.

If `parse_literals` is True, will detect any Python literals and containers such as `True` and `[]`.
Will also understand `np.nan`, `np.inf`, and `-np.inf`.

If `run_code` is True, will run any Python code prepended with `!`. Will use the context
`code_context` together with already defined `np` (NumPy), `pd` (Pandas), and `vbt` (vectorbtpro).

!!! warning
    Unpickling byte streams and running code has important security implications. Don't attempt
    to parse configs coming from untrusted sources as those can contain malicious code!

If `pack_objects` is True, will look for class paths prepended with `@` in section names,
construct an instance of `RecState` (any other keyword arguments will be included to `init_kwargs`),
and finally use `reconstruct` to reconstruct the unpacked object.

If `use_refs` is True, will substitute references prepended with `&` for actual objects.
Constructs a DAG using [graphlib](https://docs.python.org/3/library/graphlib.html).

If `use_class_ids` is True, will substitute any class ids prepended with `@` with the
corresponding class.

Other keyword arguments are forwarded to `Pickleable.decode_config_node`.

Usage:
    * File `types.ini`:

    ```ini
    string = 'hello world'
    boolean = False
    int = 123
    float = 123.45
    exp_float = 1e-10
    nan = np.nan
    inf = np.inf
    numpy = !np.array([1, 2, 3])
    pandas = !pd.Series([1, 2, 3])
    expression = !dict(sub_dict2=dict(some="value"))
    mult_expression = !import math; math.floor(1.5)
    ```

    ```pycon
    >>> from vectorbtpro import *

    >>> vbt.pprint(vbt.pdict.load("types.ini"))
    pdict(
        string='hello world',
        boolean=False,
        int=123,
        float=123.45,
        exp_float=1e-10,
        nan=np.nan,
        inf=np.inf,
        numpy=<numpy.ndarray object at 0x7fe1bf84f690 of shape (3,)>,
        pandas=<pandas.core.series.Series object at 0x7fe1c9a997f0 of shape (3,)>,
        expression=dict(
            sub_dict2=dict(
                some='value'
            )
        ),
        mult_expression=1
    )
    ```

    * File `refs.ini`:

    ```ini
    [top]
    sr = &top.sr

    [top.sr @pandas.Series]
    data = [10756.12, 10876.76, 11764.33]
    index = &top.sr.index
    name = 'Open time'

    [top.sr.index @pandas.DatetimeIndex]
    data = ["2023-01-01", "2023-01-02", "2023-01-03"]
    ```

    ```pycon
    >>> vbt.pdict.load("refs.ini")["sr"]
    2023-01-01    10756.12
    2023-01-02    10876.76
    2023-01-03    11764.33
    Name: Open time, dtype: float64
    ```
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: Pickleable
#### Função: resolve_file_path
```
Resolve a file path.

A file must have either one or two extensions: file format (required) and compression (optional).
For file format options, see `pickle_extensions` and `config_extensions`.
For compression options, see `compression_extensions`. Each can be provided either
via a suffix in `path`, or via the argument `file_format` and `compression` respectively.

When saving, uses `file_format` and `compression` from `vectorbtpro._settings.pickling`
as default options. When loading, searches for matching files in the current directory.

Path can be also None or directory, in such a case the file name will be set to the class name.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: Pickleable
#### Função: file_exists
```
Return whether a file already exists.

Resolves the file path using `Pickleable.resolve_file_path`.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: Pickleable
#### Função: save
```
Pickle/encode the instance and save to a file.

Resolves the file path using `Pickleable.resolve_file_path`.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: Pickleable
#### Função: load
```
Unpickle/decode the instance from a file.

Resolves the file path using `Pickleable.resolve_file_path`.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: Pickleable
#### Função: getsize
```
Get size of this object.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: Pickleable
#### Função: rec_state
```
Reconstruction state of the type `RecState`.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: Pickleable
#### Função: modify_state
```
Modify the reconstruction state before reconstruction.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: pdict
#### Função: load_update
```
Load dumps from a file and update this instance in-place.
```

---

# Pasta: utils
### Arquivo: pickling.py
#### Classe: pdict
#### Função: equals
```
Check two objects for equality.
```

---

# Pasta: utils
### Arquivo: profiling.py
#### Docstring do Módulo
```
Utilities for profiling time and memory.
```

---

# Pasta: utils
### Arquivo: profiling.py
#### Classe: Timer
```
Context manager to measure execution time using `timeit`.

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> with vbt.Timer() as timer:
    >>>     sleep(1)

    >>> print(timer.elapsed())
    1.01 seconds

    >>> timer.elapsed(readable=False)
    datetime.timedelta(seconds=1, microseconds=5110)
    ```
```

---

# Pasta: utils
### Arquivo: profiling.py
#### Função: with_timer
```
Decorator to run a function with `Timer`.
```

---

# Pasta: utils
### Arquivo: profiling.py
#### Função: timeit
```
Run `timeit` on a function.

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> def my_func():
    ...     sleep(1)

    >>> elapsed = vbt.timeit(my_func)
    >>> print(elapsed)
    1.01 seconds

    >>> vbt.timeit(my_func, readable=False)
    datetime.timedelta(seconds=1, microseconds=1870)
    ```
```

---

# Pasta: utils
### Arquivo: profiling.py
#### Função: with_timeit
```
Decorator to run a function with `timeit`.
```

---

# Pasta: utils
### Arquivo: profiling.py
#### Classe: MemTracer
```
Context manager to trace peak and final memory usage using `tracemalloc`.

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> with vbt.MemTracer() as tracer:
    >>>     np.random.uniform(size=1000000)

    >>> print(tracer.peak_usage())
    8.0 MB

    >>> tracer.peak_usage(readable=False)
    8005360
    ```
```

---

# Pasta: utils
### Arquivo: profiling.py
#### Função: with_memtracer
```
Decorator to run a function with `MemTracer`.
```

---

# Pasta: utils
### Arquivo: profiling.py
#### Classe: Timer
#### Função: start_time
```
Start time.
```

---

# Pasta: utils
### Arquivo: profiling.py
#### Classe: Timer
#### Função: end_time
```
End time.
```

---

# Pasta: utils
### Arquivo: profiling.py
#### Classe: Timer
#### Função: elapsed
```
Get elapsed time.

`**kwargs` are passed to `humanize.precisedelta`.
```

---

# Pasta: utils
### Arquivo: profiling.py
#### Classe: MemTracer
#### Função: final_usage
```
Get final memory usage.

`**kwargs` are passed to `humanize.naturalsize`.
```

---

# Pasta: utils
### Arquivo: profiling.py
#### Classe: MemTracer
#### Função: peak_usage
```
Get peak memory usage.

`**kwargs` are passed to `humanize.naturalsize`.
```

---

# Pasta: utils
### Arquivo: random_.py
#### Docstring do Módulo
```
Utilities for random number generation.
```

---

# Pasta: utils
### Arquivo: random_.py
#### Função: set_seed_nb
```
Set seed in numba.
```

---

# Pasta: utils
### Arquivo: random_.py
#### Função: set_seed
```
Set seed.
```

---

# Pasta: utils
### Arquivo: requests_.py
#### Docstring do Módulo
```
Utilities for requests.
```

---

# Pasta: utils
### Arquivo: requests_.py
#### Função: requests_retry_session
```
Retry `retries` times if unsuccessful.
```

---

# Pasta: utils
### Arquivo: requests_.py
#### Função: text_to_giphy_url
```
Translate text to GIF.

See https://engineering.giphy.com/contextually-aware-search-giphy-gets-work-specific/
```

---

# Pasta: utils
### Arquivo: schedule_.py
#### Docstring do Módulo
```
Utilities for scheduling jobs.
```

---

# Pasta: utils
### Arquivo: schedule_.py
#### Classe: CancelledError
```
Thrown for the operation to be cancelled.
```

---

# Pasta: utils
### Arquivo: schedule_.py
#### Classe: AsyncJob
```
Async `CustomJob`.
```

---

# Pasta: utils
### Arquivo: schedule_.py
#### Classe: AsyncScheduler
```
Async `CustomScheduler`.
```

---

# Pasta: utils
### Arquivo: schedule_.py
#### Classe: ScheduleManager
```
Class that manages `CustomScheduler`.
```

---

# Pasta: utils
### Arquivo: schedule_.py
#### Classe: AsyncScheduler
#### Função: every
```
Schedule a new periodic job of type `AsyncJob`.
```

---

# Pasta: utils
### Arquivo: schedule_.py
#### Classe: ScheduleManager
#### Função: scheduler
```
Scheduler.
```

---

# Pasta: utils
### Arquivo: schedule_.py
#### Classe: ScheduleManager
#### Função: async_task
```
Current async task.
```

---

# Pasta: utils
### Arquivo: schedule_.py
#### Classe: ScheduleManager
#### Função: every
```
Create a new job that runs every `interval` units of time.

`*args` can include at most four different arguments: `interval`, `unit`, `start_day`, and `at`,
in the strict order:

* `interval`: integer or `datetime.timedelta`
* `unit`: `ScheduleManager.units`
* `start_day`: `ScheduleManager.weekdays`
* `at`: string or `datetime.time`.

See the package `schedule` for more details.

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> def job_func(message="I'm working..."):
    ...     print(message)

    >>> my_manager = vbt.ScheduleManager()

    >>> # add jobs
    >>> my_manager.every().do(job_func, message="Hello")
    Every 1 second do job_func(message='Hello') (last run: [never], next run: 2021-03-18 19:06:47)

    >>> my_manager.every(10, 'minutes').do(job_func)
    Every 10 minutes do job_func() (last run: [never], next run: 2021-03-18 19:16:46)

    >>> my_manager.every(10, 'minutes', ':00', zero_offset=True).do(job_func)
    Every 10 minutes at 00:00:00 do job_func() (last run: [never], next run: 2022-08-18 16:10:00)

    >>> my_manager.every('hour').do(job_func)
    Every 1 hour do job_func() (last run: [never], next run: 2021-03-18 20:06:46)

    >>> my_manager.every('hour', '00:00').do(job_func)
    Every 1 hour at 00:00:00 do job_func() (last run: [never], next run: 2021-03-18 20:00:00)

    >>> my_manager.every(4, 'hours', '00:00').do(job_func)
    Every 4 hours at 00:00:00 do job_func() (last run: [never], next run: 2021-03-19 00:00:00)

    >>> my_manager.every('10:30').do(job_func)
    Every 1 day at 10:30:00 do job_func() (last run: [never], next run: 2021-03-19 10:30:00)

    >>> my_manager.every('hour', '00:00').do(job_func)
    Every 1 hour at 00:00:00 do job_func() (last run: [never], next run: 2021-03-19 10:30:00)

    >>> my_manager.every(4, 'hour', '00:00').do(job_func)
    Every 4 hours at 00:00:00 do job_func() (last run: [never], next run: 2021-03-19 10:30:00)

    >>> my_manager.every('day', '10:30').do(job_func)
    Every 1 day at 10:30:00 do job_func() (last run: [never], next run: 2021-03-19 10:30:00)

    >>> my_manager.every('day', time(9, 30, tzinfo="utc")).do(job_func)
    Every 1 day at 10:30:00 do job_func() (last run: [never], next run: 2021-03-19 10:30:00)

    >>> my_manager.every('monday').do(job_func)
    Every 1 week do job_func() (last run: [never], next run: 2021-03-22 19:06:46)

    >>> my_manager.every('wednesday', '13:15').do(job_func)
    Every 1 week at 13:15:00 do job_func() (last run: [never], next run: 2021-03-24 13:15:00)

    >>> my_manager.every('minute', ':17').do(job_func)
    Every 1 minute at 00:00:17 do job_func() (last run: [never], next run: 2021-03-18 19:07:17)

    >>> my_manager.start()
    ```

    You can still use the chained approach as done by `schedule`:

    ```pycon
    >>> my_manager.every().minute.at(':17').do(job_func)
    Every 1 minute at 00:00:17 do job_func() (last run: [never], next run: 2021-03-18 19:07:17)
    ```
```

---

# Pasta: utils
### Arquivo: schedule_.py
#### Classe: ScheduleManager
#### Função: start
```
Run pending jobs in a loop.
```

---

# Pasta: utils
### Arquivo: schedule_.py
#### Classe: ScheduleManager
#### Função: done_callback
```
Callback run when the async task is finished.
```

---

# Pasta: utils
### Arquivo: schedule_.py
#### Classe: ScheduleManager
#### Função: start_in_background
```
Run `ScheduleManager.async_start` in the background.
```

---

# Pasta: utils
### Arquivo: schedule_.py
#### Classe: ScheduleManager
#### Função: async_task_running
```
Whether the async task is running.
```

---

# Pasta: utils
### Arquivo: schedule_.py
#### Classe: ScheduleManager
#### Função: stop
```
Stop the async task.
```

---

# Pasta: utils
### Arquivo: schedule_.py
#### Classe: ScheduleManager
#### Função: clear_jobs
```
Delete scheduled jobs with the given tags, or all jobs if tag is omitted.
```

---

# Pasta: utils
### Arquivo: search.py
#### Docstring do Módulo
```
Utilities for searching.
```

---

# Pasta: utils
### Arquivo: search.py
#### Função: find_in_obj
```
Find matches in an object in a recursive manner.

Traverses dicts, tuples, lists and (frozen-)sets. Does not look for matches in keys.

If `excl_types` is not None, uses `vectorbtpro.utils.checks.is_instance_of` to check whether
the object is one of the types that are blacklisted. If so, the object is simply returned.
Same for `incl_types` for whitelisting, but it has a priority over `excl_types`.

If `max_len` is not None, processes any object only if it's shorter than the specified length.

If `max_depth` is not None, processes any object only up to a certain recursion level.
Level of 0 means dicts and other iterables are not processed, only matches are expected.

Returns a map of keys (multiple levels get represented by a tuple) to their respective values.

For defaults, see `vectorbtpro._settings.search`.
```

---

# Pasta: utils
### Arquivo: search.py
#### Função: replace_in_obj
```
Replace matches in an object in a recursive manner.

See `find_in_obj` for `match_dct` (returned value).
```

---

# Pasta: utils
### Arquivo: search.py
#### Função: any_in_obj
```
Return whether there is any match in an object in a recursive manner.

See `find_in_obj` for arguments.
```

---

# Pasta: utils
### Arquivo: search.py
#### Função: find_and_replace_in_obj
```
Find and replace matches in an object in a recursive manner.

See `find_in_obj` for arguments.

!!! note
    If the object is deep (such as a dict or a list), creates a copy of it if any match found inside,
    thus losing the reference to the original. Make sure to do a deep or hybrid copy of the object
    before proceeding for consistent behavior, or disable `make_copy` to override the original in place.
```

---

# Pasta: utils
### Arquivo: selection.py
#### Docstring do Módulo
```
Utilities for selecting.
```

---

# Pasta: utils
### Arquivo: selection.py
#### Classe: PosSel
```
Class that represents a selection by position.
```

---

# Pasta: utils
### Arquivo: selection.py
#### Classe: LabelSel
```
Class that represents a selection by label.
```

---

# Pasta: utils
### Arquivo: tagging.py
#### Docstring do Módulo
```
Utilities for working with tags.
```

---

# Pasta: utils
### Arquivo: tagging.py
#### Função: match_tags
```
Match tags in `tags` to that in `in_tags`.

Multiple tags in `tags` are combined using OR rule, that is, returns True if any of them is found in `in_tags`.
If any tag is not an identifier, evaluates it as a boolean expression.
All tags in `in_tags` must be identifiers.

Usage:
    ```pycon
    >>> from vectorbtpro.utils.tagging import match_tags

    >>> match_tags('hello', 'hello')
    True
    >>> match_tags('hello', 'world')
    False
    >>> match_tags(['hello', 'world'], 'world')
    True
    >>> match_tags('hello', ['hello', 'world'])
    True
    >>> match_tags('hello and world', ['hello', 'world'])
    True
    >>> match_tags('hello and not world', ['hello', 'world'])
    False
    ```
```

---

# Pasta: utils
### Arquivo: template.py
#### Docstring do Módulo
```
Utilities for working with templates.
```

---

# Pasta: utils
### Arquivo: template.py
#### Classe: CustomTemplate
```
Class for substituting templates.
```

---

# Pasta: utils
### Arquivo: template.py
#### Classe: Sub
```
Template string to substitute parts with the respective values from `context`.

Always returns a string.
```

---

# Pasta: utils
### Arquivo: template.py
#### Classe: Rep
```
Template string to be replaced with the respective value from `context`.
```

---

# Pasta: utils
### Arquivo: template.py
#### Classe: RepEval
```
Template expression to be evaluated using `vectorbtpro.utils.eval_.multiline_eval`
with `context` used as locals.
```

---

# Pasta: utils
### Arquivo: template.py
#### Classe: RepFunc
```
Template function to be called with argument names from `context`.
```

---

# Pasta: utils
### Arquivo: template.py
#### Função: has_templates
```
Check if the object has any templates.

Uses `vectorbtpro.utils.search.any_in_obj`.

Default can be overridden with `search_kwargs` under `vectorbtpro._settings.template`.
```

---

# Pasta: utils
### Arquivo: template.py
#### Função: substitute_templates
```
Traverses the object recursively and, if any template found, substitutes it using a context.

Uses `vectorbtpro.utils.search.find_and_replace_in_obj`.

If `strict` is True, raises an error if processing template fails. Otherwise, returns the original template.

Default can be overridden with `search_kwargs` under `vectorbtpro._settings.template`.

Usage:
    ```pycon
    >>> from vectorbtpro import *

    >>> vbt.substitute_templates(vbt.Sub('$key', {'key': 100}))
    100
    >>> vbt.substitute_templates(vbt.Sub('$key', {'key': 100}), {'key': 200})
    200
    >>> vbt.substitute_templates(vbt.Sub('$key$key'), {'key': 100})
    100100
    >>> vbt.substitute_templates(vbt.Rep('key'), {'key': 100})
    100
    >>> vbt.substitute_templates([vbt.Rep('key'), vbt.Sub('$key$key')], {'key': 100}, incl_types=list)
    [100, '100100']
    >>> vbt.substitute_templates(vbt.RepFunc(lambda key: key == 100), {'key': 100})
    True
    >>> vbt.substitute_templates(vbt.RepEval('key == 100'), {'key': 100})
    True
    >>> vbt.substitute_templates(vbt.RepEval('key == 100', strict=True))
    NameError: name 'key' is not defined
    >>> vbt.substitute_templates(vbt.RepEval('key == 100', strict=False))
    <vectorbtpro.utils.template.RepEval at 0x7fe3ad2ab668>
    ```
```

---

# Pasta: utils
### Arquivo: template.py
#### Classe: CustomTemplate
#### Função: resolve_context
```
Resolve `CustomTemplate.context`.

Merges `context` in `vectorbtpro._settings.template`, `CustomTemplate.context`, and `context`.
Automatically appends `eval_id`, `np` (NumPy), `pd` (Pandas), and `vbt` (vectorbtpro).
```

---

# Pasta: utils
### Arquivo: template.py
#### Classe: CustomTemplate
#### Função: resolve_strict
```
Resolve `CustomTemplate.strict`.

If `strict` is None, uses `strict` in `vectorbtpro._settings.template`.
```

---

# Pasta: utils
### Arquivo: template.py
#### Classe: CustomTemplate
#### Função: substitute
```
Abstract method to substitute the template `CustomTemplate.template` using
the context from merging `CustomTemplate.context` and `context`.
```

---

# Pasta: utils
### Arquivo: template.py
#### Classe: Sub
#### Função: substitute
```
Substitute parts of `Sub.template` as a regular template.
```

---

# Pasta: utils
### Arquivo: template.py
#### Classe: Rep
#### Função: substitute
```
Replace `Rep.template` as a key.
```

---

# Pasta: utils
### Arquivo: template.py
#### Classe: RepEval
#### Função: substitute
```
Evaluate `RepEval.template` as an expression.
```

---

# Pasta: utils
### Arquivo: template.py
#### Classe: RepFunc
#### Função: substitute
```
Call `RepFunc.template` as a function.
```

---

# Pasta: utils
### Arquivo: __init__.py
#### Docstring do Módulo
```
Modules with utilities that are used throughout the package.
```

---
